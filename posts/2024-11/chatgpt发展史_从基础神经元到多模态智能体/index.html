<!doctype html><html class=no-js lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>ChatGPT发展史：从基础神经元到多模态智能体 - 文字轨迹</title>
<script>(function(e,t){e[t]=e[t].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:url" content="https://ixxmu.github.io/posts/2024-11/chatgpt%E5%8F%91%E5%B1%95%E5%8F%B2_%E4%BB%8E%E5%9F%BA%E7%A1%80%E7%A5%9E%E7%BB%8F%E5%85%83%E5%88%B0%E5%A4%9A%E6%A8%A1%E6%80%81%E6%99%BA%E8%83%BD%E4%BD%93/"><meta property="og:site_name" content="文字轨迹"><meta property="og:title" content="ChatGPT发展史：从基础神经元到多模态智能体"><meta property="og:description" content="ChatGPT发展史：从基础神经元到多模态智能体 by 科学杂志1915 1"><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-11-22T05:01:50+00:00"><meta property="article:modified_time" content="2024-11-22T05:01:50+00:00"><meta property="article:tag" content="Fetched"><meta property="article:tag" content="科学杂志1915"><meta itemprop=name content="ChatGPT发展史：从基础神经元到多模态智能体"><meta itemprop=description content="ChatGPT发展史：从基础神经元到多模态智能体 by 科学杂志1915 1"><meta itemprop=datePublished content="2024-11-22T05:01:50+00:00"><meta itemprop=dateModified content="2024-11-22T05:01:50+00:00"><meta itemprop=wordCount content="270"><meta itemprop=keywords content="Fetched,科学杂志1915"><meta name=twitter:card content="summary"><meta name=twitter:title content="ChatGPT发展史：从基础神经元到多模态智能体"><meta name=twitter:description content="ChatGPT发展史：从基础神经元到多模态智能体 by 科学杂志1915 1"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=../../../css/style.css><link rel=stylesheet href=../../../css/custom.css><link rel="shortcut icon" href=../../../favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=../../../ title=文字轨迹 rel=home><div class="logo__item logo__text"><div class=logo__title>文字轨迹</div><div class=logo__tagline>故事流淌过的地方</div></div></a></div><nav class=menu><button class=menu__btn aria-haspopup=true aria-expanded=false tabindex=0>
<span class=menu__btn-title tabindex=-1>Menu</span></button><ul class=menu__list><li class=menu__item><a class=menu__link href=../../../posts/><span class=menu__text>文章</span></a></li><li class=menu__item><a class=menu__link href=../../../tags/><span class=menu__text>标签</span></a></li><li class=menu__item><a class=menu__link href=../../../about/><span class=menu__text>关于</span></a></li></ul></nav></div></header><div class="wrapper flex"><div class=primary><main class=main role=main><article class=post><header class=post__header><meta name=referrer content="never"><h1 class=post__title>ChatGPT发展史：从基础神经元到多模态智能体</h1><div class="post__meta meta"><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0a14 14 0 110 28 1 1 0 010-28m0 3a3 3 0 100 22 3 3 0 000-22m1 4h-2v8.4l6.8 4.4L22 18l-6-3.8z"/></svg><time class=meta__text datetime=2024-11-22T05:01:50Z>2024-11-22</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=../../../categories/duty/ rel=category>Duty</a></span></div><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 16 16"><path d="M8 1c2 0 3.5 2 3.5 4.5S10 9 10 9c3 1 4 2 4 6H2c0-4 1-5 4-6 0 0-1.5-1-1.5-3.5S6 1 8 1"/></svg><span class=meta__text>Bloger</span></div></div></header><div class="post__toc toc"><div class=toc__title>Page content</div><div class=toc__menu><nav id=TableOfContents><ul><li><a href=#chatgpt发展史从基础神经元到多模态智能体-by-科学杂志1915>ChatGPT发展史：从基础神经元到多模态智能体 by 科学杂志1915</a></li></ul></nav></div></div><div class="content post__content clearfix"><h2 id=chatgpt发展史从基础神经元到多模态智能体-by-科学杂志1915>ChatGPT发展史：从基础神经元到多模态智能体 by 科学杂志1915</h2><div><section data-support=96编辑器 data-style-id=38112><section><section data-width=100%><p><img data-imgfileid=502916807 data-ratio=0.16666666666666666 data-src="https://mmbiz.qpic.cn/mmbiz_gif/Ljib4So7yuWjHlnxnDPeVdtvXwRcGrHjNWRvYDiapVJw7ZhbRsgIiag5YWzWIa4SlftxM21nxqW1LHyicfec4b9B2Q/640?wx_fmt=gif" data-w=600 data-width=100% src="https://mmbiz.qpic.cn/mmbiz_gif/Ljib4So7yuWjHlnxnDPeVdtvXwRcGrHjNWRvYDiapVJw7ZhbRsgIiag5YWzWIa4SlftxM21nxqW1LHyicfec4b9B2Q/640?wx_fmt=gif"></p></section></section></section><section data-support=96编辑器 data-style-id=38404><section><section data-support=96编辑器 data-style-id=24324><p>1</p><section><section><br></section><section><p>本文从基本概念出发，介绍和解释ChatGPT用到的一系列关键技术，如机器学习、神经网络、大模型、预训练+微调范式、Scaling Law……并对ChatGPT未来可能应用领域的多模态智能体(agent)进行展望。希望帮助读者更为深入地了解和使用以ChatGPT为代表的相关工具，助力读者成为人工智能时代的弄潮儿。</p></section><section><br></section></section></section><p><br></p><p><img data-imgfileid=502916810 data-ratio=0.6666666666666666 data-src="https://mmbiz.qpic.cn/mmbiz_jpg/s76ZjTU2fwRgcd3zibhWElJbuOohPflUVeYfZtSZljTCxugO5ibQpVdsJ8lVHwxskYbgdqVpO6ibia8nrUgh3YDfzg/640?wx_fmt=jpeg&amp;from=appmsg" data-type=jpeg data-w=1080 src="https://mmbiz.qpic.cn/mmbiz_jpg/s76ZjTU2fwRgcd3zibhWElJbuOohPflUVeYfZtSZljTCxugO5ibQpVdsJ8lVHwxskYbgdqVpO6ibia8nrUgh3YDfzg/640?wx_fmt=jpeg&amp;from=appmsg"></p><p><br></p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>2022年11月30日，一家名不见经传的公司(OpenAI)悄悄上线了一个产品ChatGPT。彼时，谁也没有想到这款产品会在短短几个月内风靡全球;而2023年3月14日GPT-4的发布更是激起了一场属于生成式人工智能(artificial intelligence generated content, AIGC)的科技革命。对于普通人来说，面对这个正在给生产和生活带来巨大改变的人工智能产品，不禁会产生无数的疑问：</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p><span>●ChatGPT为什么引起如此大的重视?</span></p><p><span>●它的原理是什么?</span></p><p><span>●它真的具备人类的智慧吗?</span></p><p><span>●它将给人类社会带来哪些变化?</span></p><p><span>……</span></p></section></section><section data-support=96编辑器 data-style-id=38387><section><section><section><br></section><section><section><p>ChatGPT原理概览：<br>文字接龙游戏</p></section></section><section><br></section></section></section></section><section data-support=96编辑器 data-style-id=38404><section><p>ChatGPT最令人印象深刻的能力是它能够通过对话的方式回答用户的问题，那么ChatGPT回答问题的原理是什么呢?传统的问答系统本质上是基于数据库和搜索引擎，即通过搜索引擎在网络与数据库中搜索相关信息，然后把结果直接返回给用户。比如我们使用百度搜索“机器学习的原理是什么”，百度会跳转出各式各样的网站。这些网站是由各个企业早就开发好的，百度仅仅是根据相关度做了一个排序。</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>不同于传统问答系统中答案来源于现成的网络或者数据库，ChatGPT的回答是随着提问的进行自动生成的。这一点有点像文字接龙游戏，ChatGPT会基于前面的话不断地生成下一个合适的词汇，直到觉得不必继续生成为止。</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>比如我们问ChatGPT：“苹果是一种水果吗”，ChatGPT会基于这句话进行文字接龙，大概流程如下：</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>(1)考虑下一个可能的词汇及其对应的概率，如右表(为了方便理解只写了3个可能的形式)所示。</p><p><br></p><p><img data-imgfileid=502916808 data-ratio=0.532058492688414 data-src="https://mmbiz.qpic.cn/mmbiz_jpg/s76ZjTU2fwRgcd3zibhWElJbuOohPflUViaHnI31ZZZCGSyE4WM70F3kq3BUgBDYSwfeKluiaJ535MRTqW3qRxelg/640?wx_fmt=jpeg&amp;from=appmsg" data-type=jpeg data-w=889 data-width=41.5217% src="https://mmbiz.qpic.cn/mmbiz_jpg/s76ZjTU2fwRgcd3zibhWElJbuOohPflUViaHnI31ZZZCGSyE4WM70F3kq3BUgBDYSwfeKluiaJ535MRTqW3qRxelg/640?wx_fmt=jpeg&amp;from=appmsg"></p><p><br></p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>(2)基于上述概率分布，ChatGPT会选择概率最大的答案，即“是的”(因为其概率0.8明显大于其他选项)。</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>(3)此时这句话的内容变成 “苹果是一种水果么?是的”，ChatGPT会看下一个可能的词和对应概率是什么。</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>不断重复这个步骤，直到得到一个完整的回答。</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>从上面例子可以看出：</p><p>(1)不同于传统问答基于数据库或搜索引擎，ChatGPT的答案是在用户输入问题以后，随着问题自动生成的。</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>(2)这种生成本质上是在做文字接龙，简单来说是不断在所有可能词汇中选择概率最大的词汇来生成。</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>有些聪明的读者会有一个疑问，ChatGPT是怎么知道该选择什么词汇，又是如何给出各个可能词汇的概率呢?这正是机器学习技术的神奇之处。</p></section></section><section data-support=96编辑器 data-style-id=38387><section><section><section><br></section><section><section><p>机器学习的核心：<br>模仿人类进行学习</p></section></section><section><br></section></section></section></section><section data-support=96编辑器 data-style-id=38404><section><p>ChatGPT是机器学习的一个非常典型的应用，那么什么是机器学习呢?</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>机器学习整体思想是借鉴人类学习的过程。人类观察、归纳客观世界的实际情况，并从中学到相关的规律，当面对某一未知情况的时候，会使用已经学到的规律来解决未知的问题。同理，我们希望计算机能够自动地从海量数据中发现某种“规律”，并将这种规律应用于一些新的问题。这种规律在机器学习领域就被称为“模型”，学习的过程被称为对模型进行训练。</p><p><br></p><p><img data-imgfileid=502916809 data-ratio=0.3990740740740741 data-src="https://mmbiz.qpic.cn/mmbiz_jpg/s76ZjTU2fwRgcd3zibhWElJbuOohPflUVqmzOTjby49aLQLE39nKssTyOujWTfJrzUqLkHtZiceGkHWejic7ZRavA/640?wx_fmt=jpeg&amp;from=appmsg" data-type=jpeg data-w=1080 src="https://mmbiz.qpic.cn/mmbiz_jpg/s76ZjTU2fwRgcd3zibhWElJbuOohPflUVqmzOTjby49aLQLE39nKssTyOujWTfJrzUqLkHtZiceGkHWejic7ZRavA/640?wx_fmt=jpeg&amp;from=appmsg"></p><p><span><strong><span>机器学习和模型训练卡通示意图</span></strong></span></p><p><br></p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>关于模型训练，实际上所有机器学习模型背后都有一个假设：学习的规律是能够通过数学表示的。机器学习的核心就是想办法找到一个数学函数，让这个函数尽可能接近真实世界的数学表达式。然而很多时候人类并不知道真实的数学表示是什么形式，也无法通过传统数学推导的方式获得;人类唯一拥有的是一堆来源于真实情境的数据。机器学习的方法就是使用这些数据(训练数据)去训练我们的模型，让模型自动找到一个较好的近似结果。比如人脸识别的应用，就是想找到一个函数，这个函数的输入是人脸照片，输出是判定这张照片对应哪个人。然而人类不知道人脸识别函数是什么形式，于是就拿来一大堆人脸的照片并且标记好每个脸对应的人，交给模型去训练，让模型自动找到一个较好的人脸识别函数。这就是机器学习在做的事情。</p></section></section><section data-support=96编辑器 data-style-id=38387><section><section><section><br></section><section><section><p>神经网络与神经元：<br>可扩展的数学表达能力</p></section></section><section><br></section></section></section></section><section data-support=96编辑器 data-style-id=38404><section><p>理解了机器学习是什么，另一个概念是机器学习模型的数学表达能力。机器学习模型本质上是想要尽可能接近真实世界对应的那个函数。然而正如我们不能指望仅仅通过几条直线就画出精美绝伦的美术作品，如果机器学习模型本身比较简单，比如高中学到的线性函数</p><p><span><em>Y</em>=<em>kx</em>+<em>b</em></span></p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>那么它无论如何也不可能学习出一个复杂的函数。因此机器学习模型的一个重要考虑点就是模型的数学表达能力，当面对一个复杂问题的时候，我们希望模型数学表达能力尽可能强，这样模型才有可能学好。</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>过去几十年科学家发明了非常多不同的机器学习模型，而其中最具影响力的是一种叫作“神经网络”的模型。神经网络模型最初基于生物学的一个现象：人类神经元的基础架构非常简单，只能做一些基础的信号处理工作，但最终通过大脑能够完成复杂的思考。受此启发，科学家们开始思考是否可以构建一些简单的“神经元”，并通过神经元的连接形成网络，从而产生处理复杂信息的能力。</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>基于此，神经网络的基础单元是一个神经元的模型，其只能进行简单的计算。假设输入数据有2个维度(<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>)，那么这个神经网络可以写成</p><p><span><em>y</em>=<em>σ</em>(<em>w</em><sub>1</sub><em>x</em><sub>1</sub>+<em>w</em><sub>2</sub><em>x</em><sub>2</sub>+<em>b</em>)</span></p><p><span><br></span></p><p><img data-imgfileid=502916811 data-ratio=1.424074074074074 data-src="https://mmbiz.qpic.cn/mmbiz_jpg/s76ZjTU2fwRgcd3zibhWElJbuOohPflUVOVOgB2jibnibh42RyQNfcaEscBg9lNou1kHKcJP2oRhxHwTWVSBa1kqw/640?wx_fmt=jpeg&amp;from=appmsg" data-type=jpeg data-w=1080 src="https://mmbiz.qpic.cn/mmbiz_jpg/s76ZjTU2fwRgcd3zibhWElJbuOohPflUVOVOgB2jibnibh42RyQNfcaEscBg9lNou1kHKcJP2oRhxHwTWVSBa1kqw/640?wx_fmt=jpeg&amp;from=appmsg"></p><p><strong><span>从神经元到神经网络</span></strong><span>  (a)神经元架构（生物）；(b)基础神经元架构（人工智能）；(c)简单多层感知机。</span><br></p><p><span><br></span></p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>上述神经元的数学表达能力非常弱，只是一个简单的线性函数和一个激活函数的组合;但是我们可以很轻松地把模型变得强大起来，方案就是增加更多的“隐藏节点”。在这个时候虽然每个节点依然进行非常简单的计算，但组合起来其数学表达能力就会变得很强。感兴趣的读者可以尝试类比上述公式写出下图中简单多层感知机对应的公式，将会得到一个非常复杂的公式。这个模型也是日后深度学习的基础模型，即多层感知机<sup>[1]</sup>。</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>多层感知机的原理非常简单，但是透过它可以很好地了解神经网络的原理：虽然单个神经元非常简单，但是通过大量节点的组合就可以让模型具备非常强大的数学表达能力。而之后整个深度学习的技术路线，某种程度上就是沿着开发并训练更大更深的网络的路线前进的。</p></section></section><section data-support=96编辑器 data-style-id=38387><section><section><section><br></section><section><section><p>深度学习新范式：<br>预训练+微调范式与Scaling Law</p></section></section><section><br></section></section></section></section><section data-support=96编辑器 data-style-id=38404><section><p>深度学习领域从2012年开始蓬勃发展，更大更深且效果更好的模型不断出现。然而随着模型越来越复杂，从头训练模型的成本越来越高。于是有人提出，能否不从头训练，而是在别人训练好的模型基础上训练，从而用更低的成本达到更好的效果呢?</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>例如，科学家对一个图像分类模型进行拆分，希望研究深度学习模型里的那么多层都学到了什么东西<sup>[2]</sup>。结果发现，越接近输入层，模型学到的是越基础的信息，比如边、角、纹理等;越接近输出层，模型学到的是越接近高级组合的信息，比如公鸡的形状、船的形状等。不仅仅在图像领域如此，在自然语言、语音等很多领域也存在这个特征。</p><p><br></p><p><img data-imgfileid=502916815 data-ratio=0.462037037037037 data-src="https://mmbiz.qpic.cn/mmbiz_jpg/s76ZjTU2fwRgcd3zibhWElJbuOohPflUV8MLBibp24Nib89OE2gGr3UgR4uQyyJmjyrJlnf7p7FtPHGqosslILqnw/640?wx_fmt=jpeg&amp;from=appmsg" data-type=jpeg data-w=1080 src="https://mmbiz.qpic.cn/mmbiz_jpg/s76ZjTU2fwRgcd3zibhWElJbuOohPflUV8MLBibp24Nib89OE2gGr3UgR4uQyyJmjyrJlnf7p7FtPHGqosslILqnw/640?wx_fmt=jpeg&amp;from=appmsg"></p><p><strong><span>深度神经网络中不同层的输出</span></strong><span>  接近输入层（左侧）一般是基础信息，接近输出层（右侧）一般是某个具体的物体等信息<sup>[2]</sup>。</span><br></p><p><br></p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>基础信息往往是领域通用的信息，比如图像领域的边、角、纹理等，在各类图像识别中都会用到;而高级组合信息往往是领域专用信息，比如猫的形状只有在动物识别任务中才有用，在人脸识别的任务就没用。因此一个自然而然的逻辑是，通过领域常见数据训练出一个通用的模型，主要是学好领域通用信息;在面对某个具体场景时，只需要使用该场景数据做个小规模训练(微调)就可以了。这就是著名的预训练+微调的范式。</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>预训练+微调这一范式的出现与普及对领域产生了两个重大影响。一方面，在已有模型基础上微调大大降低了成本;另一方面，一个好的预训练模型的重要性也更加凸显，因此各大公司、科研机构更加愿意花大量成本来训练更加昂贵的基础模型。那么大模型的效果到底与什么因素有关呢?OpenAI在2020年提出了著名的Scaling Law，即当模型规模变大以后，模型的效果主要受到模型参数规模、训练数据规模和使用算力规模影响<sup>[3]</sup>。</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>Scaling Law积极的一面是为提升模型效果指明了方向，只要把模型和数据规模做得更大就可以，这也是为什么近年来大模型的规模在以指数级增长，以及基础算力资源图形处理器(graphics processing unit,  GPU)总是供不应求;但Scaling Law也揭示了一个让很多科学家绝望的事实：即模型的每一步提升都需要人类用极为夸张的算力和数据成本来“交换”。大模型的成本门槛变得非常之高，从头训练大模型成了学界的奢望，以OpenAI、谷歌、Meta、百度、智谱AI等企业为代表的业界开始发挥引领作用。</p><p><br></p><p><img data-imgfileid=502916814 data-ratio=0.5009259259259259 data-src="https://mmbiz.qpic.cn/mmbiz_jpg/s76ZjTU2fwRgcd3zibhWElJbuOohPflUVnr30HfATcxnAb70TjW4KnqBvdm9MhiadNVgrp7lfokibFXibg0niaDCPEQ/640?wx_fmt=jpeg&amp;from=appmsg" data-type=jpeg data-w=1080 src="https://mmbiz.qpic.cn/mmbiz_jpg/s76ZjTU2fwRgcd3zibhWElJbuOohPflUVnr30HfATcxnAb70TjW4KnqBvdm9MhiadNVgrp7lfokibFXibg0niaDCPEQ/640?wx_fmt=jpeg&amp;from=appmsg"></p><p><br></p><p><br></p></section></section><section data-support=96编辑器 data-style-id=38387><section><section><section><br></section><section><section><p>GPT的野心：<br>上下文学习与提示词工程</p></section></section><section><br></section></section></section></section><section data-support=96编辑器 data-style-id=38404><section><p>除了希望通过训练规模巨大的模型来提升效果以外，GPT模型在发展过程中还有一个非常雄大的野心：上下文学习(in-context learning)。</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>正如前文所述，在过去如果想要模型“学”到什么内容，需要用一大堆数据来训练我们的模型;哪怕是前文讲到的预训练+微调的范式，依然需要在已训练好的模型基础上，用一个小批量数据做训练(即微调)。因此在过去，“训练”一直是机器学习中最核心的概念。但OpenAI提出，训练本身既有成本又有门槛，希望模型面对新任务的时候不用额外训练，只需要在对话窗口里给模型一些例子，模型就自动学会了。这种模式就叫作上下文学习。</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>举一个中英文翻译的例子。过去做中英文翻译，需要使用海量的中英文数据集训练一个机器学习模型;而在上下文学习中，想要完成同样的任务，只需要给模型一些例子，比如告诉模型下面的话：</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p><span>下面是一些中文翻译成英文的例子：</span></p><p><span>我爱中国 → I love China</span></p><p><span>我喜欢写代码 → I love coding</span></p><p><span>人工智能很重要 → AI is important</span></p><p><span>现在我有一句中文，请翻译成英文。这句话是：“我今天想吃苹果”。</span></p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>这时候原本“傻傻的”模型就突然具备了翻译的能力，能够自动翻译了。</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>有过ChatGPT使用经历的读者会发现，这个输入就是提示词(prompt)。在ChatGPT使用已相当普及的今天，很多人意识不到这件事有多神奇。这就如同找一个没学过英语的孩子，给他看几个中英文翻译的句子，这个孩子就可以流畅地进行中英文翻译了。要知道这个模型可从来没有专门在中英文翻译的数据集上训练过，也就是说模型本身并没有中英文翻译的能力，但它竟然通过对话里的一些例子就突然脱胎换骨“顿悟”了中英文翻译，这真的非常神奇!</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>上下文学习的相关机制到今天依然是学界讨论的热点，而恰恰因为GPT模型具有上下文学习的能力，一个好的提示词非常重要。提示词工程逐步成为一个热门的领域，甚至出现了一种新的职业叫作“提示词工程师”(prompt engineer)，就是通过写出更好的提示词让ChatGPT发挥更大的作用。</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>ChatGPT原理总结如下：</p><p>(1) ChatGPT本质是在做文字接龙的游戏，在游戏中它会根据候选词汇的概率来挑选下一个词。</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>(2) ChatGPT背后是一个非常庞大的神经网络，比如GPT-3有1700亿个参数(训练成本在100万美元以上)。</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>(3)基于庞大的神经网络，面对一句话时，模型可以准确给出候选词汇的概率，从而完成文字接龙的操作。</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>(4)这种有巨大规模进行语言处理的模型，也叫作大语言模型(large language model)。</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>(5)以GPT为代表的大语言模型具备上下文学习的能力，因此一个好的提示词至关重要。</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>理解了ChatGPT的原理，相信有读者会进一步提问：ChatGPT虽然很神奇但终归是一个语言模型，为什么大家对它抱有如此高的期望呢?</p></section></section><section data-support=96编辑器 data-style-id=38387><section><section><section><br></section><section><section><p>ChatGPT的未来：<br>多模态的AI智能体</p></section></section><section><br></section></section></section></section><section data-support=96编辑器 data-style-id=38404><section><p>为了解ChatGPT为什么引起如此高的关注，我们可以回顾人类文明发展历史上公认的3次工业革命。</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>(1)第一次工业革命以瓦特蒸汽机为代表，其本质是发明并使用一些简单机器来解放人类的体力劳动。</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>(2)第二次工业革命以内燃机和电力的使用为代表，其本质是能够使用各种能源来解决机器的动力问题。</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>(3)第三次工业革命以电子和信息技术为代表，其本质是通过电子信息的方式加快信息收集、传输、处理的效率，并进一步优化了对机器的控制。</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>工业革命的本质在于用机器替代人类劳动从而解放生产力。前3次分别从机器、动力(能源)和控制(电子和信息)的角度解放了人类劳动，人类已经能够通过简单的方式来指挥机器生产。此时的生产逻辑是人类的大脑对外部信息做判断，机器根据人类大脑的判断来具体执行动作。这之后人类文明面对的最直接的问题就是如何将大脑也解放出来。这个问题成为人工智能研究中最核心的问题，即面对一个复杂的问题，能够同领域专家一样快速正确地做出回应——这恰好就是ChatGPT的能力所在。</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>表面上看以ChatGPT为代表的大语言模型的能力是正确回答问题，实际上它可以像人类大脑一样对复杂问题进行准确的决策，这就打通了人类技术的所有环节。比如在自动化实验领域，过去我们花了很多时间研究可编程机器人和机器人的精确控制，希望能够用机器人取代人类做一些科学实验，但是最后发现终归还是要由科学家来确定具体合成实验的操作，并给机器人详细编码(硬件或者软件方式)。有了ChatGPT，科学家只需要说出自己的需求，ChatGPT会自动在文献库里搜索相关材料配方，然后编写相关机器人指令，并指挥机器人自动合成相关材料，从而实现真正意义上的自动化实验<sup>[5]</sup>。这种能够自动感知环境、做出决策并采取行动的AI机器人，被称为AI智能体(agent)。</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>既然我们希望AI智能体能够处理现实生活中的复杂情况，那么传统大语言模型只能通过语言进行对话的能力就不够了。AI智能体要求模型能够读取各式各样的输入，比如图像信息、声音信息、不同传感器的信息、互联网的信息等，并能够根据智能体做出的决策再反过来进行合适的动作，比如输出一张图像或一段声音、写一段代码、操作机器人完成某项动作等。这种能够使用多种不同类型的输入或输出模式来处理信息和任务的能力，叫作多模态;而具备这种能力的智能体，被称为多模态AI智能体。以ChatGPT为代表的大语言模型的完善实际上为多模态AI智能体提供了强大的“大脑”，人类过去相关科学技术将会以前所未有的速度串联起来，相信《钢铁侠》(Iron Man)里的人工智能助理贾维斯(J.A.R.V.I.S.)会在不远的将来成为现实。</p></section></section><section data-support=96编辑器 data-style-id=38404><section><section data-support=96编辑器 data-style-id=38387><section><section><section><br></section><section><section><p>再看ChatGPT：<br>大语言模型真的具有智慧吗</p></section></section><section><br></section></section></section></section></section></section><section data-support=96编辑器 data-style-id=38404><section><p>对ChatGPT的介绍已经接近尾声，然而随着ChatGPT出现，另一个问题引起了广泛争议，那就是以ChatGPT为代表的大语言模型真的具有智慧甚至自我意识吗?这个问题实际上见仁见智，比如一部分学者认为ChatGPT本质上就是文字接龙的概率游戏，根本没有什么智慧可言，更不用说是自我意识;而另一部分学者则认为大语言模型庞大的参数里可能蕴含着一些对人类知识的理解，虽然不一定到“意识”的程度，但可以认为已经具备了一些智慧。</p></section></section><section data-support=96编辑器 data-style-id=38404><section><p>这些争论的本质实际上是因为在深度学习领域，实践应用已经远远领先于模型理论的研究。展望未来，一方面希望看到更多更先进的人工智能算法与应用的落地，从而更好地方便我们的生活;另一方面也希望人类能够更加了解我们创造出来的人工智能模型，从而打造出更加安全可靠的模型，让人工智能真正助力人类社会的进步。</p></section></section><section data-support=96编辑器 data-style-id=38404><section><section data-support=96编辑器 data-style-id=37069><section><section><section><section><section><p><img data-imgfileid=502916812 data-ratio=1.2692307692307692 data-src="https://mmbiz.qpic.cn/mmbiz_png/xlnvrKyPG63IciajYoibUnTqAEzzPYYZuF5daHj65hU33O3AAySMK4kicO3mpFuvic31kPxtdhQTneOoxiawkuxVia1w/640?wx_fmt=png" data-w=26 data-width=100% src="https://mmbiz.qpic.cn/mmbiz_png/xlnvrKyPG63IciajYoibUnTqAEzzPYYZuF5daHj65hU33O3AAySMK4kicO3mpFuvic31kPxtdhQTneOoxiawkuxVia1w/640?wx_fmt=png"></p></section><section><br></section></section><section><section><section><p><span>作者简介</span></p></section></section></section></section></section><section><section><p>王一博：深势科技教学总监，北京科学智能研究院开源社区负责人，北京100080。wangyb@dp.tech</p><p>Wang Yibo: DPTechnology’s Director of the Teaching Division, Head of the open source community of AISI (AI for Science Institute, Beijing), Beijing 100080.</p></section></section><section><p><img data-imgfileid=502916813 data-ratio=0.23170731707317074 data-src="https://mmbiz.qpic.cn/mmbiz_png/xlnvrKyPG63IciajYoibUnTqAEzzPYYZuF4IicrbGPw02BpzWyWIqR5DuksmsNgXZn4jghe8YovvlTHRptnNVc4fA/640?wx_fmt=png" data-w=82 data-width=100% src="https://mmbiz.qpic.cn/mmbiz_png/xlnvrKyPG63IciajYoibUnTqAEzzPYYZuF4IicrbGPw02BpzWyWIqR5DuksmsNgXZn4jghe8YovvlTHRptnNVc4fA/640?wx_fmt=png"></p></section></section></section><p><br></p></section></section><section data-support=96编辑器 data-style-id=38404><section><p><span>[1] Rumelhart D E, Hinton E G, Williams R J. Learning representations by back-propagating errors. Nature, 1986, 323(6088): 533–536.</span></p></section></section><section data-support=96编辑器 data-style-id=38404><section><p><span>[2] Wei D L, Zhou B L, Torralba A，et al. mNeuron: A matlab plugin to visualize neurons from deep models. (2015)[2024-07-05]. https://donglaiw.github.io/proj/mneuron/index.html.</span></p></section></section><section data-support=96编辑器 data-style-id=38404><section><p><span>[3] Kaplan J, McCandlish S, Henighan T, et al. Scaling laws for neural language models. arXiv preprint arXiv: 2001.08361, 2020.</span></p></section></section><section data-support=96编辑器 data-style-id=38404><section><p><span>[4] Zhao W X, Zhou K, Li J, et al. A survey of large language models. arXiv preprint arXiv: 2303.18223, 2023.</span></p></section></section><section data-support=96编辑器 data-style-id=38404><section><p><span>[5] Boiko D A, MacKnight R, Kline B, et al. Autonomous chemical research with large language models. Nature, 2023, 624(7992): 570-578.</span></p></section></section><section data-support=96编辑器 data-style-id=38404><section><p><span>关键词：ChatGPT   机器学习   神经网络   大模型   多模态智能体</span><span>■</span></p><p><span><br></span></p></section></section><section><img data-backh=26 data-backw=578 data-fileid=502909649 data-imgfileid=502916819 data-ratio=0.0453125 data-s=300,640 data-src="https://mmbiz.qpic.cn/mmbiz_jpg/s76ZjTU2fwRX3DGcgFicyLUXDBb8pXm7DRwZTibg3G7D3wjMgAOLS94zeavedhp1gBqqTURUJOKCpjicwNROyiadsg/640?wx_fmt=other&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" data-type=jpeg data-w=640 src="https://mmbiz.qpic.cn/mmbiz_jpg/s76ZjTU2fwRX3DGcgFicyLUXDBb8pXm7DRwZTibg3G7D3wjMgAOLS94zeavedhp1gBqqTURUJOKCpjicwNROyiadsg/640?wx_fmt=other&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1"></section><section><br></section><section><span>  本文刊载于2024年第76卷第5期《科学》杂志（P45-P49）</span></section><section data-support=96编辑器 data-style-id=26790><section><section data-support=96编辑器 data-style-id=29088><section><p><img data-imgfileid=502916822 data-ratio=0.06833333333333333 data-src="https://mmbiz.qpic.cn/mmbiz_gif/Ljib4So7yuWiaOaXgAhSRtJzxejaOKF2UicUXX6zkG0dl13R44jqnFK4mYyhhZjtIiaxEyeTXSnGqLpedIKBlXRcBw/640?wx_fmt=gif&amp;wxfrom=5&amp;wx_lazy=1&amp;tp=webp" data-w=600 src="https://mmbiz.qpic.cn/mmbiz_gif/Ljib4So7yuWiaOaXgAhSRtJzxejaOKF2UicUXX6zkG0dl13R44jqnFK4mYyhhZjtIiaxEyeTXSnGqLpedIKBlXRcBw/640?wx_fmt=gif&amp;wxfrom=5&amp;wx_lazy=1&amp;tp=webp"></p></section></section></section></section><section data-support=96编辑器 data-style-id=50262><section><section><p><span><strong>《科学》</strong>杂志被评定为</span></p><p><span>2022年度</span></p><p><span><strong>中国人文社会科学期刊</strong><strong>AMI综合评价</strong>（A刊）<strong>扩展期刊</strong></span></p><section data-width=90%><p><img data-imgfileid=502916823 data-ratio=0.7074074074074074 data-src="https://mmbiz.qpic.cn/mmbiz_jpg/s76ZjTU2fwTwuYzdyCic4Y4CVAJVypvqicuQlzJMtt4yMenhWVVic1YIfuhicGbjOE3j8JfuQSNbSXdGlmEm7u8gfg/640?wx_fmt=other&amp;from=appmsg&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1&amp;tp=webp" data-type=jpeg data-w=1080 data-width=100% src="https://mmbiz.qpic.cn/mmbiz_jpg/s76ZjTU2fwTwuYzdyCic4Y4CVAJVypvqicuQlzJMtt4yMenhWVVic1YIfuhicGbjOE3j8JfuQSNbSXdGlmEm7u8gfg/640?wx_fmt=other&amp;from=appmsg&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1&amp;tp=webp"></p></section><section><br></section></section></section></section><section data-support=96编辑器 data-style-id=23871><section><span>本刊已被中国知网、万方数字化期刊库、维普网、超星、龙源期刊网等多家数据库收录，并在今日头条上也有不定期转载，《科学》的今日头条账号昵称也是“科学杂志1915”。实体纸刊在中国邮政报刊订阅商城官网、杂志铺官网有售。</span></section><section><section data-width=100%><section data-width=47%><section><br></section></section><section><p><img data-imgfileid=502916821 data-ratio=1.0307692307692307 data-w=130 data-src="https://mmbiz.qpic.cn/mmbiz_gif/Ljib4So7yuWiaKhMsMaVnDCANQowkLnx4SvTugIJpXo8rqsXDMvaj7Em080GhMD6kwA7ZKyluvoXbpZicQ0j7vhuw/640?wx_fmt=gif&amp;wxfrom=5&amp;wx_lazy=1&amp;tp=webp" src="https://mmbiz.qpic.cn/mmbiz_gif/Ljib4So7yuWiaKhMsMaVnDCANQowkLnx4SvTugIJpXo8rqsXDMvaj7Em080GhMD6kwA7ZKyluvoXbpZicQ0j7vhuw/640?wx_fmt=gif&amp;wxfrom=5&amp;wx_lazy=1&amp;tp=webp"></p></section><section data-width=47%><section><br></section></section></section></section></section><section><span><br></span></section><section><span>《科学》杂志于1915年1月在上海问世，</span><br data-filtered=filtered></section><section><span> 早年由任鸿隽，杨杏佛，胡明复，赵元任等学者编辑写作，</span></section><section><span>是我国历史最长的综合性科学刊物。</span></section><section><span>杂志定位为高级科普期刊，致力于科学知识、理念和科学精神的传播，科学与人文互动，历史和前沿并举，为提升我国全民科学素质和建设创新型国家服务。</span><span>杂志现任主编为中国科学院院士白春礼先生，主办单位为上海科学技术出版社有限公司<span>。</span></span></section><section><br></section><section><span>购买《科学》杂志<br data-filtered=filtered>请拨打上海科学技术出版社邮购部电话：</span><br data-filtered=filtered><span><strong><span>021-53203260</span></strong></span><br data-filtered=filtered><span>联系人：王先生<br data-filtered=filtered>工作时间：周一到周五，8:30-16:00</span></section><p><br></p><p><img data-imgfileid=502916820 data-ratio=1 data-s=300,640 data-src="https://mmbiz.qpic.cn/mmbiz_jpg/s76ZjTU2fwSeCD6CoM6nFZ38B7kvibW0Ox52W4demvfv28cibnia5haEAUXTsGlI1jfgwibQqg0Tl3Jq9HhGHHRjJA/640?wx_fmt=other&amp;from=appmsg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" data-type=jpeg data-w=500 src="https://mmbiz.qpic.cn/mmbiz_jpg/s76ZjTU2fwSeCD6CoM6nFZ38B7kvibW0Ox52W4demvfv28cibnia5haEAUXTsGlI1jfgwibQqg0Tl3Jq9HhGHHRjJA/640?wx_fmt=other&amp;from=appmsg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1"></p><section><span>长按识别二维码</span></section><section><span>订阅《科学》</span></section><p><br></p><section><img data-backh=39 data-backw=578 data-fileid=502909650 data-imgfileid=502916825 data-ratio=0.0671875 data-s=300,640 data-src="https://mmbiz.qpic.cn/mmbiz_jpg/s76ZjTU2fwRX3DGcgFicyLUXDBb8pXm7DcumicaRibDSLXH425kP4EmJ58QcxCWnUaTVcWZCibP7yJqd796D9XNy9Q/640?wx_fmt=other&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1&amp;tp=webp" data-type=jpeg data-w=640 src="https://mmbiz.qpic.cn/mmbiz_jpg/s76ZjTU2fwRX3DGcgFicyLUXDBb8pXm7DcumicaRibDSLXH425kP4EmJ58QcxCWnUaTVcWZCibP7yJqd796D9XNy9Q/640?wx_fmt=other&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1&amp;tp=webp"></section><section><br></section><section><img data-cropselx1=0 data-cropselx2=106 data-cropsely1=0 data-cropsely2=106 data-fileid=502909651 data-imgfileid=502916827 data-ratio=1 data-s=300,640 data-src="https://mmbiz.qpic.cn/mmbiz_jpg/s76ZjTU2fwR0wyh0d8JnzMUBw2HqYEpHnZkJY8kNDoVgKNaJEDWr1vGSxRY1odWTkuYhxMibO69vDldChxsOffg/640?wx_fmt=other&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1&amp;tp=webp" data-type=jpeg data-w=364 src="https://mmbiz.qpic.cn/mmbiz_jpg/s76ZjTU2fwR0wyh0d8JnzMUBw2HqYEpHnZkJY8kNDoVgKNaJEDWr1vGSxRY1odWTkuYhxMibO69vDldChxsOffg/640?wx_fmt=other&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1&amp;tp=webp"></section><section><span>投稿邮箱：kexuemag@sstp.cn</span></section><p><span>《科学》编辑部收件地址：</span></p><p><span>上海市闵行区号景路159弄世纪出版园A座1006室</span></p><p><span>邮编：201101</span></p><section><span>编辑部电话：021-53203068</span></section><section><br></section><section><section><section><span><a href="http://mp.weixin.qq.com/s?__biz=MzA5OTEwNzg3OQ==&amp;mid=2650387905&amp;idx=1&amp;sn=38f9b35aaa2d984efcd22381a673473c&amp;chksm=888a4089bffdc99f5df6c76606242fb27ec60a7c947274ddeb27da7358b0378964c97dbdff34&amp;scene=21#wechat_redirect" target=_blank data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA5OTEwNzg3OQ==&amp;mid=2650387905&amp;idx=1&amp;sn=38f9b35aaa2d984efcd22381a673473c&amp;chksm=888a4089bffdc99f5df6c76606242fb27ec60a7c947274ddeb27da7358b0378964c97dbdff34&amp;scene=21#wechat_redirect" data-linktype=2 wah-hotarea=click>◎本文由《科学》杂志原创，转载请注明详情</a></span></section></section></section><section><span> </span><a href="http://mp.weixin.qq.com/s?__biz=MzA5OTEwNzg3OQ==&amp;mid=2650387905&amp;idx=1&amp;sn=38f9b35aaa2d984efcd22381a673473c&amp;chksm=888a4089bffdc99f5df6c76606242fb27ec60a7c947274ddeb27da7358b0378964c97dbdff34&amp;scene=21#wechat_redirect" target=_blank data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA5OTEwNzg3OQ==&amp;mid=2650387905&amp;idx=1&amp;sn=38f9b35aaa2d984efcd22381a673473c&amp;chksm=888a4089bffdc99f5df6c76606242fb27ec60a7c947274ddeb27da7358b0378964c97dbdff34&amp;scene=21#wechat_redirect" data-linktype=2 wah-hotarea=click><span>此色块为转载声明链接，转载请先阅读</span></a><span>  </span></section><section><img data-backh=24 data-backw=578 data-before-oversubscription-url=https://mmbiz.qpic.cn/mmbiz_jpg/s76ZjTU2fwRX3DGcgFicyLUXDBb8pXm7DsbHFwg1Qib7w1O8hs3BWy1Pk24kMicEKFLzsqRw5LhjpSbQzFSwVMN8w/640? data-fileid=502909654 data-imgfileid=502916824 data-ratio=0.0421875 data-s=300,640 data-src="https://mmbiz.qpic.cn/mmbiz_jpg/s76ZjTU2fwRX3DGcgFicyLUXDBb8pXm7DsbHFwg1Qib7w1O8hs3BWy1Pk24kMicEKFLzsqRw5LhjpSbQzFSwVMN8w/640?wx_fmt=other&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1&amp;tp=webp" data-type=jpeg data-w=640 src="https://mmbiz.qpic.cn/mmbiz_jpg/s76ZjTU2fwRX3DGcgFicyLUXDBb8pXm7DsbHFwg1Qib7w1O8hs3BWy1Pk24kMicEKFLzsqRw5LhjpSbQzFSwVMN8w/640?wx_fmt=other&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1&amp;tp=webp"></section><section><span>《科学》2024年第</span><span><strong>76</strong></span><span>卷第<strong>5</strong>期已出版发行</span></section><p><br></p><p><img data-cropselx1=0 data-cropselx2=558 data-cropsely1=0 data-cropsely2=558 data-imgfileid=502916828 data-ratio=1 data-s=300,640 data-src="https://mmbiz.qpic.cn/mmbiz_jpg/s76ZjTU2fwTC6HaAVlZX3gbbGue7hPp5uIRpiaibXJPTyXHmck50I4mwhkQNDpmDV5c3VF0wfZEWqvXejVciaB66Q/640?wx_fmt=jpeg&amp;from=appmsg" data-type=jpeg data-w=1080 src="https://mmbiz.qpic.cn/mmbiz_jpg/s76ZjTU2fwTC6HaAVlZX3gbbGue7hPp5uIRpiaibXJPTyXHmck50I4mwhkQNDpmDV5c3VF0wfZEWqvXejVciaB66Q/640?wx_fmt=jpeg&amp;from=appmsg"></p><section><img data-imgfileid=502916826 data-ratio=0.21296296296296297 data-s=300,640 data-src="https://mmbiz.qpic.cn/mmbiz_jpg/s76ZjTU2fwTUTd9fIeJXelNc5BFZmkttjFtCkfbicic900VNBAoBWOeZbmlnSDQx0qFMbuC6GhlicLN6Ca1MUEGBg/640?wx_fmt=other&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1&amp;tp=webp" data-type=jpeg data-w=1080 src="https://mmbiz.qpic.cn/mmbiz_jpg/s76ZjTU2fwTUTd9fIeJXelNc5BFZmkttjFtCkfbicic900VNBAoBWOeZbmlnSDQx0qFMbuC6GhlicLN6Ca1MUEGBg/640?wx_fmt=other&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1&amp;tp=webp"></section><section><strong><img data-backh=39 data-backw=578 data-before-oversubscription-url="https://mmbiz.qpic.cn/mmbiz_jpg/s76ZjTU2fwRX3DGcgFicyLUXDBb8pXm7DcumicaRibDSLXH425kP4EmJ58QcxCWnUaTVcWZCibP7yJqd796D9XNy9Q/640?wx_fmt=jpeg" data-fileid=502909655 data-imgfileid=502916829 data-ratio=0.0671875 data-s=300,640 data-src="https://mmbiz.qpic.cn/mmbiz_jpg/s76ZjTU2fwRX3DGcgFicyLUXDBb8pXm7DcumicaRibDSLXH425kP4EmJ58QcxCWnUaTVcWZCibP7yJqd796D9XNy9Q/640?wx_fmt=other&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1&amp;tp=webp" data-type=jpeg data-w=640 src="https://mmbiz.qpic.cn/mmbiz_jpg/s76ZjTU2fwRX3DGcgFicyLUXDBb8pXm7DcumicaRibDSLXH425kP4EmJ58QcxCWnUaTVcWZCibP7yJqd796D9XNy9Q/640?wx_fmt=other&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1&amp;tp=webp"></strong><br></section><section><br></section><section><mp-common-profile data-id="MzA5OTEwNzg3OQ==" data-pluginname=mpprofile data-headimg="http://mmbiz.qpic.cn/mmbiz_png/s76ZjTU2fwQBA6p0WkC52ouYdOerKbYXmxXPibFic3ibnTlcl1u3XWDRwIRhrckTiaJfC7cGlReiaSKibr8wZHCKyJpQ/300?wx_fmt=png&amp;wxfrom=19" data-nickname=科学杂志1915 data-alias=kexuemag data-signature=《科学》1915年创刊，是我国历史最悠久的综合性科学刊物，她开启了中国科学传播事业的新时代。1985年第二次复刊的《科学》致力于全视野、综合性地介绍现代科学技术的前沿发展。名誉主编：周光召；主编：白春礼。 data-from=2 data-is_biz_ban=0 data-weui-theme=light data-origin_num=526 data-isban=0 data-biz_account_status=0 data-index=0></mp-common-profile></section><section><span>别忘了点赞 / 加关注哦!</span></section><p><mp-style-type data-value=3></mp-style-type></p></div><hr><a href=https://mp.weixin.qq.com/s/54YpunKLxWrLDiRoSjA5uA ,target=_blank rel="noopener noreferrer">原文链接</a></div><footer class=post__footer><div class="post__tags tags clearfix"><svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M4 0h8s2 0 4 2l15 15s2 2 0 4L21 31s-2 2-4 0L2 16s-2-2-2-4V3s0-3 4-3m3 10a3 3 0 000-6 3 3 0 000 6"/></svg><ul class=tags__list><li class=tags__item><a class="tags__link btn" href=../../../tags/fetched/ rel=tag>fetched</a></li><li class=tags__item><a class="tags__link btn" href=../../../tags/%E7%A7%91%E5%AD%A6%E6%9D%82%E5%BF%971915/ rel=tag>科学杂志1915</a></li></ul></div></footer></article></main><div class="authorbox clearfix"><div class=authorbox__header><span class=authorbox__name>About Bloger</span></div></div><nav class="pager flex"><div class="pager__item pager__item--prev"><a class=pager__link href=../../../posts/2024-11/%E5%A6%82%E4%BD%95%E5%9C%A8%E6%98%93%E9%80%9D%E7%9A%84%E6%B0%B4%E4%B8%8A%E5%86%99%E6%B0%B8%E6%81%92/ rel=prev><span class=pager__subtitle>«&#8201;Previous</span><p class=pager__title>如何在易逝的水上写永恒</p></a></div><div class="pager__item pager__item--next"><a class=pager__link href=../../../posts/2024-11/%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%88%B6%E5%BA%A6%E5%BB%BA%E8%AE%BE%E5%8E%86%E5%8F%B2%E7%9A%84%E7%AE%80%E7%95%A5%E5%9B%9E%E9%A1%BE/ rel=next><span class=pager__subtitle>Next&#8201;»</span><p class=pager__title>中国科学院制度建设历史的简略回顾</p></a></div></nav><section class=comments><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//kkitown.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section></div><aside class=sidebar><div class="widget-search widget"><form class=widget-search__form role=search method=get action=https://google.com/search><input class=widget-search__field type=search placeholder=Search… name=q aria-label=Search…>
<input class=widget-search__submit type=submit value=Search>
<input type=hidden name=sitesearch value=https://ixxmu.github.io/></form></div><div class="widget-recent widget"><h4 class=widget__title>Recent Posts</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=../../../posts/2025-09/%E9%97%B2%E9%B1%BC%E4%B8%8A%E9%82%A3%E4%BA%9B%E9%80%86%E5%A4%A9%E7%9A%84%E6%B5%8B%E5%BA%8F%E4%BB%AA/>闲鱼上那些逆天的测序仪</a></li><li class=widget__item><a class=widget__link href=../../../posts/2025-09/%E8%AF%B4%E8%B5%B7%E8%BE%9B%E8%8A%B7%E8%95%BE_%E6%88%91%E6%83%B3%E8%B5%B7%E6%97%A9%E5%B9%B4%E4%B8%80%E4%BA%9B%E5%93%81%E5%91%B3_%E7%8B%AC%E7%89%B9_%E7%9A%84%E6%81%90%E6%80%96%E7%89%87/>说起辛芷蕾，我想起早年一些品味“独特”的恐怖片</a></li><li class=widget__item><a class=widget__link href=../../../posts/2025-09/%E5%A4%9A%E6%AC%A1%E7%94%9F%E5%AD%90%E5%90%8E_28%E5%B2%81%E5%86%9C%E6%9D%91%E6%99%BA%E9%9A%9C%E5%A5%B3%E5%AD%A9%E6%82%84%E7%84%B6%E7%A6%BB%E4%B8%96/>多次生子后，28岁农村智障女孩悄然离世</a></li><li class=widget__item><a class=widget__link href=../../../posts/2025-09/openai%E7%BD%95%E8%A7%81%E5%8F%91%E8%AE%BA%E6%96%87_%E6%88%91%E4%BB%AC%E6%89%BE%E5%88%B0%E4%BA%86ai%E5%B9%BB%E8%A7%89%E7%9A%84%E7%BD%AA%E9%AD%81%E7%A5%B8%E9%A6%96/>OpenAI罕见发论文：我们找到了AI幻觉的罪魁祸首</a></li><li class=widget__item><a class=widget__link href=../../../posts/2025-09/%E5%8D%97%E4%BA%AC%E5%8D%96%E6%B7%AB%E5%A4%B4%E7%9B%AE1_34%E4%BA%BF%E6%B8%AF%E5%85%83%E8%A2%AB%E7%91%9E%E9%93%B6%E5%8E%9F%E5%89%AF%E6%80%BB%E7%9B%91%E7%A7%81%E5%90%9E/>南京卖淫头目1.34亿港元被瑞银原副总监私吞</a></li></ul></div></div><div class="widget-categories widget"><h4 class=widget__title>Categories</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=../../../categories/duty/>Duty</a></li><li class=widget__item><a class=widget__link href=../../../categories/duty2/>Duty2</a></li></ul></div></div><div class="widget-taglist widget"><h4 class=widget__title>Tags</h4><div class=widget__content><a class="widget-taglist__link widget__link btn" href=../../../tags/cnbeta/ title=Cnbeta>Cnbeta (148)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/datawhale/ title=Datawhale>Datawhale (9)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/drpei/ title=Drpei>Drpei (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/fetched/ title=Fetched>Fetched (755)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/githubdaily/ title=GitHubDaily>GitHubDaily (7)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E4%B8%81%E9%A6%99%E5%9B%AD/ title=丁香园>丁香园 (30)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E4%BA%BA%E7%89%A9/ title=人物>人物 (15)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E5%8D%97%E6%96%B9%E5%91%A8%E6%9C%AB/ title=南方周末>南方周末 (11)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E5%B0%8F%E4%BC%97%E8%BD%AF%E4%BB%B6/ title=小众软件>小众软件 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E5%B0%8F%E5%A4%A7%E5%A4%AB%E6%BC%AB%E7%94%BB/ title=小大夫漫画>小大夫漫画 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E5%B0%91%E6%95%B0%E6%B4%BE/ title=少数派>少数派 (6)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E5%BE%AA%E5%9B%A0%E7%BC%89%E8%8D%AF/ title=循因缉药>循因缉药 (9)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%96%B0%E4%B9%A1%E5%9C%9F/ title=新乡土>新乡土 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%98%9F%E7%90%83%E5%95%86%E4%B8%9A%E8%AF%84%E8%AE%BA/ title=星球商业评论>星球商业评论 (7)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%9C%88%E5%85%89%E5%8D%9A%E5%AE%A2/ title=月光博客>月光博客 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%9C%AA%E9%97%BBcode/ title=未闻Code>未闻Code (6)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%A7%BD%E8%BE%B9%E5%BE%80%E4%BA%8B/ title=槽边往事>槽边往事 (9)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%AF%8F%E6%97%A5%E4%BA%BA%E7%89%A9/ title=每日人物>每日人物 (10)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%B5%AA%E6%BD%AE%E5%B7%A5%E4%BD%9C%E5%AE%A4/ title=浪潮工作室>浪潮工作室 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%B5%AE%E4%B9%8B%E9%9D%99/ title=浮之静>浮之静 (6)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%81%BC%E8%A7%81/ title=灼见>灼见 (8)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%86%8A%E8%A8%80%E7%86%8A%E8%AF%AD/ title=熊言熊语>熊言熊语 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%94%9F%E4%BF%A1%E6%8A%80%E8%83%BD%E6%A0%91/ title=生信技能树>生信技能树 (10)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%94%9F%E4%BF%A1%E8%8F%9C%E9%B8%9F%E5%9B%A2/ title=生信菜鸟团>生信菜鸟团 (6)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%9F%A5%E8%AF%86%E5%88%86%E5%AD%90/ title=知识分子>知识分子 (14)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%BD%91%E6%98%93%E6%95%B0%E8%AF%BB/ title=网易数读>网易数读 (7)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E8%B4%A2%E6%96%B0/ title=财新>财新 (6)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E8%B4%A2%E7%BB%8F%E4%B8%89%E5%88%86%E9%92%9F/ title=财经三分钟>财经三分钟 (7)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E8%B5%9B%E5%85%88%E7%94%9F/ title=赛先生>赛先生 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E9%83%91%E5%B7%9E%E6%A5%BC%E5%B8%82/ title=郑州楼市>郑州楼市 (32)</a></div><a href=../../../tags/></a></div><div class="widget-social widget"><h4 class="widget-social__title widget__title">Social</h4><div class="widget-social__content widget__content"><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=Facebook rel="noopener noreferrer" href=https://facebook.com/username target=_blank><svg class="widget-social__link-icon icon icon-facebook" width="24" height="24" viewBox="0 0 352 352"><path d="m0 32v288c0 17.5 14.5 32 32 32h288c17.5.0 32-14.5 32-32V32c0-17.5-14.5-32-32-32H32C14.5.0.0 14.5.0 32zm320 0v288h-83V212h41.5l6-48H237v-31c0-14 3.5-23.5 23.5-23.5h26V66c-4.4-.6-19.8-1.5-37.5-1.5-36.9.0-62 22.2-62 63.5v36h-42v48h42v108H32V32z"/></svg>
<span>Facebook</span></a></div><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=Twitter rel="noopener noreferrer" href=https://twitter.com/username target=_blank><svg class="widget-social__link-icon icon icon-twitter" width="24" height="24" viewBox="0 0 384 312"><path d="m384 36.9c-14.1 6.3-29.3 10.5-45.2 12.4 16.3-9.7 28.8-25.2 34.6-43.6-15.2 9-32.1 15.6-50 19.1-14.4-15.2-34.9-24.8-57.5-24.8-43.5.0-78.8 35.3-78.8 78.8.0 6.2.7 12.2 2 17.9-65.5-3.3-123.5-34.6-162.4-82.3C20 26 16.1 39.6 16.1 54c0 27.3 13.9 51.4 35 65.6-12.9-.4-25.1-4-35.7-9.9v1c0 38.2 27.2 70 63.2 77.2-6.6 1.8-13.6 2.8-20.8 2.8-5.1.0-10-.5-14.8-1.4 10 31.3 39.1 54.1 73.6 54.7-27 21.1-60.9 33.7-97.8 33.7-6.4.0-12.6-.4-18.8-1.1C34.9 299 76.3 312 120.8 312c144.9.0 224.1-120 224.1-224.1.0-3.4-.1-6.8-.2-10.2 15.4-11.1 28.7-25 39.3-40.8z"/></svg>
<span>Twitter</span></a></div><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=Instagram rel="noopener noreferrer" href=https://www.instagram.com/username target=_blank><svg class="widget-social__link-icon icon icon-instagram" width="24" height="24" viewBox="0 0 256 256"><circle cx="193" cy="59" r="15"/><path fill-rule="evenodd" d="M101 0h54c41 0 58.4 3.9 74.5 17C256.2 37.5 256 74.8 256 97.7v60c0 26.7.0 60.4-26.5 81.4-16 13.4-33.5 16.9-74.5 16.9h-54c-41 0-57.5-3.5-74.5-16.9C1 218.9.5 186.3.1 160.5L0 155V97.7c0-23-.2-60.2 26.5-80.7C45 2 60 0 101 0zm4.9 23h44.3c45.8.0 58.3 3.5 70.3 17.5 11.8 13.2 12 30.1 12.5 62.9V156c.2 20.8.3 45.8-12.5 59.5-12 14-24.5 17.5-70.3 17.5h-44.3c-45.9.0-57.3-3.5-70.4-17.5-12.2-13-12.3-36.5-12.4-56.7v-55.6c.4-32.6.7-49.6 12.4-62.7C48 26.5 60 23 105.9 23zm19.6 144.5a42 42 0 100-84 42 42 0 000 84zm0 22.5a64.5 64.5.0 100-129 64.5 64.5.0 000 129z"/></svg>
<span>Instagram</span></a></div></div></div></aside></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2025 文字轨迹.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=../../../js/menu.js></script><script src=../../../js/custom.js></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script></body></html>