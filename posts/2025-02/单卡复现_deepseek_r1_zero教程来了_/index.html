<!doctype html><html class=no-js lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>单卡复现 DeepSeek R1 Zero教程来了！ - 文字轨迹</title>
<script>(function(e,t){e[t]=e[t].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:url" content="https://ixxmu.github.io/posts/2025-02/%E5%8D%95%E5%8D%A1%E5%A4%8D%E7%8E%B0_deepseek_r1_zero%E6%95%99%E7%A8%8B%E6%9D%A5%E4%BA%86_/"><meta property="og:site_name" content="文字轨迹"><meta property="og:title" content="单卡复现 DeepSeek R1 Zero教程来了！"><meta property="og:description" content="单卡复现 DeepSeek R1 Zero教程来了！ by Datawhale Datawhale干货 作者：邓恺俊，Datawhale成员项目代码可见：unlock-deepseek/Datawhale-R1（https://github."><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-15T15:18:45+00:00"><meta property="article:modified_time" content="2025-02-15T15:18:45+00:00"><meta property="article:tag" content="Fetched"><meta property="article:tag" content="Datawhale"><meta itemprop=name content="单卡复现 DeepSeek R1 Zero教程来了！"><meta itemprop=description content="单卡复现 DeepSeek R1 Zero教程来了！ by Datawhale Datawhale干货 作者：邓恺俊，Datawhale成员项目代码可见：unlock-deepseek/Datawhale-R1（https://github."><meta itemprop=datePublished content="2025-02-15T15:18:45+00:00"><meta itemprop=dateModified content="2025-02-15T15:18:45+00:00"><meta itemprop=wordCount content="460"><meta itemprop=keywords content="Fetched,Datawhale"><meta name=twitter:card content="summary"><meta name=twitter:title content="单卡复现 DeepSeek R1 Zero教程来了！"><meta name=twitter:description content="单卡复现 DeepSeek R1 Zero教程来了！ by Datawhale Datawhale干货 作者：邓恺俊，Datawhale成员项目代码可见：unlock-deepseek/Datawhale-R1（https://github."><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=../../../css/style.css><link rel=stylesheet href=../../../css/custom.css><link rel="shortcut icon" href=../../../favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=../../../ title=文字轨迹 rel=home><div class="logo__item logo__text"><div class=logo__title>文字轨迹</div><div class=logo__tagline>故事流淌过的地方</div></div></a></div><nav class=menu><button class=menu__btn aria-haspopup=true aria-expanded=false tabindex=0>
<span class=menu__btn-title tabindex=-1>Menu</span></button><ul class=menu__list><li class=menu__item><a class=menu__link href=../../../posts/><span class=menu__text>文章</span></a></li><li class=menu__item><a class=menu__link href=../../../tags/><span class=menu__text>标签</span></a></li><li class=menu__item><a class=menu__link href=../../../about/><span class=menu__text>关于</span></a></li></ul></nav></div></header><div class="wrapper flex"><div class=primary><main class=main role=main><article class=post><header class=post__header><meta name=referrer content="never"><h1 class=post__title>单卡复现 DeepSeek R1 Zero教程来了！</h1><div class="post__meta meta"><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0a14 14 0 110 28 1 1 0 010-28m0 3a3 3 0 100 22 3 3 0 000-22m1 4h-2v8.4l6.8 4.4L22 18l-6-3.8z"/></svg><time class=meta__text datetime=2025-02-15T15:18:45Z>2025-02-15</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=../../../categories/duty/ rel=category>Duty</a></span></div><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 16 16"><path d="M8 1c2 0 3.5 2 3.5 4.5S10 9 10 9c3 1 4 2 4 6H2c0-4 1-5 4-6 0 0-1.5-1-1.5-3.5S6 1 8 1"/></svg><span class=meta__text>Bloger</span></div></div></header><div class="post__toc toc"><div class=toc__title>Page content</div><div class=toc__menu><nav id=TableOfContents><ul><li><a href=#单卡复现-deepseek-r1-zero教程来了-by-datawhale>单卡复现 DeepSeek R1 Zero教程来了！ by Datawhale</a></li></ul></nav></div></div><div class="content post__content clearfix"><h2 id=单卡复现-deepseek-r1-zero教程来了-by-datawhale>单卡复现 DeepSeek R1 Zero教程来了！ by Datawhale</h2><div><section data-style="-webkit-tap-highlight-color: transparent; margin-bottom: 0px; outline: 0px; text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px; background-color: rgb(255, 255, 255); letter-spacing: 0.544px; text-wrap: wrap; caret-color: rgb(34, 34, 34); font-family: 'Helvetica Neue', Helvetica, 'Hiragino Sans GB', 'Microsoft YaHei', Arial, sans-serif; visibility: visible; line-height: 27.2px; color: rgb(163, 163, 163) !important;" mp-original-font-size=17 mp-original-line-height=27.200000762939453 data-mpa-powered-by=yiban.io><h1 data-tool=mdnice编辑器><section><section data-style="-webkit-tap-highlight-color: transparent; margin-bottom: 0px; outline: 0px; text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px; background-color: rgb(255, 255, 255); letter-spacing: 0.544px; text-wrap: wrap; caret-color: rgb(34, 34, 34); font-family: 'Helvetica Neue', Helvetica, 'Hiragino Sans GB', 'Microsoft YaHei', Arial, sans-serif; visibility: visible; line-height: 27.2px; color: rgb(163, 163, 163) !important;" mp-original-font-size=17 mp-original-line-height=27.200000762939453><section><section powered-by=xiumi.us><section><section data-id=85660 data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)"><section><p><span> Datawhale干货 </span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);"><section><span><strong>作者</strong><strong>：</strong><strong>邓恺俊，Datawhale成员</strong></span></section></section></section></section></section><section><mp-common-profile data-id="MzIyNjM2MzQyNg==" data-pluginname=mpprofile data-headimg="http://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsEXsBwQkpYLtE2vhn7Z3RVOSRu5M1VicIgqgMRKLsxsibK7OUSqUb1rUO4pfXnQyFYKqhryAIeh4MOg/300?wx_fmt=png&amp;wxfrom=19" data-nickname=Datawhale data-alias=Datawhale data-signature="一个专注于AI领域的开源组织，汇聚了众多优秀学习者，使命-for the learner，和学习者一起成长。" data-from=2 data-weuitheme=light data-origin_num=709 data-isban=0 data-biz_account_status=0 data-index=0></mp-common-profile></section></section></section></section></section></h1></section><section><span>项目代码可见：unlock-deepseek/Datawhale-R1（https://github.com/datawhalechina/unlock-deepseek），欢迎关注和 star！</span></section><section><span><strong><span>其余所有开源内容见文末。</span></strong></span></section><hr><section>各位同学好，我是来自 Unlock-DeepSeek 团队的邓恺俊。</section><section>之前有同学问：主播主播，你们团队的复现的 R1 Zero 确实很强，但是还是太耗算力资源，没 3 张 A800 啊，还有没有更经济更简单的方式来学习 R1 Zero 的复现呢?　</section><section>有的，兄弟，有的有的，像这样的方案还有九个（开玩笑）。今天我们来介绍一个有趣的方法，<span>能够让你在单卡复现 DeepSeek R1 Zero，甚至只用一块 4090 显卡也能轻松实现！</span>　</section><h2>为什么单卡就能复现？</h2><section>你可能会问：“原来需要 3 张 A800，如今怎么只需单卡？这其中有什么黑科技？” 答案就在于我们引入了 <span>Unsloth + LoRA</span>。</section><section>Unsloth 的核心优势在于：　</section><ul><li><section><span>强化学习算法优化</span><span>：集成了多种强化学习（RL）算法，并通过底层代码优化（如优化计算图、减少冗余操作），显著提升了大模型在推理和微调时的性能。</span></section></li><li><section><span>最新量化技术</span><span>：大幅降低显存消耗，使得原本需要多卡的大模型也能在单卡上运行。</span></section></li><li><section><span>完整的 LoRA 和 QLoRA 微调支持</span><span>：即使显存有限，也能通过少量资源复现 R1 Zero。</span></section></li></ul><section>这就为我们提供了一个成本更低、实现更简单的方案。Unsloth 官方博客提到：<span>仅需 7G VRAM，就能训练 Qwen2.5-1.5B 的模型。</span>　</section><section><span>Unsloth GitHub：https://github.com/unslothai/unsloth</span></section><h2>环境搭建</h2><h3>安装 Unsloth</h3><section>环境搭建部分在<a target=_blank href="https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&amp;mid=2247700308&amp;idx=1&amp;sn=aa6324d30cc6d054c1dbb238b013b9b5&amp;scene=21#wechat_redirect" textvalue=之前的公众号文章 linktype=text imgurl imgdata=null data-itemshowtype=0 tab=innerlink data-linktype=2>之前的公众号文章</a>中已有详细说明，这里只需在原有基础上补充安装 Unsloth 及指定版本的 trl 库即可。　</section><section><a target=_blank href="https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&amp;mid=2247700308&amp;idx=1&amp;sn=aa6324d30cc6d054c1dbb238b013b9b5&amp;scene=21#wechat_redirect" textvalue="DeepSeek R1 Zero中文复现教程来了！" linktype=text imgurl imgdata=null data-itemshowtype=0 tab=innerlink data-linktype=2>DeepSeek R1 Zero中文复现教程来了！</a><br></section><section>补充说明：在<a target=_blank href="https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&amp;mid=2247700308&amp;idx=1&amp;sn=aa6324d30cc6d054c1dbb238b013b9b5&amp;scene=21#wechat_redirect" textvalue=之前公众号发布 linktype=text imgurl imgdata=null data-itemshowtype=0 tab=innerlink data-linktype=2>之前公众号发布</a>的多卡训练代码中，错误的引入了“思考长度奖励函数”，并且没有在代码中使用 flash-attn，Unlock-DeepSeek 团队已经修复代码，请使用仓库中的最新代码，我们同时更新一版训练示意图。　</section><section><img alt=Image data-imgfileid=100217506 data-ratio=0.5435185185185185 data-type=png data-w=1080 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsGt21pKOJHHrciaMISUdqTXzicfWZh6T5qIeicxcfB6535UOphgTPRP1SYh0AibcCekTsicpHrNRiaal5SQ/640?wx_fmt=png&amp;from=appmsg" src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsGt21pKOJHHrciaMISUdqTXzicfWZh6T5qIeicxcfB6535UOphgTPRP1SYh0AibcCekTsicpHrNRiaal5SQ/640?wx_fmt=png&amp;from=appmsg"></section><p>本文中仅展示与前文有差异的代码部分，同时我们提供了完整的训练代码，请在文末获取。　</p><blockquote data-type=2 data-url data-author-name data-content-utf8-length=37 data-source-title data-text="注意：由于官方的 trl 库还未更新，为了兼容 Unsloth，我们需要安装特定版本的 trl。具体命令如下：" data-editid=38xa3x3jf99cy3ljb4><section><p>注意：为了兼容 Unsloth，我们需要安装特定版本的 trl。具体命令如下：</p></section></blockquote><p><br></p><section><section><ul><li><li><li><li><li></ul><pre data-lang=nginx><code><span><span># 安装 unsloth 和 vllm</span></span></code><code><span><span>pip</span> install unsloth vllm</span></code><code><span><br></span></code><code><span><span># 安装指定版本的 trl（兼容 unsloth）</span></span></code><code><span>pip install trl==<span>0</span>.<span>15</span>.<span>0</span></span></code></pre></section></section><section><pre><section><span>参考自：https://docs.unsloth.ai/get-started/unsloth-notebooks</span></section></pre></section><h3>配置文件修改</h3><p>大部分配置与之前的 <span>Datawhale-R1.yaml</span> 文件保持一致。为了支持单卡复现 R1 Zero，我们做了如下调整：　</p><ul><li><section><span>LoRA 参数设置</span><span>：启用 LoRA 微调，调整 LoRA 秩数（</span><span>lora_r</span><span>）为 64（常用的选择有 8、16、32、64、128 等），并设置 </span><span>lora_alpha</span><span> 为 32。</span></section></li><li><section><span>限制回答长度</span><span>：将 </span><span>max_completion_length</span><span> 设置为 1024，以控制输出长度。</span></section></li><li><section><span>优化器调整：</span><span>优化器设置为 </span><span>adamw_8bit</span><span>，以加速训练。</span></section></li></ul><section><section>注意：为了更节省内存，这里的 <span>max_completion_length</span> 被设置为 1024，但是这可能会影响模型的发挥，如果你的资源充足，设置更高（4096、8196）可能会获得更好的效果，但是也会加重资源消耗。若内存不足可以调节 <span>vllm_gpu_memory_utilization</span>，适当降低。除此之外，如果有更多资源，可以考虑将优化器 <span>optim</span> 调整为 <span>adamw_torch</span>，这有助于更好地复现模型。　</section></section><section><section><br></section><section><ul><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li></ul><pre data-lang=bash><code><span># LoRA 参数调整</span></code><code><span>lora_r: 64        # LoRA 秩数，选择任意大于 0 的数字！建议使用 8, 16, 32, 64, 128</span></code><code><span>lora_alpha: 32    # LoRA alpha 值</span></code><code><span><br></span></code><code><span># 训练参数</span></code><code><span>learning_rate: 1.0e-5 # 学习率，调整为1e-5</span></code><code><span><br></span></code><code><span># GRPO 算法参数</span></code><code><span>beta: 0.001       # KL 惩罚因子</span></code><code><span>optim: adamw_8bit # 使用 8bit 优化器以加速训练</span></code><code><span>max_prompt_length: 256       # 输入 prompt 的最大长度</span></code><code><span>max_completion_length: 1024  # 输出回答长度，包含推理思维链</span></code><code><span>num_generations: 4</span></code><code><span>use_vllm: true               # 启用 vLLM 加速推理</span></code><code><span>vllm_gpu_memory_utilization: 0.4  # vLLM 的 GPU 内存利用率（内存紧张时可适当降低）</span></code></pre></section><p><span>LoRA微调参考：https://zhuanlan.zhihu.com/p/663557294</span></p></section><h2>启动训练</h2><p>启动训练的代码很简单，由于我们只需要单卡，不需要涉及到配置复杂的 Accelerate 库，直接运行以下代码即可运行。　</p><section><ul><li></ul><pre data-lang=css><code><span><span>python</span> <span>train_Datawhale-R1_unsloth</span><span>.py</span> <span>--config</span> <span>Datawhale-R1_unsloth</span><span>.yaml</span></span></code></pre></section><h2>训练代码优化解读</h2><p>基于 Unsloth 框架，我们对原始代码做了简化和优化。主要思路有两点：　</p><h3>打补丁提升训练速度</h3><p>在执行强化学习训练的代码之前，我们添加了两行代码，利用 <span>PatchFastRL</span> 函数对某些 RL 算法（如 GRPO）进行“打补丁”。这个操作实际上在底层优化了计算图、减少了冗余计算，从而加速训练过程。　</p><section><ul><li><li></ul><pre data-lang=python><code><span>from unsloth import FastLanguageModel, PatchFastRL</span></code><code><span>PatchFastRL("GRPO", FastLanguageModel)  # 对 GRPO 算法打补丁</span></code></pre></section><h3>GRPO 训练函数的改进</h3><section>除此之外，我们还改进了 <span>grpo_function</span> 里面的函数，在这之中进行了一些优化，具体在代码的 14～34 行中，具体来说，我们加入以下两个方式：</section><ul><li><section><span>模型加载</span><span>：通过 </span><span>FastLanguageModel.from_pretrained</span><span> 方法加载预训练模型，并启用 vLLM 快速推理，同时支持 4 位加载（或 LoRA 16 位）。</span></section></li><li><section><span>PEFT 微调</span><span>：利用 </span><span>get_peft_model</span><span> 方法对模型应用 LoRA 微调，指定了目标模块、LoRA 参数以及梯度检查点，确保在有限显存条件下依然能有效训练。</span></section></li></ul><section><ul><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li><li></ul><pre data-lang=python><code><span><span># 定义 GRPO 训练函数</span></span></code><code><span><span><span>def</span> <span>grpo_function</span><span>(</span></span></span></code><code><span>    model_args: ModelConfig,</span></code><code><span>    dataset_args: DatasetArguments,</span></code><code><span>    training_args: GRPOConfig,</span></code><code><span>    callbacks: List,</span></code><code><span><span>)</span>:</span></code><code><span>    <span># 记录模型参数</span></span></code><code><span>    logger.info(<span>f"Model parameters <span>{model_args}</span>"</span>)</span></code><code><span>    <span># 记录训练/评估参数</span></span></code><code><span>    logger.info(<span>f"Training/evaluation parameters <span>{training_args}</span>"</span>)</span></code><code><span><br></span></code><code><span>    <span># 从预训练模型加载模型和分词器</span></span></code><code><span>    model, tokenizer = FastLanguageModel.from_pretrained(</span></code><code><span>        model_name=model_args.model_name_or_path,  <span># 模型名称或路径</span></span></code><code><span>        fast_inference=<span>True</span>,  <span># 启用 vLLM 快速推理</span></span></code><code><span>        load_in_4bit=<span>True</span>,  <span># 是否以 4 位加载模型，False 表示使用 LoRA 16 位</span></span></code><code><span>        max_lora_rank=model_args.lora_r,  <span># 设置 LoRA 的最大秩</span></span></code><code><span>        max_seq_length=training_args.max_completion_length,  <span># 设置最大序列长度</span></span></code><code><span>        gpu_memory_utilization=training_args.vllm_gpu_memory_utilization,  <span># GPU 内存利用率，若内存不足可减少</span></span></code><code><span>        attn_implementation=model_args.attn_implementation, <span># 设置注意力实现方式 flash attention</span></span></code><code><span>    )</span></code><code><span><br></span></code><code><span>    <span># PEFT 模型</span></span></code><code><span>    model = FastLanguageModel.get_peft_model(</span></code><code><span>        model,</span></code><code><span>        r = model_args.lora_r, </span></code><code><span>        target_modules = [</span></code><code><span>            <span>"q_proj"</span>, <span>"k_proj"</span>, <span>"v_proj"</span>, <span>"o_proj"</span>, <span># 如果 OOM 内存不足，可以移除 QKVO</span></span></code><code><span>            <span>"gate_proj"</span>, <span>"up_proj"</span>, <span>"down_proj"</span>,</span></code><code><span>        ],  </span></code><code><span>        lora_alpha = model_args.lora_alpha,  <span># 设置 LoRA 的 alpha 值</span></span></code><code><span>        use_gradient_checkpointing = <span>"unsloth"</span>,  <span># 启用 unsloth 的梯度检查</span></span></code><code><span>        random_state = training_args.seed,  <span># 设置随机种子</span></span></code><code><span>    )</span></code></pre></section><section>如果遇到 Out of Memory 显存不足问题，可以移除 <span>target_modules</span> 中的 <span>"q_proj", "k_proj", "v_proj", "o_proj"</span>。<span><br></span></section><p><span>参考自：https://unsloth.ai/blog/r1-reasoning</span></p><p><span>模型量化参考：LLM量化综合指南（8bits/4bits）https://zhuanlan.zhihu.com/p/671007819</span></p><h2>训练结果与一些思考</h2><section>以下是训练结果的部分截图，大致与我们复现 Tiny Zero 和 Mini R1 的结果类似，这里就不再做详细分析。　</section><section><img alt=Image data-imgfileid=100217505 data-ratio=0.1814814814814815 data-type=png data-w=1080 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsGt21pKOJHHrciaMISUdqTXzzHrpkBGAOTfeRMyYUwqyHFTQQiaWgyROROgJMy3icKO6zgO8v0QzrbOA/640?wx_fmt=png&amp;from=appmsg" src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsGt21pKOJHHrciaMISUdqTXzzHrpkBGAOTfeRMyYUwqyHFTQQiaWgyROROgJMy3icKO6zgO8v0QzrbOA/640?wx_fmt=png&amp;from=appmsg"></section><section>接下来分享一些学习 R1 Zero 过程中的思考（非严谨学术研究，个人观点，仅供参考）。　</section><h3>Aha moment 是 RL 训练的结果吗？</h3><section>在开始研究之前，我对 Aha moment（顿悟时刻）这个概念充满好奇，这仿佛是 DeepSeek 在经过 RL 训练后突然获得的超能力。但深入学习阅读了 oat 的文章后，发现 Aha moment 并不是凭空出现的，它可能在 base 模型和 SFT 阶段就已经埋下了种子。RL 训练做的事情，更像是一个"放大器"，通过设计的奖励机制，最大化了模型产生顿悟时刻的概率。换句话说，RL 训练将模型原本浅层的自我反思能力，转化为更有深度和效果的思考过程。　</section><section><span>参考OAT文章：There May Not be Aha Moment in R1-Zero-like Training — A Pilot Study：https://oatllm.notion.site/oat-zero　</span></section><h3>思考长度越长越有效吗？</h3><section>在社区中有一种普遍的看法：RL 训练让模型的输出变得更长，从而提升效果。这个观点确实有一定道理，因为 RL 强化了模型的思考过程，生成更多的 token 是自然的结果。</section><section>然而，问题是：<span>更长的思考真的意味着更好的结果吗？</span>　</section><section>在复现 Tiny Zero 的过程中，我观察到一个有趣的现象：<span>token 数量呈现先降后升的趋势</span>。这个现象可以这样解释：最初，由于存在格式奖励（format reward），模型必须保证格式正确，然而长度过长的输出比较难学习到答案的格式，并且会包含很多对解决任务无用的 token，所以 token 数量自然先会下降，模型先学简单的格式，保留有利于正确计算的 token，再去学复杂的计算，先简后繁；随着训练的进行，模型开始进行更多的尝试和反思以得出正确答案，输出长度逐渐增加并趋于稳定。这一观察也印证了 OAT 的结论：<span>输出长度与自我反思的质量并不一定存在线性关联。</span></section><h3>S1 文章的一些结论和思考</h3><section>最近我也看了李飞飞团队的 S1 文章，详细分析了其方法和 R1 Zero 的不同。总的来说，<span>S1</span> 通过少量高质量数据（约 1k + SFT + 设计 Prompt）进行训练，而 <span>R1 Zero</span> 则是通过基础训练（Base）加 RL 强化训练完成的。在 S1 中，他们采用了 <span>budget forcing</span> 方法，在测试时强制设定最大和最小的思考 token 数量。具体而言：　</section><ul><li><section><span>通过添加 </span><span>"end-of-thinking token 分隔符"</span><span> 和 </span><span>"Final Answer"</span><span> 来控制思考上限；</span></section></li><li><section><span>通过禁止生成分隔符并添加 </span><span>"wait"</span><span> 提示词来控制思考下限。</span></section></li></ul><section>实验结果表明，适度增加思考 token 数量确实能够提升模型在 AIME24 基准测试上的表现。然而，他们也发现，过度抑制思考结束反而会导致模型陷入无效循环。这个发现非常符合直觉：就像人类的思考一样，<span>简单问题（比如数一个单词中的字母数量）并不需要过度思考</span>，而真正需要延长思考时间的，往往是那些较为复杂的问题。　</section><section><span>s1参考阅读：<a target=_blank href="https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&amp;mid=2652562988&amp;idx=2&amp;sn=85722bbd6e6142e1e24c3af68006b08e&amp;scene=21#wechat_redirect" textvalue=16张H100训26分钟，超越o1-preview！李飞飞等用1K样本，揭秘测试时Scaling linktype=text imgurl imgdata=null data-itemshowtype=0 tab=innerlink data-linktype=2>16张H100训26分钟，超越o1-preview！李飞飞等用1K样本，揭秘测试时Scaling</a></span></section><h2>总结与展望</h2><section>首先再次感谢 Unsloth 的优化和社区小伙伴的努力，这不仅使得大模型的训练和推理更加高效，还大幅降低了显存消耗，使得即便是仅一块显卡，也能轻松完成 R1 Zero 的复现，这也提供了一个更经济、更简便的复现方案，也为低资源环境下的大模型应用开辟了新的可能。　</section><section>同时我们也计划深入探讨 R1 的蒸馏方法，以进一步降低模型的计算需求并提高其可扩展性。此外，我们也将持续优化代码和算法，推动更多开源社区的创新和合作。我们欢迎大家参与讨论和分享，期待和大家一起在开源社区中共创更多精彩内容。　</section><h1>完整文件获取<span></span></h1><section>Unlock-DeepSeek 团队后续会陆续发布更多关于 DeepSeek 相关工作解读的文章，敬请关注，我们下次再见！　</section><section><section>Unlock-DeepSeek 项目主页：https://datawhalechina.github.io/unlock-deepseek/　</section><section>Github 仓库：https://github.com/datawhalechina/unlock-deepseek　</section><section>Gitee 国内仓库：https://gitee.com/anine09/unlock-deepseek　</section><section>Swanlab 实验数据：https://swanlab.cn/@Kedreamix/Datawhale-R1-by_Kedreamix/runs/sqxeo1i3v8hgzclm3nwkk</section><section>复现文件在 Datawhale-R1 文件夹，请仔细阅读 Datawhale-R1/README.md。</section><section>Unlock-DeepSeek 项目目前并不完善，并且正在快速迭代，请持续关注。　</section></section><section><img alt=图片 data-backh=234 data-backw=578 data-galleryid data-imgfileid=100217508 data-ratio=0.40555555555555556 data-s=300,640 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsEoyqBuoSeZBYGia4FrNqibThuOLnV2mc5w2np56PD2KQyAOMdyh50NHEqKXDBIVfm4rXeUCnXtU1mg/640?wx_fmt=other&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1&amp;tp=webp" data-type=png data-w=900 src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsEoyqBuoSeZBYGia4FrNqibThuOLnV2mc5w2np56PD2KQyAOMdyh50NHEqKXDBIVfm4rXeUCnXtU1mg/640?wx_fmt=other&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1&amp;tp=webp"><strong><span><span>一起“</span><span><strong><span>点</span></strong></span><span><strong><span>赞<span>”</span></span></strong></span><strong><span>三连</span></strong><span>↓</span></span></strong></section><p><mp-style-type data-value=10000></mp-style-type></p></div><hr><a href=https://mp.weixin.qq.com/s/0Q369GFqA_VguWeiL0MwDg ,target=_blank rel="noopener noreferrer">原文链接</a></div><footer class=post__footer><div class="post__tags tags clearfix"><svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M4 0h8s2 0 4 2l15 15s2 2 0 4L21 31s-2 2-4 0L2 16s-2-2-2-4V3s0-3 4-3m3 10a3 3 0 000-6 3 3 0 000 6"/></svg><ul class=tags__list><li class=tags__item><a class="tags__link btn" href=../../../tags/fetched/ rel=tag>fetched</a></li><li class=tags__item><a class="tags__link btn" href=../../../tags/datawhale/ rel=tag>Datawhale</a></li></ul></div></footer></article></main><div class="authorbox clearfix"><div class=authorbox__header><span class=authorbox__name>About Bloger</span></div></div><nav class="pager flex"><div class="pager__item pager__item--prev"><a class=pager__link href=../../../posts/2025-02/%E8%83%96%E4%B8%9C%E6%9D%A553%E9%A1%B5%E6%8A%A5%E5%91%8A%E8%B0%83%E6%9F%A5%E7%BA%A2%E5%86%85%E8%A3%A4%E6%8E%89%E8%89%B2%E4%BA%8B%E4%BB%B6_%E5%A4%9A%E5%90%8D%E7%AE%A1%E7%90%86%E5%B1%82%E8%A2%AB%E5%A4%84%E7%90%86/ rel=prev><span class=pager__subtitle>«&#8201;Previous</span><p class=pager__title>胖东来53页报告调查红内裤掉色事件 多名管理层被处理</p></a></div><div class="pager__item pager__item--next"><a class=pager__link href=../../../posts/2025-02/%E7%A7%91%E7%A0%94%E6%96%B0%E5%A7%BF%E5%8A%BF_%E7%94%A8_deepseek_%E5%A4%8D%E5%88%BB_nature_%E6%80%9D%E8%B7%AF_%E8%BF%99%E4%B8%AA_ai_%E7%A5%9E%E5%99%A8%E8%AE%A9%E6%88%91%E5%B0%91%E7%86%AC%E5%8D%8A%E5%B9%B4%E5%A4%9C/ rel=next><span class=pager__subtitle>Next&#8201;»</span><p class=pager__title>科研新姿势：用 Deepseek 复刻 Nature 思路？这个 AI 神器让我少熬半年夜</p></a></div></nav><section class=comments><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//kkitown.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section></div><aside class=sidebar><div class="widget-search widget"><form class=widget-search__form role=search method=get action=https://google.com/search><input class=widget-search__field type=search placeholder=Search… name=q aria-label=Search…>
<input class=widget-search__submit type=submit value=Search>
<input type=hidden name=sitesearch value=https://ixxmu.github.io/></form></div><div class="widget-recent widget"><h4 class=widget__title>Recent Posts</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=../../../posts/2025-04/%E7%89%B9%E6%9C%97%E6%99%AE%E9%98%B5%E8%90%A5%E8%A2%AB%E8%B4%A8%E7%96%91%E5%85%AC%E7%84%B6%E6%93%8D%E6%8E%A7%E8%82%A1%E5%B8%82%E7%89%9F%E5%88%A9_%E7%99%BD%E5%AE%AB%E5%9B%9E%E5%BA%94/>特朗普阵营被质疑公然操控股市牟利 白宫回应</a></li><li class=widget__item><a class=widget__link href=../../../posts/2025-04/colossal_biosciences%E5%A4%8D%E6%B4%BB1%E4%B8%87%E5%A4%9A%E5%B9%B4%E5%89%8D_%E5%86%B0%E5%8E%9F%E7%8B%BC_/>Colossal Biosciences复活1万多年前“冰原狼”</a></li><li class=widget__item><a class=widget__link href=../../../posts/2025-04/%E6%B0%B8%E8%BE%89%E9%AB%98%E7%AE%A1%E8%B0%88%E8%B0%83%E6%94%B9%E8%A2%AB%E4%BA%8E%E4%B8%9C%E6%9D%A5%E6%89%93%E6%96%AD_%E8%B4%A8%E9%97%AE%E6%9C%88%E8%B5%9A200%E4%B8%87%E4%B8%BA%E4%BD%95%E4%B8%8D%E6%B6%A8%E5%B7%A5%E8%B5%84/>永辉高管谈调改被于东来打断 质问月赚200万为何不涨工资</a></li><li class=widget__item><a class=widget__link href=../../../posts/2025-04/%E9%87%91%E4%BB%B7%E7%96%AF%E6%B6%A8_%E8%83%96%E4%B8%9C%E6%9D%A5%E7%BB%88%E6%AD%A2%E6%97%A7%E9%87%91%E5%85%91%E6%8D%A2%E5%9B%9E%E6%94%B6%E6%9C%8D%E5%8A%A1/>金价疯涨 胖东来终止旧金兑换回收服务</a></li><li class=widget__item><a class=widget__link href=../../../posts/2025-04/%E5%89%8Dopenai%E7%A0%94%E7%A9%B6%E5%91%9876%E9%A1%B5%E7%A1%AC%E6%A0%B8%E6%8E%A8%E6%BC%94_2027%E5%B9%B4asi%E6%8E%A5%E7%AE%A1%E4%B8%96%E7%95%8C_%E4%BA%BA%E7%B1%BB%E6%88%90npc/>前OpenAI研究员76页硬核推演：2027年ASI接管世界 人类成NPC</a></li></ul></div></div><div class="widget-categories widget"><h4 class=widget__title>Categories</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=../../../categories/duty/>Duty</a></li><li class=widget__item><a class=widget__link href=../../../categories/duty2/>Duty2</a></li></ul></div></div><div class="widget-taglist widget"><h4 class=widget__title>Tags</h4><div class=widget__content><a class="widget-taglist__link widget__link btn" href=../../../tags/cnbeta/ title=Cnbeta>Cnbeta (108)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/datawhale/ title=Datawhale>Datawhale (9)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/fetched/ title=Fetched>Fetched (662)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/githubdaily/ title=GitHubDaily>GitHubDaily (7)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E4%B8%81%E9%A6%99%E5%9B%AD/ title=丁香园>丁香园 (28)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E4%BA%BA%E7%89%A9/ title=人物>人物 (15)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E5%8D%97%E6%96%B9%E5%91%A8%E6%9C%AB/ title=南方周末>南方周末 (11)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E5%B0%8F%E4%BC%97%E8%BD%AF%E4%BB%B6/ title=小众软件>小众软件 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E5%B0%8F%E5%A4%A7%E5%A4%AB%E6%BC%AB%E7%94%BB/ title=小大夫漫画>小大夫漫画 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E5%B0%91%E6%95%B0%E6%B4%BE/ title=少数派>少数派 (6)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E5%BE%AA%E5%9B%A0%E7%BC%89%E8%8D%AF/ title=循因缉药>循因缉药 (7)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%98%9F%E7%90%83%E5%95%86%E4%B8%9A%E8%AF%84%E8%AE%BA/ title=星球商业评论>星球商业评论 (6)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%9C%AA%E9%97%BBcode/ title=未闻Code>未闻Code (6)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%A7%BD%E8%BE%B9%E5%BE%80%E4%BA%8B/ title=槽边往事>槽边往事 (7)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%AF%8F%E6%97%A5%E4%BA%BA%E7%89%A9/ title=每日人物>每日人物 (6)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%B5%AE%E4%B9%8B%E9%9D%99/ title=浮之静>浮之静 (6)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%81%BC%E8%A7%81/ title=灼见>灼见 (7)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%94%9F%E4%BF%A1%E6%8A%80%E8%83%BD%E6%A0%91/ title=生信技能树>生信技能树 (10)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%94%9F%E4%BF%A1%E8%8F%9C%E9%B8%9F%E5%9B%A2/ title=生信菜鸟团>生信菜鸟团 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%9F%A5%E8%AF%86%E5%88%86%E5%AD%90/ title=知识分子>知识分子 (13)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%BD%91%E6%98%93%E6%95%B0%E8%AF%BB/ title=网易数读>网易数读 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E8%B4%A2%E7%BB%8F%E4%B8%89%E5%88%86%E9%92%9F/ title=财经三分钟>财经三分钟 (7)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E8%B5%9B%E5%85%88%E7%94%9F/ title=赛先生>赛先生 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E9%83%91%E5%B7%9E%E6%A5%BC%E5%B8%82/ title=郑州楼市>郑州楼市 (32)</a></div><a href=../../../tags/></a></div><div class="widget-social widget"><h4 class="widget-social__title widget__title">Social</h4><div class="widget-social__content widget__content"><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=Facebook rel="noopener noreferrer" href=https://facebook.com/username target=_blank><svg class="widget-social__link-icon icon icon-facebook" width="24" height="24" viewBox="0 0 352 352"><path d="m0 32v288c0 17.5 14.5 32 32 32h288c17.5.0 32-14.5 32-32V32c0-17.5-14.5-32-32-32H32C14.5.0.0 14.5.0 32zm320 0v288h-83V212h41.5l6-48H237v-31c0-14 3.5-23.5 23.5-23.5h26V66c-4.4-.6-19.8-1.5-37.5-1.5-36.9.0-62 22.2-62 63.5v36h-42v48h42v108H32V32z"/></svg>
<span>Facebook</span></a></div><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=Twitter rel="noopener noreferrer" href=https://twitter.com/username target=_blank><svg class="widget-social__link-icon icon icon-twitter" width="24" height="24" viewBox="0 0 384 312"><path d="m384 36.9c-14.1 6.3-29.3 10.5-45.2 12.4 16.3-9.7 28.8-25.2 34.6-43.6-15.2 9-32.1 15.6-50 19.1-14.4-15.2-34.9-24.8-57.5-24.8-43.5.0-78.8 35.3-78.8 78.8.0 6.2.7 12.2 2 17.9-65.5-3.3-123.5-34.6-162.4-82.3C20 26 16.1 39.6 16.1 54c0 27.3 13.9 51.4 35 65.6-12.9-.4-25.1-4-35.7-9.9v1c0 38.2 27.2 70 63.2 77.2-6.6 1.8-13.6 2.8-20.8 2.8-5.1.0-10-.5-14.8-1.4 10 31.3 39.1 54.1 73.6 54.7-27 21.1-60.9 33.7-97.8 33.7-6.4.0-12.6-.4-18.8-1.1C34.9 299 76.3 312 120.8 312c144.9.0 224.1-120 224.1-224.1.0-3.4-.1-6.8-.2-10.2 15.4-11.1 28.7-25 39.3-40.8z"/></svg>
<span>Twitter</span></a></div><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=Instagram rel="noopener noreferrer" href=https://www.instagram.com/username target=_blank><svg class="widget-social__link-icon icon icon-instagram" width="24" height="24" viewBox="0 0 256 256"><circle cx="193" cy="59" r="15"/><path fill-rule="evenodd" d="M101 0h54c41 0 58.4 3.9 74.5 17C256.2 37.5 256 74.8 256 97.7v60c0 26.7.0 60.4-26.5 81.4-16 13.4-33.5 16.9-74.5 16.9h-54c-41 0-57.5-3.5-74.5-16.9C1 218.9.5 186.3.1 160.5L0 155V97.7c0-23-.2-60.2 26.5-80.7C45 2 60 0 101 0zm4.9 23h44.3c45.8.0 58.3 3.5 70.3 17.5 11.8 13.2 12 30.1 12.5 62.9V156c.2 20.8.3 45.8-12.5 59.5-12 14-24.5 17.5-70.3 17.5h-44.3c-45.9.0-57.3-3.5-70.4-17.5-12.2-13-12.3-36.5-12.4-56.7v-55.6c.4-32.6.7-49.6 12.4-62.7C48 26.5 60 23 105.9 23zm19.6 144.5a42 42 0 100-84 42 42 0 000 84zm0 22.5a64.5 64.5.0 100-129 64.5 64.5.0 000 129z"/></svg>
<span>Instagram</span></a></div></div></div></aside></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2025 文字轨迹.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=../../../js/menu.js></script><script src=../../../js/custom.js></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script></body></html>