<!doctype html><html class=no-js lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>零基础入门：DeepSeek微调的评测教程来了！ - 文字轨迹</title>
<script>(function(e,t){e[t]=e[t].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:url" content="https://ixxmu.github.io/posts/2025-02/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8_deepseek%E5%BE%AE%E8%B0%83%E7%9A%84%E8%AF%84%E6%B5%8B%E6%95%99%E7%A8%8B%E6%9D%A5%E4%BA%86_/"><meta property="og:site_name" content="文字轨迹"><meta property="og:title" content="零基础入门：DeepSeek微调的评测教程来了！"><meta property="og:description" content="零基础入门：DeepSeek微调的评测教程来了！ by Datawhale Datawhale干货 作者：牧小熊，Datawhale成员"><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-27T15:08:54+00:00"><meta property="article:modified_time" content="2025-02-27T15:08:54+00:00"><meta property="article:tag" content="Fetched"><meta property="article:tag" content="Datawhale"><meta itemprop=name content="零基础入门：DeepSeek微调的评测教程来了！"><meta itemprop=description content="零基础入门：DeepSeek微调的评测教程来了！ by Datawhale Datawhale干货 作者：牧小熊，Datawhale成员"><meta itemprop=datePublished content="2025-02-27T15:08:54+00:00"><meta itemprop=dateModified content="2025-02-27T15:08:54+00:00"><meta itemprop=wordCount content="907"><meta itemprop=keywords content="Fetched,Datawhale"><meta name=twitter:card content="summary"><meta name=twitter:title content="零基础入门：DeepSeek微调的评测教程来了！"><meta name=twitter:description content="零基础入门：DeepSeek微调的评测教程来了！ by Datawhale Datawhale干货 作者：牧小熊，Datawhale成员"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=../../../css/style.css><link rel=stylesheet href=../../../css/custom.css><link rel="shortcut icon" href=../../../favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=../../../ title=文字轨迹 rel=home><div class="logo__item logo__text"><div class=logo__title>文字轨迹</div><div class=logo__tagline>故事流淌过的地方</div></div></a></div><nav class=menu><button class=menu__btn aria-haspopup=true aria-expanded=false tabindex=0>
<span class=menu__btn-title tabindex=-1>Menu</span></button><ul class=menu__list><li class=menu__item><a class=menu__link href=../../../posts/><span class=menu__text>文章</span></a></li><li class=menu__item><a class=menu__link href=../../../tags/><span class=menu__text>标签</span></a></li><li class=menu__item><a class=menu__link href=../../../about/><span class=menu__text>关于</span></a></li></ul></nav></div></header><div class="wrapper flex"><div class=primary><main class=main role=main><article class=post><header class=post__header><meta name=referrer content="never"><h1 class=post__title>零基础入门：DeepSeek微调的评测教程来了！</h1><div class="post__meta meta"><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0a14 14 0 110 28 1 1 0 010-28m0 3a3 3 0 100 22 3 3 0 000-22m1 4h-2v8.4l6.8 4.4L22 18l-6-3.8z"/></svg><time class=meta__text datetime=2025-02-27T15:08:54Z>2025-02-27</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=../../../categories/duty/ rel=category>Duty</a></span></div><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 16 16"><path d="M8 1c2 0 3.5 2 3.5 4.5S10 9 10 9c3 1 4 2 4 6H2c0-4 1-5 4-6 0 0-1.5-1-1.5-3.5S6 1 8 1"/></svg><span class=meta__text>Bloger</span></div></div></header><div class="post__toc toc"><div class=toc__title>Page content</div><div class=toc__menu><nav id=TableOfContents><ul><li><a href=#零基础入门deepseek微调的评测教程来了-by-datawhale>零基础入门：DeepSeek微调的评测教程来了！ by Datawhale</a></li></ul></nav></div></div><div class="content post__content clearfix"><h2 id=零基础入门deepseek微调的评测教程来了-by-datawhale>零基础入门：DeepSeek微调的评测教程来了！ by Datawhale</h2><div><section data-tool=mdnice编辑器 data-website=https://www.mdnice.com><section data-mpa-powered-by=yiban.io><section powered-by=xiumi.us><section><section powered-by=xiumi.us><section><section data-id=85660 data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)"><section><p><span> Datawhale干货 </span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);"><p><span><strong>作者：牧小熊，Datawhale成员</strong></span></p></section></section></section></section><section><mp-common-profile data-id="MzIyNjM2MzQyNg==" data-pluginname=mpprofile data-headimg="http://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsEXsBwQkpYLtE2vhn7Z3RVOSRu5M1VicIgqgMRKLsxsibK7OUSqUb1rUO4pfXnQyFYKqhryAIeh4MOg/300?wx_fmt=png&amp;wxfrom=19" data-nickname=Datawhale data-alias=Datawhale data-signature="一个专注于AI领域的开源组织，汇聚了众多优秀学习者，使命-for the learner，和学习者一起成长。" data-from=2 data-is_biz_ban=0 data-weui-theme=light data-origin_num=719 data-isban=0 data-biz_account_status=0 data-index=0></mp-common-profile></section></section></section></section></section><blockquote data-tool=mdnice编辑器><p>前言：大模型评测是一个系统工程，本文希望通过比较通俗的方式给大家直观感受大模型微调后的效果，相关是思路想法旨在起到抛砖引玉的效果，如果学习者对大模型评测有深厚的兴趣，可以从不同的角度进行学习。</p></blockquote><p data-tool=mdnice编辑器><span>三天前，看到了我们 Datawhale 公众号上发了文章</span><a target=_blank href="https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&amp;mid=2247701892&amp;idx=1&amp;sn=c324f7689c7e5c9d8cf320f2b65e772a&amp;scene=21#wechat_redirect" textvalue=《零基础入门：DeepSeek微调教程来了！》 linktype=text imgurl imgdata=null data-itemshowtype=0 tab=innerlink data-linktype=2><span>《零基础入门：DeepSeek 微调教程来了！》</span></a><span>反响很好，其中的内容写的非常接地气，适合学习者进行学习体验。</span></p><figure data-tool=mdnice编辑器><img alt data-backh=243 data-backw=349 data-imgfileid=100218446 data-ratio=0.6953703703703704 data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/vI9nYe94fsG0NuVTbKjD4hyyC3EWREW9hoRjtYVO01pXiaNZr8dvMfmHerQcuicqYq4d5U3Bnur2HibENNJBIlNibg/640?wx_fmt=jpeg&amp;from=appmsg" data-type=jpeg data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/vI9nYe94fsG0NuVTbKjD4hyyC3EWREW9hoRjtYVO01pXiaNZr8dvMfmHerQcuicqYq4d5U3Bnur2HibENNJBIlNibg/640?wx_fmt=jpeg&amp;from=appmsg"></figure><p data-tool=mdnice编辑器><span>于是，我尝试在那篇文章的基础上进行了复现，并对内容进行了一些延伸，帮助读者更加直观的感受大模型微调对模型的调整。</span></p><p data-tool=mdnice编辑器><span>为了方便学习与体验，本文中选择的模型是蒸馏后 DeepSeek-R1-Distill-Qwen-7B 模型，显卡选择是 RTX4090 24G。</span></p><p data-tool=mdnice编辑器><span>Deepseek 模型以及数据集均来源于魔塔社区 medical-o1-reasoning-SFT。</span></p><h2 data-tool=mdnice编辑器><span>1. 微调教程复现</span></h2><pre data-tool=mdnice编辑器><code><span>import</span> torch<br><span>import</span> matplotlib.pyplot <span>as</span> plt<br><span>from</span> transformers <span>import</span> (<br>    AutoTokenizer,<br>    AutoModelForCausalLM,<br>    TrainingArguments,<br>    Trainer,<br>    TrainerCallback<br>)<br><span>from</span> peft <span>import</span> LoraConfig, get_peft_model<br><span>from</span> datasets <span>import</span> load_dataset<br><span>import</span> os<br><br>os.environ[<span>"CUDA_VISIBLE_DEVICES"</span>] = <span>"0"</span>  <span># 指定使用GPU </span><br><br><span># 配置路径（根据实际路径修改）</span><br>model_path = <span>"xxxx"</span>  <span># 模型路径</span><br>data_path = <span>"xxxx"</span>  <span># 数据集路径</span><br>output_path = <span>"xxxx"</span>  <span># 微调后模型保存路径</span><br><br><br><span># 设置设备参数</span><br>DEVICE = <span>"cuda"</span>  <span># 使用CUDA</span><br>DEVICE_ID = <span>"0"</span>  <span># CUDA设备ID，如果未设置则为空</span><br>device = <span>f"<span>{DEVICE}</span>:<span>{DEVICE_ID}</span>"</span> <span>if</span> DEVICE_ID <span>else</span> DEVICE  <span># 组合CUDA设备信息</span><br><span># 自定义回调记录Loss</span><br><span><span>class</span> <span>LossCallback</span><span>(TrainerCallback)</span>:</span><br>    <span><span>def</span> <span>__init__</span><span>(self)</span>:</span><br>        self.losses = []<br><br>    <span><span>def</span> <span>on_log</span><span>(self, args, state, control, logs=None, **kwargs)</span>:</span><br>        <span>if</span> <span>"loss"</span> <span>in</span> logs:<br>            self.losses.append(logs[<span>"loss"</span>])<br><br><span># 数据预处理函数</span><br><span><span>def</span> <span>process_data</span><span>(tokenizer)</span>:</span><br>    dataset = load_dataset(<span>"json"</span>, data_files=data_path, split=<span>"train[:1500]"</span>)<br><br>    <span><span>def</span> <span>format_example</span><span>(example)</span>:</span><br>        instruction = <span>f"诊断问题：<span>{example[<span>'Question'</span>]}</span>\n详细分析：<span>{example[<span>'Complex_CoT'</span>]}</span>"</span><br>        inputs = tokenizer(<br>            <span>f"<span>{instruction}</span>\n### 答案：\n<span>{example[<span>'Response'</span>]}</span>&lt;|endoftext|&gt;"</span>,<br>            padding=<span>"max_length"</span>,<br>            truncation=<span>True</span>,<br>            max_length=<span>512</span>,<br>            return_tensors=<span>"pt"</span><br>        )<br>        <span>return</span> {<span>"input_ids"</span>: inputs[<span>"input_ids"</span>].squeeze(<span>0</span>), <span>"attention_mask"</span>: inputs[<span>"attention_mask"</span>].squeeze(<span>0</span>)}<br><br>    <span>return</span> dataset.map(format_example, remove_columns=dataset.column_names)<br><br><span># LoRA配置</span><br>peft_config = LoraConfig(<br>    r=<span>16</span>,<br>    lora_alpha=<span>32</span>,<br>    target_modules=[<span>"q_proj"</span>, <span>"v_proj"</span>],<br>    lora_dropout=<span>0.05</span>,<br>    bias=<span>"none"</span>,<br>    task_type=<span>"CAUSAL_LM"</span><br>)<br><br><span># 训练参数配置</span><br>training_args = TrainingArguments(<br>    output_dir=output_path,<br>    per_device_train_batch_size=<span>2</span>,  <span># 显存优化设置</span><br>    gradient_accumulation_steps=<span>4</span>,  <span># 累计梯度相当于batch_size=8</span><br>    num_train_epochs=<span>3</span>,<br>    learning_rate=<span>3e-4</span>,<br>    fp16=<span>True</span>,  <span># 开启混合精度</span><br>    logging_steps=<span>20</span>,<br>    save_strategy=<span>"no"</span>,<br>    report_to=<span>"none"</span>,<br>    optim=<span>"adamw_torch"</span>,<br>    no_cuda=<span>False</span>,  <span># 强制使用CUDA</span><br>    dataloader_pin_memory=<span>False</span>,  <span># 加速数据加载</span><br>    remove_unused_columns=<span>False</span>,  <span># 防止删除未使用的列</span><br>    device=<span>"cuda:0"</span> <span># 指定使用的GPU设备    </span><br>)<br><br><span><span>def</span> <span>main</span><span>()</span>:</span><br>    <span># 创建输出目录</span><br>    os.makedirs(output_path, exist_ok=<span>True</span>)<br><br>    <span># 加载tokenizer</span><br>    tokenizer = AutoTokenizer.from_pretrained(model_path)<br>    tokenizer.pad_token = tokenizer.eos_token<br><br>    <span># 加载模型到GPU</span><br>    model = AutoModelForCausalLM.from_pretrained(<br>        model_path,<br>        torch_dtype=torch.float16,<br>        device_map=device<br>    )<br>    model = get_peft_model(model, peft_config)<br>    model.print_trainable_parameters()<br><br>    <span># 准备数据</span><br>    dataset = process_data(tokenizer)<br><br>    <span># 训练回调</span><br>    loss_callback = LossCallback()<br><br>    <span># 数据加载器</span><br>    <span><span>def</span> <span>data_collator</span><span>(data)</span>:</span><br>        batch = {<br>            <span>"input_ids"</span>: torch.stack([torch.tensor(d[<span>"input_ids"</span>]) <span>for</span> d <span>in</span> data]).to(device),<br>            <span>"attention_mask"</span>: torch.stack([torch.tensor(d[<span>"attention_mask"</span>]) <span>for</span> d <span>in</span> data]).to(device),<br>            <span>"labels"</span>: torch.stack([torch.tensor(d[<span>"input_ids"</span>]) <span>for</span> d <span>in</span> data]).to(device)  <span># 使用input_ids作为labels</span><br>        }<br>        <span>return</span> batch<br><br>    <span># 创建Trainer</span><br>    trainer = Trainer(<br>        model=model,<br>        args=training_args,<br>        train_dataset=dataset,<br>        data_collator=data_collator,<br>        callbacks=[loss_callback]<br>    )<br><br>    <span># 开始训练</span><br>    print(<span>"开始训练..."</span>)<br>    trainer.train()<br><br>    <span># 保存最终模型</span><br>    trainer.model.save_pretrained(output_path)<br>    print(<span>f"模型已保存至：<span>{output_path}</span>"</span>)<br><br>    <span># 绘制训练集损失Loss曲线</span><br>    plt.figure(figsize=(<span>10</span>, <span>6</span>))<br>    plt.plot(loss_callback.losses)<br>    plt.title(<span>"Training Loss Curve"</span>)<br>    plt.xlabel(<span>"Steps"</span>)<br>    plt.ylabel(<span>"Loss"</span>)<br>    plt.savefig(os.path.join(output_path, <span>"loss_curve.png"</span>))<br>    print(<span>"Loss曲线已保存"</span>)<br><br><span>if</span> __name__ == <span>"__main__"</span>:<br>    main()<br></code></pre><section><a target=_blank href="https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&amp;mid=2247701892&amp;idx=1&amp;sn=c324f7689c7e5c9d8cf320f2b65e772a&amp;scene=21#wechat_redirect" textvalue=微调的相关讲解可以直接参考上一篇公众号的内容 linktype=text imgurl imgdata=null data-itemshowtype=0 tab=innerlink data-linktype=2><span>微调的相关讲解可以直接参考上一篇公众号的内容</span></a><span>，我们看看 LOSS 曲线。</span></section><figure data-tool=mdnice编辑器><img alt data-imgfileid=100218444 data-ratio=0.6 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsG0NuVTbKjD4hyyC3EWREW9oumJF0wsS6FNmjUr4y7kKKQL4XhVRWol6ISOWORicXC2Oocy7yscZjA/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1000 src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsG0NuVTbKjD4hyyC3EWREW9oumJF0wsS6FNmjUr4y7kKKQL4XhVRWol6ISOWORicXC2Oocy7yscZjA/640?wx_fmt=png&amp;from=appmsg"></figure><p data-tool=mdnice编辑器><span>可以看到经过简单的微调，模型的 LOSS 值是有降低，说明 Deepseek 模型是对训练集的数据集有拟合的。</span></p><h2 data-tool=mdnice编辑器><span>2.直观比较模型生成</span></h2><p data-tool=mdnice编辑器><span>模型微调完，生成的内容效果如何，怎么进行比较呢？</span></p><p data-tool=mdnice编辑器><span>这个时候我们首先想到的是直接比较「微调模型」和「原始模型」对同一个问题生成的回答内容进行比较。</span></p><p data-tool=mdnice编辑器><span>因此我们可以统一提示词，统一相关的问题，然后比较生成的答案。</span></p><p data-tool=mdnice编辑器><span>具体代码如下：</span></p><pre data-tool=mdnice编辑器><code><span>import</span> torch<br><span>from</span> transformers <span>import</span> AutoTokenizer, AutoModelForCausalLM<br><span>from</span> peft <span>import</span> PeftModel<br><span>import</span> os<br><span>import</span> json<br><span>from</span> bert_score <span>import</span> score<br><span>from</span> tqdm <span>import</span> tqdm<br><span># 设置可见GPU设备（根据实际GPU情况调整）</span><br>os.environ[<span>"CUDA_VISIBLE_DEVICES"</span>] = <span>"0"</span>  <span># 指定仅使用GPU </span><br><br><span># 路径配置 ------------------------------------------------------------------------</span><br>base_model_path = <span>"xxxxx"</span>  <span># 原始预训练模型路径</span><br>peft_model_path = <span>"xxxxx"</span>  <span># LoRA微调后保存的适配器路径</span><br><br><span># 模型加载 ------------------------------------------------------------------------</span><br><span># 初始化分词器（使用与训练时相同的tokenizer）</span><br>tokenizer = AutoTokenizer.from_pretrained(base_model_path)<br><br><span># 加载基础模型（半精度加载节省显存）</span><br>base_model = AutoModelForCausalLM.from_pretrained(<br>    base_model_path,<br>    torch_dtype=torch.float16,  <span># 使用float16精度</span><br>    device_map=<span>"auto"</span>           <span># 自动分配设备（CPU/GPU）</span><br>)<br><br><span># 加载LoRA适配器（在基础模型上加载微调参数）</span><br>lora_model = PeftModel.from_pretrained(<br>    base_model, <br>    peft_model_path,<br>    torch_dtype=torch.float16,<br>    device_map=<span>"auto"</span><br>)<br><span># 合并LoRA权重到基础模型（提升推理速度，但会失去再次训练的能力）</span><br>lora_model = lora_model.merge_and_unload()  <br>lora_model.eval()  <span># 设置为评估模式</span><br><br><span># 生成函数 ------------------------------------------------------------------------</span><br><span><span>def</span> <span>generate_response</span><span>(model, prompt)</span>:</span><br>    <span>"""统一的生成函数<br>    参数：<br>        model : 要使用的模型实例<br>        prompt : 符合格式要求的输入文本<br>    返回：<br>        清洗后的回答文本<br>    """</span><br>    <span># 输入编码（保持与训练时相同的处理方式）</span><br>    inputs = tokenizer(<br>        prompt,<br>        return_tensors=<span>"pt"</span>,          <span># 返回PyTorch张量</span><br>        max_length=<span>1024</span>,               <span># 最大输入长度（与训练时一致）</span><br>        truncation=<span>True</span>,              <span># 启用截断</span><br>        padding=<span>"max_length"</span>          <span># 填充到最大长度（保证batch一致性）</span><br>    ).to(model.device)               <span># 确保输入与模型在同一设备</span><br><br>    <span># 文本生成（关闭梯度计算以节省内存）</span><br>    <span>with</span> torch.no_grad():<br>        outputs = model.generate(<br>            input_ids=inputs.input_ids,<br>            attention_mask=inputs.attention_mask,<br>            max_new_tokens=<span>1024</span>,       <span># 生成内容的最大token数（控制回答长度）</span><br>            temperature=<span>0.7</span>,         <span># 温度参数（0.0-1.0，值越大随机性越强）</span><br>            top_p=<span>0.9</span>,               <span># 核采样参数（保留累积概率前90%的token）</span><br>            repetition_penalty=<span>1.1</span>,  <span># 重复惩罚系数（&gt;1.0时抑制重复内容）</span><br>            eos_token_id=tokenizer.eos_token_id,  <span># 结束符ID</span><br>            pad_token_id=tokenizer.pad_token_id,  <span># 填充符ID </span><br>        )<br>    <br>    <span># 解码与清洗输出</span><br>    full_text = tokenizer.decode(outputs[<span>0</span>], skip_special_tokens=<span>True</span>)  <span># 跳过特殊token</span><br>    answer = full_text.split(<span>"### 答案：\n"</span>)[<span>-1</span>].strip()  <span># 提取答案部分</span><br>    <span>return</span> answer<br><br><span># 对比测试函数 --------------------------------------------------------------------</span><br><span><span>def</span> <span>compare_models</span><span>(question)</span>:</span><br>    <span>"""模型对比函数<br>    参数：<br>        question : 自然语言形式的医疗问题<br>    """</span><br>    <span># 构建符合训练格式的prompt（注意与训练时格式完全一致）</span><br>    prompt = <span>f"诊断问题：<span>{question}</span>\n详细分析：\n### 答案：\n"</span><br>    <br>    <span># 双模型生成</span><br>    base_answer = generate_response(base_model, prompt)  <span># 原始模型</span><br>    lora_answer = generate_response(lora_model, prompt)  <span># 微调模型</span><br>    <br>    <span># 终端彩色打印对比结果</span><br>    print(<span>"\n"</span> + <span>"="</span>*<span>50</span>)  <span># 分隔线</span><br>    print(<span>f"问题：<span>{question}</span>"</span>)<br>    print(<span>"-"</span>*<span>50</span>)<br>    print(<span>f"\033[1;34m[原始模型]\033[0m\n<span>{base_answer}</span>"</span>)  <span># 蓝色显示原始模型结果</span><br>    print(<span>"-"</span>*<span>50</span>)<br>    print(<span>f"\033[1;32m[LoRA模型]\033[0m\n<span>{lora_answer}</span>"</span>)  <span># 绿色显示微调模型结果</span><br>    print(<span>"="</span>*<span>50</span> + <span>"\n"</span>)<br><br><span># 主程序 ------------------------------------------------------------------------</span><br><span>if</span> __name__ == <span>"__main__"</span>:<br>        <span># 测试问题集（可自由扩展）</span><br>    test_questions = [<br>        <span>"根据描述，一个1岁的孩子在夏季头皮出现多处小结节，长期不愈合，且现在疮大如梅，溃破流脓，口不收敛，头皮下有空洞，患处皮肤增厚。这种病症在中医中诊断为什么病？"</span><br>    ]<br>    <br>    <span># 遍历测试问题</span><br>    <span>for</span> q <span>in</span> test_questions:<br>        compare_models(q)<br><br></code></pre><section><span>来看看模型对同一个问题输出结果的差异，这里为了凸显图像微调后与原始模型的差异，选择了训练集中的一条数据进行测试，读者可以根据自己的情况随机测试。</span></section><section><span>我们来看看生成的内容。</span></section><figure data-tool=mdnice编辑器><img alt data-imgfileid=100218445 data-ratio=0.3592592592592593 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsG0NuVTbKjD4hyyC3EWREW9waib6FZyNCPxyoVbxAZ7xIsy3cfxwQNJRUpyAjxGibaMia0EMq5DnDn5A/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsG0NuVTbKjD4hyyC3EWREW9waib6FZyNCPxyoVbxAZ7xIsy3cfxwQNJRUpyAjxGibaMia0EMq5DnDn5A/640?wx_fmt=png&amp;from=appmsg"></figure><figure data-tool=mdnice编辑器><img alt data-imgfileid=100218448 data-ratio=0.462037037037037 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsG0NuVTbKjD4hyyC3EWREW9eUlmopuCewGPJE0HfYmRrRWDfwEibtFg6g2aslt3YUSNibCVodbdrWFA/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsG0NuVTbKjD4hyyC3EWREW9eUlmopuCewGPJE0HfYmRrRWDfwEibtFg6g2aslt3YUSNibCVodbdrWFA/640?wx_fmt=png&amp;from=appmsg"></figure><p data-tool=mdnice编辑器><span>根据生成的内容，看起来 LoRA 微调后的模型好像还是和原始模型有些不同的，但是这个回答要比较的话就很抽象，毕竟作为学习者我们对医疗领域的问题可能了解的也不太多，能否通过一些比较直观的方法来体现微调后模型与原始模型的差异呢？</span></p><p data-tool=mdnice编辑器><span>这个时候我们想到了能否通过文本的相似性来评估，可以使用 bertscore 对模型进行比较，那 bertscore 是什么呢？我们来看看 Deepseek 满血版给我的答复，输出的内容太多了，这里就不全部粘贴过来，主体来说就是衡量语意的相似性，那我们似乎可以通过 berscore 来比较训练集的答案和模型生成的答案，来比较直观的看看微调后的模型与原始模型的差异。</span></p><figure data-tool=mdnice编辑器><img alt data-imgfileid=100218447 data-ratio=0.8152654867256637 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsG0NuVTbKjD4hyyC3EWREW9Dib35d0lzkK7sN2ZdAOdVDD14bbwkTxEhiccM0KpmibMrbXGHBkjbo5TA/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=904 src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsG0NuVTbKjD4hyyC3EWREW9Dib35d0lzkK7sN2ZdAOdVDD14bbwkTxEhiccM0KpmibMrbXGHBkjbo5TA/640?wx_fmt=png&amp;from=appmsg"></figure><p data-tool=mdnice编辑器><span>这里为了方便学习者进行学习，以下代码中选择的 bert 模型是最基础的 bert-base-chinese 模型，同样可以在魔塔社区进行下载。</span></p><p data-tool=mdnice编辑器><span>需要说明的是，考虑到部分学习者可能无法访问 hugging face 的官网，这里的 bert-base-chinese 模型采用离线的模型进行加载。</span></p><p data-tool=mdnice编辑器><span>温馨提示，模型的评估非常消耗资源，这里建议学习者只调用 10 条数据集即可。</span></p><p data-tool=mdnice编辑器><span>ok，我们来看看代码：</span></p><pre data-tool=mdnice编辑器><code><span>import</span> torch<br><span>from</span> transformers <span>import</span> AutoTokenizer, AutoModelForCausalLM<br><span>from</span> peft <span>import</span> PeftModel<br><span>import</span> os<br><span>import</span> json<br><span>from</span> bert_score <span>import</span> score<br><span>from</span> tqdm <span>import</span> tqdm<br><span># 设置可见GPU设备（根据实际GPU情况调整）</span><br>os.environ[<span>"CUDA_VISIBLE_DEVICES"</span>] = <span>"0"</span>  <span># 指定仅使用GPU </span><br><br><span># 路径配置 ------------------------------------------------------------------------</span><br>base_model_path = <span>"xxxxxx/DeepSeek-R1-Distill-Qwen-7B"</span>  <span># 原始预训练模型路径</span><br>peft_model_path = <span>"xxxxxx/output"</span>  <span># LoRA微调后保存的适配器路径</span><br><br><span># 模型加载 ------------------------------------------------------------------------</span><br><span># 初始化分词器（使用与训练时相同的tokenizer）</span><br>tokenizer = AutoTokenizer.from_pretrained(base_model_path)<br><br><span># 加载基础模型（半精度加载节省显存）</span><br>base_model = AutoModelForCausalLM.from_pretrained(<br>    base_model_path,<br>    torch_dtype=torch.float16,  <span># 使用float16精度</span><br>    device_map=<span>"auto"</span>           <span># 自动分配设备（CPU/GPU）</span><br>)<br><br><span># 加载LoRA适配器（在基础模型上加载微调参数）</span><br>lora_model = PeftModel.from_pretrained(<br>    base_model, <br>    peft_model_path,<br>    torch_dtype=torch.float16,<br>    device_map=<span>"auto"</span><br>)<br><span># 合并LoRA权重到基础模型（提升推理速度，但会失去再次训练的能力）</span><br>lora_model = lora_model.merge_and_unload()  <br>lora_model.eval()  <span># 设置为评估模式</span><br><br><span># 生成函数 ------------------------------------------------------------------------</span><br><span><span>def</span> <span>generate_response</span><span>(model, prompt)</span>:</span><br>    <span>"""统一的生成函数<br>    参数：<br>        model : 要使用的模型实例<br>        prompt : 符合格式要求的输入文本<br>    返回：<br>        清洗后的回答文本<br>    """</span><br>    <span># 输入编码（保持与训练时相同的处理方式）</span><br>    inputs = tokenizer(<br>        prompt,<br>        return_tensors=<span>"pt"</span>,          <span># 返回PyTorch张量</span><br>        max_length=<span>1024</span>,               <span># 最大输入长度（与训练时一致）</span><br>        truncation=<span>True</span>,              <span># 启用截断</span><br>        padding=<span>"max_length"</span>          <span># 填充到最大长度（保证batch一致性）</span><br>    ).to(model.device)               <span># 确保输入与模型在同一设备</span><br><br>    <span># 文本生成（关闭梯度计算以节省内存）</span><br>    <span>with</span> torch.no_grad():<br>        outputs = model.generate(<br>            input_ids=inputs.input_ids,<br>            attention_mask=inputs.attention_mask,<br>            max_new_tokens=<span>1024</span>,       <span># 生成内容的最大token数（控制回答长度）</span><br>            temperature=<span>0.7</span>,         <span># 温度参数（0.0-1.0，值越大随机性越强）</span><br>            top_p=<span>0.9</span>,               <span># 核采样参数（保留累积概率前90%的token）</span><br>            repetition_penalty=<span>1.1</span>,  <span># 重复惩罚系数（&gt;1.0时抑制重复内容）</span><br>            eos_token_id=tokenizer.eos_token_id,  <span># 结束符ID</span><br>            pad_token_id=tokenizer.pad_token_id,  <span># 填充符ID </span><br>        )<br>    <br>    <span># 解码与清洗输出</span><br>    full_text = tokenizer.decode(outputs[<span>0</span>], skip_special_tokens=<span>True</span>)  <span># 跳过特殊token</span><br>    answer = full_text.split(<span>"### 答案：\n"</span>)[<span>-1</span>].strip()  <span># 提取答案部分</span><br>    <span>return</span> answer<br><br><span># 对比测试函数 --------------------------------------------------------------------</span><br><span><span>def</span> <span>compare_models</span><span>(question)</span>:</span><br>    <span>"""模型对比函数<br>    参数：<br>        question : 自然语言形式的医疗问题（如"小孩感冒怎么办？"）<br>    """</span><br>    <span># 构建符合训练格式的prompt（注意与训练时格式完全一致）</span><br>    prompt = <span>f"诊断问题：<span>{question}</span>\n详细分析：\n### 答案：\n"</span><br>    <br>    <span># 双模型生成</span><br>    base_answer = generate_response(base_model, prompt)  <span># 原始模型</span><br>    lora_answer = generate_response(lora_model, prompt)  <span># 微调模型</span><br>    <br>    <span># 终端彩色打印对比结果</span><br>    print(<span>"\n"</span> + <span>"="</span>*<span>50</span>)  <span># 分隔线</span><br>    print(<span>f"问题：<span>{question}</span>"</span>)<br>    print(<span>"-"</span>*<span>50</span>)<br>    print(<span>f"\033[1;34m[原始模型]\033[0m\n<span>{base_answer}</span>"</span>)  <span># 蓝色显示原始模型结果</span><br>    print(<span>"-"</span>*<span>50</span>)<br>    print(<span>f"\033[1;32m[LoRA模型]\033[0m\n<span>{lora_answer}</span>"</span>)  <span># 绿色显示微调模型结果</span><br>    print(<span>"="</span>*<span>50</span> + <span>"\n"</span>)<br><br><span># 主程序 ------------------------------------------------------------------------</span><br><span>if</span> __name__ == <span>"__main__"</span>:<br>    <span># 测试问题集（可自由扩展）</span><br>    <span># test_questions = [</span><br>    <span>#     "根据描述，一个1岁的孩子在夏季头皮出现多处小结节，长期不愈合，且现在疮大如梅，溃破流脓，口不收敛，头皮下有空洞，患处皮肤增厚。这种病症在中医中诊断为什么病？"</span><br>    <span># ]</span><br>    <br>    <span># # 遍历测试问题</span><br>    <span># for q in test_questions:</span><br>    <span>#     compare_models(q)</span><br>    <span># 加载测试数据</span><br>    <span>####-----------批量测试---------------#</span><br>    <span>with</span> open(<span>"xxxxxx/data/medical_o1_sft_Chinese.json"</span>) <span>as</span> f:<br>        test_data = json.load(f) <br><br>    <span># 数据量比较大，我们只选择10条数据进行测试</span><br>    test_data=test_data[:<span>10</span>]<br>    <span># 批量生成回答</span><br>    <span><span>def</span> <span>batch_generate</span><span>(model, questions)</span>:</span><br>        answers = []<br>        <span>for</span> q <span>in</span> tqdm(questions):<br>            prompt = <span>f"诊断问题：<span>{q}</span>\n详细分析：\n### 答案：\n"</span><br>            ans = generate_response(model, prompt)<br>            answers.append(ans)<br>        <span>return</span> answers<br><br>    <span># 生成结果</span><br>    base_answers = batch_generate(base_model, [d[<span>"Question"</span>] <span>for</span> d <span>in</span> test_data])<br>    lora_answers = batch_generate(lora_model, [d[<span>"Question"</span>] <span>for</span> d <span>in</span> test_data])<br>    ref_answers = [d[<span>"Response"</span>] <span>for</span> d <span>in</span> test_data]<br><br>    bert_model_path=<span>"xxxxx/model/bert-base-chinese"</span><br>    <span># 计算BERTScore</span><br>    _, _, base_bert = score(base_answers, ref_answers, lang=<span>"zh"</span>,model_type=bert_model_path,num_layers=<span>12</span>,device=<span>"cuda"</span>)<br>    _, _, lora_bert = score(lora_answers, ref_answers, lang=<span>"zh"</span>,model_type=bert_model_path,num_layers=<span>12</span>,device=<span>"cuda"</span>)<br>    print(<span>f"BERTScore | 原始模型: <span>{base_bert.mean().item():<span>.3</span>f}</span> | LoRA模型: <span>{lora_bert.mean().item():<span>.3</span>f}</span>"</span>)<br><br></code></pre><p data-tool=mdnice编辑器>我们来看看结果：</p><figure data-tool=mdnice编辑器><img alt=结果 data-imgfileid=100218451 data-ratio=0.06334841628959276 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsG0NuVTbKjD4hyyC3EWREW9tfBw7tZcwyQhUzKPKtbggiaru2vSicW0yt87icJI0ZLcnGJsVg44Xdw8A/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=442 src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsG0NuVTbKjD4hyyC3EWREW9tfBw7tZcwyQhUzKPKtbggiaru2vSicW0yt87icJI0ZLcnGJsVg44Xdw8A/640?wx_fmt=png&amp;from=appmsg"><figcaption>结果</figcaption></figure><p data-tool=mdnice编辑器><span>可以看到利用 bertscore 比较数据集的参考答案与模型生成答案的相似性来看，LoRA微调后的结果和原始模型相比还是有细微的差异，随着 LoRA 微调的训练轮次加深，甚至我们故意让大模型产生“过拟合”后，比较这个相似性，这个结果的差异应该会进一步加大，可以从一个相对定性的角度给学习者提供一个新的视角。</span></p><h2 data-tool=mdnice编辑器><span>3. 后记</span></h2><p data-tool=mdnice编辑器><span>大模型的评测是一个相对来说比较复杂且体系的内容，特别是金融与医疗领域涉及到比较强专业性，实际的企业部署过程中会有更加多样化的方法来评估模型生成的好坏。</span></p><p data-tool=mdnice编辑器><span>本文尽可能的从初学者的角度去切入，让学习者能比较简单且直接的了解模型微调后与原始模型的差异。</span></p><p data-tool=mdnice编辑器><span>本文的目的旨在对<a target=_blank href="https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&amp;mid=2247701892&amp;idx=1&amp;sn=c324f7689c7e5c9d8cf320f2b65e772a&amp;scene=21#wechat_redirect" textvalue=" Deepseek 微调文章" linktype=text imgurl imgdata=null data-itemshowtype=0 tab=innerlink data-linktype=2>「Deepseek 微调文章</a>」后续工作的延伸，也期望通过这种比较初级的方法帮助学习者了解微调与模型评测，起到抛砖引玉的效果，如果学习者对大模型评测有深厚的兴趣，可以从不同的角度进行学习。</span><span></span></p></section><p><span><img alt=图片 data-backh=234 data-backw=578 data-imgfileid=100218454 data-ratio=0.40555555555555556 data-w=900 data-src="https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsGxu3P5YibTO899okS0X9WaLmQCtia4U8Eu1xWCz9t8Qtq9PH6T1bTcxibiaCIkGzAxpeRkRFYqibVmwSw/640?wx_fmt=other&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1&amp;tp=webp" src="https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsGxu3P5YibTO899okS0X9WaLmQCtia4U8Eu1xWCz9t8Qtq9PH6T1bTcxibiaCIkGzAxpeRkRFYqibVmwSw/640?wx_fmt=other&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1&amp;tp=webp"></span><strong><span><span>一起“</span><span><span>点赞</span></span><span><strong><span>”</span></strong></span><strong><span>三连</span></strong><span>↓</span></span></strong><span></span></p><p><mp-style-type data-value=10000></mp-style-type></p></div><hr><a href=https://mp.weixin.qq.com/s/ndNDMyipWJQzthRoJppIBw ,target=_blank rel="noopener noreferrer">原文链接</a></div><footer class=post__footer><div class="post__tags tags clearfix"><svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M4 0h8s2 0 4 2l15 15s2 2 0 4L21 31s-2 2-4 0L2 16s-2-2-2-4V3s0-3 4-3m3 10a3 3 0 000-6 3 3 0 000 6"/></svg><ul class=tags__list><li class=tags__item><a class="tags__link btn" href=../../../tags/fetched/ rel=tag>fetched</a></li><li class=tags__item><a class="tags__link btn" href=../../../tags/datawhale/ rel=tag>Datawhale</a></li></ul></div></footer></article></main><div class="authorbox clearfix"><div class=authorbox__header><span class=authorbox__name>About Bloger</span></div></div><nav class="pager flex"><div class="pager__item pager__item--prev"><a class=pager__link href=../../../posts/2025-02/chatgpt_%E9%99%8D%E6%99%BA%E8%A7%A3%E6%95%91___deepseek_%E8%81%94%E7%BD%91%E5%AE%9E%E6%B5%8B/ rel=prev><span class=pager__subtitle>«&#8201;Previous</span><p class=pager__title>ChatGPT 降智解救 & DeepSeek 联网实测</p></a></div><div class="pager__item pager__item--next"><a class=pager__link href=../../../posts/2025-02/%E9%A9%BE%E9%A9%AD_deep_research_%E4%BD%A0%E5%BF%85%E9%A1%BB%E7%9F%A5%E9%81%93%E7%9A%84_100_%E4%BB%B6%E4%BA%8B_%E4%B8%80%E6%AC%A1%E6%80%A7%E6%9E%84%E5%BB%BA%E5%87%BA%E4%BD%A0%E7%9A%84_deep_research_%E4%B8%AA%E4%BA%BA%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/ rel=next><span class=pager__subtitle>Next&#8201;»</span><p class=pager__title>驾驭 deep research，你必须知道的 100 件事｜一次性构建出你的 deep research 个人知识体系</p></a></div></nav><section class=comments><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//kkitown.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section></div><aside class=sidebar><div class="widget-search widget"><form class=widget-search__form role=search method=get action=https://google.com/search><input class=widget-search__field type=search placeholder=Search… name=q aria-label=Search…>
<input class=widget-search__submit type=submit value=Search>
<input type=hidden name=sitesearch value=https://ixxmu.github.io/></form></div><div class="widget-recent widget"><h4 class=widget__title>Recent Posts</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=../../../posts/2025-09/%E9%97%B2%E9%B1%BC%E4%B8%8A%E9%82%A3%E4%BA%9B%E9%80%86%E5%A4%A9%E7%9A%84%E6%B5%8B%E5%BA%8F%E4%BB%AA/>闲鱼上那些逆天的测序仪</a></li><li class=widget__item><a class=widget__link href=../../../posts/2025-09/%E8%AF%B4%E8%B5%B7%E8%BE%9B%E8%8A%B7%E8%95%BE_%E6%88%91%E6%83%B3%E8%B5%B7%E6%97%A9%E5%B9%B4%E4%B8%80%E4%BA%9B%E5%93%81%E5%91%B3_%E7%8B%AC%E7%89%B9_%E7%9A%84%E6%81%90%E6%80%96%E7%89%87/>说起辛芷蕾，我想起早年一些品味“独特”的恐怖片</a></li><li class=widget__item><a class=widget__link href=../../../posts/2025-09/%E5%A4%9A%E6%AC%A1%E7%94%9F%E5%AD%90%E5%90%8E_28%E5%B2%81%E5%86%9C%E6%9D%91%E6%99%BA%E9%9A%9C%E5%A5%B3%E5%AD%A9%E6%82%84%E7%84%B6%E7%A6%BB%E4%B8%96/>多次生子后，28岁农村智障女孩悄然离世</a></li><li class=widget__item><a class=widget__link href=../../../posts/2025-09/openai%E7%BD%95%E8%A7%81%E5%8F%91%E8%AE%BA%E6%96%87_%E6%88%91%E4%BB%AC%E6%89%BE%E5%88%B0%E4%BA%86ai%E5%B9%BB%E8%A7%89%E7%9A%84%E7%BD%AA%E9%AD%81%E7%A5%B8%E9%A6%96/>OpenAI罕见发论文：我们找到了AI幻觉的罪魁祸首</a></li><li class=widget__item><a class=widget__link href=../../../posts/2025-09/%E5%8D%97%E4%BA%AC%E5%8D%96%E6%B7%AB%E5%A4%B4%E7%9B%AE1_34%E4%BA%BF%E6%B8%AF%E5%85%83%E8%A2%AB%E7%91%9E%E9%93%B6%E5%8E%9F%E5%89%AF%E6%80%BB%E7%9B%91%E7%A7%81%E5%90%9E/>南京卖淫头目1.34亿港元被瑞银原副总监私吞</a></li></ul></div></div><div class="widget-categories widget"><h4 class=widget__title>Categories</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=../../../categories/duty/>Duty</a></li><li class=widget__item><a class=widget__link href=../../../categories/duty2/>Duty2</a></li></ul></div></div><div class="widget-taglist widget"><h4 class=widget__title>Tags</h4><div class=widget__content><a class="widget-taglist__link widget__link btn" href=../../../tags/cnbeta/ title=Cnbeta>Cnbeta (148)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/datawhale/ title=Datawhale>Datawhale (9)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/drpei/ title=Drpei>Drpei (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/fetched/ title=Fetched>Fetched (755)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/githubdaily/ title=GitHubDaily>GitHubDaily (7)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E4%B8%81%E9%A6%99%E5%9B%AD/ title=丁香园>丁香园 (30)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E4%BA%BA%E7%89%A9/ title=人物>人物 (15)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E5%8D%97%E6%96%B9%E5%91%A8%E6%9C%AB/ title=南方周末>南方周末 (11)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E5%B0%8F%E4%BC%97%E8%BD%AF%E4%BB%B6/ title=小众软件>小众软件 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E5%B0%8F%E5%A4%A7%E5%A4%AB%E6%BC%AB%E7%94%BB/ title=小大夫漫画>小大夫漫画 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E5%B0%91%E6%95%B0%E6%B4%BE/ title=少数派>少数派 (6)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E5%BE%AA%E5%9B%A0%E7%BC%89%E8%8D%AF/ title=循因缉药>循因缉药 (9)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%96%B0%E4%B9%A1%E5%9C%9F/ title=新乡土>新乡土 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%98%9F%E7%90%83%E5%95%86%E4%B8%9A%E8%AF%84%E8%AE%BA/ title=星球商业评论>星球商业评论 (7)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%9C%88%E5%85%89%E5%8D%9A%E5%AE%A2/ title=月光博客>月光博客 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%9C%AA%E9%97%BBcode/ title=未闻Code>未闻Code (6)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%A7%BD%E8%BE%B9%E5%BE%80%E4%BA%8B/ title=槽边往事>槽边往事 (9)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%AF%8F%E6%97%A5%E4%BA%BA%E7%89%A9/ title=每日人物>每日人物 (10)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%B5%AA%E6%BD%AE%E5%B7%A5%E4%BD%9C%E5%AE%A4/ title=浪潮工作室>浪潮工作室 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%B5%AE%E4%B9%8B%E9%9D%99/ title=浮之静>浮之静 (6)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%81%BC%E8%A7%81/ title=灼见>灼见 (8)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%86%8A%E8%A8%80%E7%86%8A%E8%AF%AD/ title=熊言熊语>熊言熊语 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%94%9F%E4%BF%A1%E6%8A%80%E8%83%BD%E6%A0%91/ title=生信技能树>生信技能树 (10)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%94%9F%E4%BF%A1%E8%8F%9C%E9%B8%9F%E5%9B%A2/ title=生信菜鸟团>生信菜鸟团 (6)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%9F%A5%E8%AF%86%E5%88%86%E5%AD%90/ title=知识分子>知识分子 (14)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%BD%91%E6%98%93%E6%95%B0%E8%AF%BB/ title=网易数读>网易数读 (7)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E8%B4%A2%E6%96%B0/ title=财新>财新 (6)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E8%B4%A2%E7%BB%8F%E4%B8%89%E5%88%86%E9%92%9F/ title=财经三分钟>财经三分钟 (7)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E8%B5%9B%E5%85%88%E7%94%9F/ title=赛先生>赛先生 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E9%83%91%E5%B7%9E%E6%A5%BC%E5%B8%82/ title=郑州楼市>郑州楼市 (32)</a></div><a href=../../../tags/></a></div><div class="widget-social widget"><h4 class="widget-social__title widget__title">Social</h4><div class="widget-social__content widget__content"><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=Facebook rel="noopener noreferrer" href=https://facebook.com/username target=_blank><svg class="widget-social__link-icon icon icon-facebook" width="24" height="24" viewBox="0 0 352 352"><path d="m0 32v288c0 17.5 14.5 32 32 32h288c17.5.0 32-14.5 32-32V32c0-17.5-14.5-32-32-32H32C14.5.0.0 14.5.0 32zm320 0v288h-83V212h41.5l6-48H237v-31c0-14 3.5-23.5 23.5-23.5h26V66c-4.4-.6-19.8-1.5-37.5-1.5-36.9.0-62 22.2-62 63.5v36h-42v48h42v108H32V32z"/></svg>
<span>Facebook</span></a></div><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=Twitter rel="noopener noreferrer" href=https://twitter.com/username target=_blank><svg class="widget-social__link-icon icon icon-twitter" width="24" height="24" viewBox="0 0 384 312"><path d="m384 36.9c-14.1 6.3-29.3 10.5-45.2 12.4 16.3-9.7 28.8-25.2 34.6-43.6-15.2 9-32.1 15.6-50 19.1-14.4-15.2-34.9-24.8-57.5-24.8-43.5.0-78.8 35.3-78.8 78.8.0 6.2.7 12.2 2 17.9-65.5-3.3-123.5-34.6-162.4-82.3C20 26 16.1 39.6 16.1 54c0 27.3 13.9 51.4 35 65.6-12.9-.4-25.1-4-35.7-9.9v1c0 38.2 27.2 70 63.2 77.2-6.6 1.8-13.6 2.8-20.8 2.8-5.1.0-10-.5-14.8-1.4 10 31.3 39.1 54.1 73.6 54.7-27 21.1-60.9 33.7-97.8 33.7-6.4.0-12.6-.4-18.8-1.1C34.9 299 76.3 312 120.8 312c144.9.0 224.1-120 224.1-224.1.0-3.4-.1-6.8-.2-10.2 15.4-11.1 28.7-25 39.3-40.8z"/></svg>
<span>Twitter</span></a></div><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=Instagram rel="noopener noreferrer" href=https://www.instagram.com/username target=_blank><svg class="widget-social__link-icon icon icon-instagram" width="24" height="24" viewBox="0 0 256 256"><circle cx="193" cy="59" r="15"/><path fill-rule="evenodd" d="M101 0h54c41 0 58.4 3.9 74.5 17C256.2 37.5 256 74.8 256 97.7v60c0 26.7.0 60.4-26.5 81.4-16 13.4-33.5 16.9-74.5 16.9h-54c-41 0-57.5-3.5-74.5-16.9C1 218.9.5 186.3.1 160.5L0 155V97.7c0-23-.2-60.2 26.5-80.7C45 2 60 0 101 0zm4.9 23h44.3c45.8.0 58.3 3.5 70.3 17.5 11.8 13.2 12 30.1 12.5 62.9V156c.2 20.8.3 45.8-12.5 59.5-12 14-24.5 17.5-70.3 17.5h-44.3c-45.9.0-57.3-3.5-70.4-17.5-12.2-13-12.3-36.5-12.4-56.7v-55.6c.4-32.6.7-49.6 12.4-62.7C48 26.5 60 23 105.9 23zm19.6 144.5a42 42 0 100-84 42 42 0 000 84zm0 22.5a64.5 64.5.0 100-129 64.5 64.5.0 000 129z"/></svg>
<span>Instagram</span></a></div></div></div></aside></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2025 文字轨迹.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=../../../js/menu.js></script><script src=../../../js/custom.js></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script></body></html>