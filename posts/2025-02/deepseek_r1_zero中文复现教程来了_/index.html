<!doctype html><html class=no-js lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>DeepSeek R1 Zero中文复现教程来了！ - 文字轨迹</title>
<script>(function(e,t){e[t]=e[t].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:url" content="https://ixxmu.github.io/posts/2025-02/deepseek_r1_zero%E4%B8%AD%E6%96%87%E5%A4%8D%E7%8E%B0%E6%95%99%E7%A8%8B%E6%9D%A5%E4%BA%86_/"><meta property="og:site_name" content="文字轨迹"><meta property="og:title" content="DeepSeek R1 Zero中文复现教程来了！"><meta property="og:description" content="DeepSeek R1 Zero中文复现教程来了！ by Datawhale Datawhale干货 作者：骆秀韬，Datawhale成员"><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-06T14:21:31+00:00"><meta property="article:modified_time" content="2025-02-06T14:21:31+00:00"><meta property="article:tag" content="Fetched"><meta property="article:tag" content="Datawhale"><meta itemprop=name content="DeepSeek R1 Zero中文复现教程来了！"><meta itemprop=description content="DeepSeek R1 Zero中文复现教程来了！ by Datawhale Datawhale干货 作者：骆秀韬，Datawhale成员"><meta itemprop=datePublished content="2025-02-06T14:21:31+00:00"><meta itemprop=dateModified content="2025-02-06T14:21:31+00:00"><meta itemprop=wordCount content="1880"><meta itemprop=keywords content="Fetched,Datawhale"><meta name=twitter:card content="summary"><meta name=twitter:title content="DeepSeek R1 Zero中文复现教程来了！"><meta name=twitter:description content="DeepSeek R1 Zero中文复现教程来了！ by Datawhale Datawhale干货 作者：骆秀韬，Datawhale成员"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=../../../css/style.css><link rel=stylesheet href=../../../css/custom.css><link rel="shortcut icon" href=../../../favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=../../../ title=文字轨迹 rel=home><div class="logo__item logo__text"><div class=logo__title>文字轨迹</div><div class=logo__tagline>故事流淌过的地方</div></div></a></div><nav class=menu><button class=menu__btn aria-haspopup=true aria-expanded=false tabindex=0>
<span class=menu__btn-title tabindex=-1>Menu</span></button><ul class=menu__list><li class=menu__item><a class=menu__link href=../../../posts/><span class=menu__text>文章</span></a></li><li class=menu__item><a class=menu__link href=../../../tags/><span class=menu__text>标签</span></a></li><li class=menu__item><a class=menu__link href=../../../about/><span class=menu__text>关于</span></a></li></ul></nav></div></header><div class="wrapper flex"><div class=primary><main class=main role=main><article class=post><header class=post__header><meta name=referrer content="never"><h1 class=post__title>DeepSeek R1 Zero中文复现教程来了！</h1><div class="post__meta meta"><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0a14 14 0 110 28 1 1 0 010-28m0 3a3 3 0 100 22 3 3 0 000-22m1 4h-2v8.4l6.8 4.4L22 18l-6-3.8z"/></svg><time class=meta__text datetime=2025-02-06T14:21:31Z>2025-02-06</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=../../../categories/duty/ rel=category>Duty</a></span></div><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 16 16"><path d="M8 1c2 0 3.5 2 3.5 4.5S10 9 10 9c3 1 4 2 4 6H2c0-4 1-5 4-6 0 0-1.5-1-1.5-3.5S6 1 8 1"/></svg><span class=meta__text>Bloger</span></div></div></header><div class="post__toc toc"><div class=toc__title>Page content</div><div class=toc__menu><nav id=TableOfContents><ul><li><a href=#deepseek-r1-zero中文复现教程来了-by-datawhale>DeepSeek R1 Zero中文复现教程来了！ by Datawhale</a></li></ul></nav></div></div><div class="content post__content clearfix"><h2 id=deepseek-r1-zero中文复现教程来了-by-datawhale>DeepSeek R1 Zero中文复现教程来了！ by Datawhale</h2><div><section data-mpa-powered-by=yiban.io data-style="-webkit-tap-highlight-color: transparent; margin-bottom: 0px; outline: 0px; text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px; background-color: rgb(255, 255, 255); letter-spacing: 0.544px; text-wrap: wrap; caret-color: rgb(34, 34, 34); font-family: 'Helvetica Neue', Helvetica, 'Hiragino Sans GB', 'Microsoft YaHei', Arial, sans-serif; visibility: visible; line-height: 27.2px; color: rgb(163, 163, 163) !important;" mp-original-font-size=17 mp-original-line-height=27.200000762939453><h1 data-tool=mdnice编辑器><section><section data-style="-webkit-tap-highlight-color: transparent; margin-bottom: 0px; outline: 0px; text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px; background-color: rgb(255, 255, 255); letter-spacing: 0.544px; text-wrap: wrap; caret-color: rgb(34, 34, 34); font-family: 'Helvetica Neue', Helvetica, 'Hiragino Sans GB', 'Microsoft YaHei', Arial, sans-serif; visibility: visible; line-height: 27.2px; color: rgb(163, 163, 163) !important;" mp-original-font-size=17 mp-original-line-height=27.200000762939453><section><section powered-by=xiumi.us><section><section data-id=85660 data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)"><section><p><span> Datawhale干货 </span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);"><p><span><strong>作者</strong><strong>：</strong><strong>骆秀韬，Datawhale成员</strong></span></p></section></section></section></section><section><mp-common-profile data-id="MzIyNjM2MzQyNg==" data-pluginname=mpprofile data-headimg="http://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsEXsBwQkpYLtE2vhn7Z3RVOSRu5M1VicIgqgMRKLsxsibK7OUSqUb1rUO4pfXnQyFYKqhryAIeh4MOg/300?wx_fmt=png&amp;wxfrom=19" data-nickname=Datawhale data-alias=Datawhale data-signature="一个专注于AI领域的开源组织，汇聚了众多优秀学习者，使命-for the learner，和学习者一起成长。" data-from=2 data-weuitheme=light data-origin_num=702 data-isban=0 data-biz_account_status=0 data-index=0 data-is_biz_ban=0></mp-common-profile></section></section></section></section></section></h1></section><section><span>项目代码可见：unlock-deepseek/Datawhale-R1（https://github.com/datawhalechina/unlock-deepseek），欢迎关注和 star！</span></section><section><span><strong><span>其余所有开源内容见文末。</span></strong></span></section><hr><section>各位同学好，我是来自 Unlock-DeepSeek 开源项目团队的骆师傅。先说结论，<span>我们（Datawhale X 似然实验室）使用 3 张 A800(80G) 计算卡，花了 20 小时训练时间，做出了可能是国内首批 DeepSeek R1 Zero 的中文复现版本，我们把它叫做 Datawhale-R1，用于 R1 Zero 复现教学。</span>　</section><p><img data-imgfileid=100216634 data-ratio=0.3972222222222222 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUds9yrFwyfhXWBiauyA0pRkib9r9seYR79TQffkpraEEtb8HGLic3YcarQ/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUds9yrFwyfhXWBiauyA0pRkib9r9seYR79TQffkpraEEtb8HGLic3YcarQ/640?wx_fmt=png&amp;from=appmsg"></p><p>按照 5.5 元 ~ 7.0 元每小时的价格计算，3 张 A800 花费最低为 3 * 5.5 * 20 = 330 元，预计花费接近 420 元，而 <span>TinyZero</span><span>（https://github.com/Jiayi-Pan/TinyZero） </span>项目用了 4 张 A800 训练了 8 小时，预计花费为：224 元，这中间的差异可能是由于硬件性能瓶颈和框架差异带来的（我们用的是 Huggingface TRL，TinyZero 使用的是 veRL）。所以建议<span>大家如果真的要复现，请使用 TinyZero 项目</span>，我们出于教育目的使用 TRL 为大家报告这个结果。　</p><p>另外，不是所有人都能随时随地调用 3 张 A800 的，我们正在努力减小硬件资源要求，让复现工作尽可能平民化（比如在 4090 上跑）。在这里特别感谢：似然实验室，提供本次复现的计算资源，并与 Datawhale 团队合作贡献了本教程。　</p><section><mp-common-profile data-pluginname=mpprofile data-id="MzI5NzQxODU2Nw==" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/lT7vzwjGsCheD71q8AZsAuppFnRUmy5JSBHMhoI0fUto4TEV9pwvIremTGpPs1V0YvHeV0WlebqQVuBUDibJwqA/0?wx_fmt=png" data-nickname=似然实验室 data-alias=LikelihoodLab data-signature="人工智能实验室，我们相信AI 改变世界，让世界更加美好。" data-from=0 data-is_biz_ban=0 data-service_type=1></mp-common-profile></section><p>回到正题，首先回答一个关键问题：为什么这个方案更贵，而我们却选择了它？答案就是：它更符合教育目的，截止本文发布，大部分同学没有足够的资源来亲手体验复现流程，但是我们希望大家能更清楚的看到，复现 R1 Zero 的过程中都发生了什么，真正对复现原理有个大致把握，就算做“云玩家”也要学到知识，看完骆师傅做一遍就好像自己也做了一遍。　</p><section><p>本方案在 <span>mini-r1</span><span>（https://www.philschmid.de/mini-deepseek-r1）</span>的基础上改进而来。　</p></section><h1>环境搭建</h1><h2>配置基础工具</h2><p>首先我们要搭建环境，作为手把手教程以及骆师傅的看家本领，我们会在这部分说得细致些。结合国内的实际情况，我们需要的环境信息如下：　</p><section><p>暂时无法支持非 Linux 系统（Windows、MacOS）　</p></section><ul><li><section><span>CUDA > 12.0 （我们使用的是 CUDA 12.4）</span></section></li><li><section><span>Python 建议版本为 3.12（我们使用 Miniforge 管理虚拟环境）</span></section></li><li><section><span>Pytorch 版本为 2.5.1 (GPU版本，请使用 </span><span>torch.cuda.is_available()</span><span> 检查能否正常识别 GPU 设备）</span></section></li></ul><section><p>建议使用 Miniforge / Conda 来安装 Pytorch，我们在南方科技大学的开源镜像源测试，下载速度会比官网 pip 安装快不少，请在下面的网址找到适合你硬件的 2.5.1 版本：https://pytorch.org/get-started/previous-versions/，推荐使用 mamba 安装（安装 Miniforge 后直接将 conda 替换为 mamba）　</p></section><h2>编译安装 flash-attn</h2><p>接着重头戏就来了，我们需要编译安装 Flash Attention 包，这步非常消耗 CPU 资源，非常不建议CPU核心少的玩家执行。如果你没有办法在“有生之年”编译完 Flash Attention，可以在 https://github.com/Dao-AILab/flash-attention/releases/ 找到与你环境对应的编译好的包。（没对应上的话，改环境反而更快，相信我，编译很慢）　</p><p>这个步骤倒是很简单，执行下面的命令：　</p><section><p><br></p><pre><p>pip install packaging<br>pip install ninja <span># 用于加速编译</span><br><br><span># 编译安装 Flash Attention 包</span><br>pip install flash-attn --no-build-isolation<br><br><span># 注意！如果你的设备CPU核心多，但是运行内存小于 96 GB，请适当设置 MAX_JOBS 的数量，并替换为下面的命令，参考：https://github.com/Dao-AILab/flash-attention#installation-and-features</span><br>MAX_JOBS=4 pip install flash-attn --no-build-isolation</p></pre></section><p>按下回车后，可以泡杯咖啡，打开 <span>htop</span> 看 CPU 疯狂运作，再重新品读一遍<span>《DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning》</span><span>（https://arxiv.org/abs/2501.12948）　</span></p><p>等待 flash-attn 安装完毕后，我们就可以安装其他涉及到的库了，我们提供了一份 <span>requirements.txt</span> 在<span> Unlock-DeepSeek</span><span>（https://github.com/datawhalechina/unlock-deepseek）</span>项目，核心列表如下：　</p><section><p><br></p><pre><p>setuptools&lt;<span>71.0.0</span><br>transformers==<span>4.48.1</span><br>datasets==<span>3.1.0</span><br>accelerate==<span>1.3.0</span><br>hf-transfer==<span>0.1.9</span><br>deepspeed==<span>0.15.4</span><br>trl==<span>0.14.0</span><br>vllm==<span>0.7.0</span><br>modelscope==<span>1.22.3</span><br>swanlab==<span>0.4.6</span><br>huggingface-hub==<span>0.28.1</span></p></pre></section><section><p>大家也可以在这个地址找到我们所有涉及的 Python 包列表：https://swanlab.cn/@anine09/datawhale-r1/runs/4tp31j1zxbm1fshjsi53b/environment/requirements　</p></section><h2>下载模型和数据集</h2><p>接下来我们需要下载数据集和模型，在本次实验中，我们使用的数据集为：<span>Jiayi-Pan/Countdown-Tasks-3to4</span><span>（https://huggingface.co/datasets/Jiayi-Pan/Countdown-Tasks-3to4）</span>，模型为：<span>Qwen/Qwen2.5-3B-Instruct</span><span>（https://huggingface.co/Qwen/Qwen2.5-3B-Instruct）</span>，我们目前不建议用小于 3B 的模型（其他社区多次报告，小于 3B 的模型无法学会推理，经过我们的测试，确实！）　</p><p>数据集下载方式：　</p><section><p><br></p><pre><p><span>export</span> HF_ENDPOINT=https://hf-mirror.com <span># 更换为国内镜像源，这个只用执行一次，每次重新打开终端就要重新执行，或者写入 .bashrc</span><br><br><span># 下载数据集，替换整个 &lt;xxx&gt; 为你自己的内容</span><br>huggingface-cli download --repo-type dataset --resume-download Jiayi-Pan/Countdown-Tasks-3to4 --local-dir &lt;你想要存放的路径，比如：dataset&gt;</p></pre></section><p>模型下载方式，哪个速度快用哪个：　</p><ul><li><section><span>方案一，Huggingface 镜像源</span></section></li></ul><section><p><br></p><pre><p><span># 下载模型，替换整个 &lt;xxx&gt; 为你自己的内容</span><br>huggingface-cli download --resume-download Qwen/Qwen2.5-3B-Instruct --local-dir &lt;你想要存放的路径，比如：models&gt;</p></pre></section><ul><li><section><span>方案二，ModelScope 下载</span></section></li></ul><p>新建 <span>model_download.py</span> 文件，填入以下内容，替换整个 &lt;xxx> <xxx>为你自</xxx><xxx>己的内容,保存后使用 </xxx><span>python model_download.py</span> 执行下载。　</p><section><p><br></p><pre><p><span>from</span> modelscope <span>import</span> snapshot_download<br>model_dir = snapshot_download(<span>'Qwen/Qwen2.5-3B-Instruct'</span>, cache_dir=<span>'&lt;你想要存放的路径，比如：models&gt;'</span>, revision=<span>'master'</span>)</p></pre></section><h2>编写配置文件和训练代码</h2><p>接下来我们需要准备 3 个文件，我们会在 <span>Unlock-DeepSeek</span><span>（https://github.com/datawhalechina/unlock-deepseek）</span> 项目中提供完整的复现文件，方便同学们直接使用。　</p><ul><li><section><span>第一个是 Accelerate 配置文件，用于分布式训练（三张卡）。新建 </span><span>deepspeed_zero3.yaml</span><span> 填入以下内容并保存（不是 DeepSeek，别看错！）。</span></section></li></ul><section><p><br></p><pre><p><span>compute_environment:</span> <span>LOCAL_MACHINE</span><br><span>debug:</span> <span>false</span><br><span>deepspeed_config:</span><br>  <span>deepspeed_multinode_launcher:</span> <span>standard</span><br>  <span>offload_optimizer_device:</span> <span>none</span><br>  <span>offload_param_device:</span> <span>none</span><br>  <span>zero3_init_flag:</span> <span>true</span><br>  <span>zero3_save_16bit_model:</span> <span>true</span><br>  <span>zero_stage:</span> <span>3</span><br><span>distributed_type:</span> <span>DEEPSPEED</span><br><span>downcast_bf16:</span> <span>'no'</span><br><span>machine_rank:</span> <span>0</span><br><span>main_training_function:</span> <span>main</span><br><span>mixed_precision:</span> <span>bf16</span><br><span>num_machines:</span> <span>1</span><br><span>num_processes:</span> <span>8</span> <span># 我们在这里保持常规默认的 8 卡机器，会在后面的启动命令中覆盖新值</span><br><span>rdzv_backend:</span> <span>static</span><br><span>same_network:</span> <span>true</span><br><span>tpu_env:</span> []<br><span>tpu_use_cluster:</span> <span>false</span><br><span>tpu_use_sudo:</span> <span>false</span><br><span>use_cpu:</span> <span>false</span></p></pre></section><p>一般来说，这个文件内容不需要修改，如果有定制需求，请不要使用这个文件，运行 <span>accelerate config</span> 自行设定。　</p><p>在介绍下一个文件之前，我们强烈建议大家使用 <span>Swanlab</span><span>（https://swanlab.cn/）</span> 来可视化追踪实验过程，打开：https://swanlab.cn/login ，登录之后点击图中所示的 Quick Start，或者打开：https://swanlab.cn/space/~/settings ，复制 API Key。　</p><p><img data-imgfileid=100216633 data-ratio=1.0731481481481482 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUSde5Fd9qoBMXpQykhwicfjibJ1PvmrBQWKtu729U10tlTjPNqE33ENHg/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUSde5Fd9qoBMXpQykhwicfjibJ1PvmrBQWKtu729U10tlTjPNqE33ENHg/640?wx_fmt=png&amp;from=appmsg"></p><p><img data-imgfileid=100216631 data-ratio=0.5425925925925926 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyU2Cb5atQYuNOaoV9Dxc1Yv8w3NSKaXEmkzdlFib7AtLaEcvvL1fkXPCA/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyU2Cb5atQYuNOaoV9Dxc1Yv8w3NSKaXEmkzdlFib7AtLaEcvvL1fkXPCA/640?wx_fmt=png&amp;from=appmsg"></p><section>在终端输入<span>swanlab login</span>，直接粘贴（你是看不见东西被粘贴上去的），回车，出现类似如下提示就是登录成功。　</section><section><img data-imgfileid=100216630 data-ratio=0.13333333333333333 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyU8tRuCP9yX4tMbrez1Q7VxYkrfrFl3XBdvJj4f5KJicyFmH7J71YYx0A/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyU8tRuCP9yX4tMbrez1Q7VxYkrfrFl3XBdvJj4f5KJicyFmH7J71YYx0A/640?wx_fmt=png&amp;from=appmsg"></section><ul><li><section><span>第二个是 TRL 配置文件，在这里我们会设定训练的超参数。新建 </span><span>Datawhale-R1.yaml</span><span> 填入以下内容，并根据实际情况修改</span><span>（阅读注释）</span><span>，并保存。</span></section></li></ul><section><section><br></section><pre><section><span># 模型参数</span><br><span>model_name_or_path:</span> <span>&lt;你的模型存放的路径，比如：models/Qwen/Qwen2.5-3B-Instruct&gt;</span><br><span>model_revision:</span> <span>main</span><br><span>torch_dtype:</span> <span>bfloat16</span><br><span>attn_implementation:</span> <span>flash_attention_2</span><br><span>bf16:</span> <span>true</span><br><span>tf32:</span> <span>true</span><br><span>output_dir:</span> <span>&lt;你想要模型输出的路径，比如</span> <span>output/Datawhale-R1&gt;</span><br><br><span># 数据集参数</span><br><span>dataset_id_or_path:</span> <span>&lt;你的数据集存放的路径，比如：dataset&gt;</span><br><br><span># Swanlab 训练流程记录参数</span><br><span>swanlab:</span> <span>true</span> <span># 是否开启 Swanlab </span><br><span>workspace:</span> <span>&lt;用户名&gt;</span><br><span>project:</span> <span>&lt;项目名，整个复现项目的名称，例如：Datawhale-R1-by_xxx&gt;</span><br><span>experiment_name:</span> <span>&lt;实验名，某次超参数运行的自定义名称，例如：qwen2.5-3B-lr:5e-7_beta:0.001&gt;</span><br><br><span># 训练参数</span><br><span>max_steps:</span> <span>450</span> <span># 最大训练步长</span><br><span>per_device_train_batch_size:</span> <span>1</span><br><span>gradient_accumulation_steps:</span> <span>8</span><br><span>gradient_checkpointing:</span> <span>true</span><br><span>gradient_checkpointing_kwargs:</span><br>  <span>use_reentrant:</span> <span>false</span><br><span>learning_rate:</span> <span>5.0e-7</span> <span># 学习率，调整过，参见下文介绍</span><br><span>lr_scheduler_type:</span> <span>cosine</span> <span># 学习率衰减方案</span><br><span>warmup_ratio:</span> <span>0.03</span> <span># 学习率预热比率（对于整个步长），好用！</span><br><span>seed:</span> <span>2025</span> <span># 随机种子，方便实验复现</span><br><br><span># GRPO 算法参数</span><br><span>beta:</span> <span>0.001</span> <span># KL 惩罚因子，调整过，参见下文介绍</span><br><span>max_prompt_length:</span> <span>256</span> <span># 输入 prompt 最大长度，本实验基本不会有太大变化</span><br><span>max_completion_length:</span> <span>4096</span> <span># 输出回答长度，包含推理思维链，设为 4K 比较合适</span><br><span>num_generations:</span> <span>8</span><br><span>use_vllm:</span> <span>true</span> <span># 启用 vllm 来加速推理</span><br><span>vllm_device:</span> <span>&lt;计算卡编号，例如：cuda:2&gt;</span> <span># 留出一张卡来启用 vllm 推理，参见下文介绍</span><br><span>vllm_gpu_memory_utilization:</span> <span>0.5</span><br><br><span># Logging arguments</span><br><span>logging_strategy:</span> <span>steps</span><br><span>logging_steps:</span> <span>1</span><br><span>save_strategy:</span> <span>"steps"</span><br><span>save_steps:</span> <span>50</span> <span># 每隔多少步保存一次</span></section></pre></section><section>我们并没有介绍全部参数，如果需要调整，请查阅 Huggingface 相关文档。当然，直接询问 DeepSeek 可能是更快的方式。　</section><section>这份配置文件中有一些值得大家注意的地方：　</section><ul><li><section><span>learning_rate</span><span> 和 </span><span>beta</span><span> 在 GRPO 的原始论文</span><span>《DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models》</span><span>（https://arxiv.org/abs/2402.03300）</span><span>里分别为 </span><span>1e-6</span><span> 和 </span><span>0.04</span><span>。在这里我们根据</span><span>《Unraveling RLHF and Its Variants: Progress and Practical Engineering Insights》</span><span>（https://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights）</span><span>将其调整为 </span><span>5e-7</span><span> 和 </span><span>0.001</span><span>。</span></section></li><li><section><span>vllm_device</span><span> 本实验需要留出一张卡作为 vllm 的推理卡，假设我们手上有 3 张卡（编号cuda: 0, cuda: 1, cuda: 2)，我们需要指定其中一张卡为 vllm 推理卡，例如我们指定最后一张 </span><span>cuda:2</span><span>。另外，如果你使用了</span><span>CUDA_VISIBLE_DEVICES</span><span> 情况会有些不一样，比如我们有 8 张卡（编号 cuda:0-7），指定编号为 1、2、3 的卡可见（</span><span>CUDA_VISIBLE_DEVICES=1,2,3</span><span>)，这时我们想指定最后一张卡为 vllm 推理卡，则是需要设置为 </span><span>cuda:2</span><span>，因为设置完可见性后，cuda:1 -> cuda:0，cuda:2 -> cuda:1，cuda:3 -> cuda:2，所以原先的 3 号卡变为了新编号的 2 号卡。</span></section></li><li><section><span>save_steps</span><span> 在 </span><span>mini-r1</span><span>（https://www.philschmid.de/mini-deepseek-r1）</span><span> 中是被设为 </span><span>25</span><span>，但是跑完整个训练后，保存的文件大小达到了 700+ GB！因为不仅包含了模型，还包含了其他卡的优化器状态和其他检查点信息，我们在这里改为 </span><span>50</span><span>，但仍然要提醒同学们设置成合适自己的大小（训练代码中已经包含结束后保存模型的代码）。</span></section></li><li><section><span>最后，就是创建训练代码文件 </span><span>train_Datawhale-R1.py</span><span> 并保存，我们几乎给每个关键步骤都添加了注释（建议大家从后往前读），在后文我们会再梳理一遍核心步骤。</span></section></li></ul><section><section><br></section><pre><section><span>import</span> logging<br><span>import</span> os<br><span>import</span> random<br><span>import</span> re<br><span>from</span> dataclasses <span>import</span> dataclass<br><span>from</span> datetime <span>import</span> datetime<br><span>from</span> typing <span>import</span> <span>List</span><br><br><span>from</span> datasets <span>import</span> load_dataset<br><span>from</span> swanlab.integration.transformers <span>import</span> SwanLabCallback<br><span>from</span> transformers <span>import</span> AutoTokenizer<br><span>from</span> transformers.trainer_utils <span>import</span> get_last_checkpoint<br><span>from</span> trl <span>import</span> GRPOConfig, GRPOTrainer, ModelConfig, TrlParser<br><br><span>@dataclass</span><br><span>class</span> <span>DatasetArguments</span>:<br>    <span>"""数据集参数的数据类"""</span><br><br>    <span># 数据集 ID 或路径</span><br>    dataset_id_or_path: <span>str</span> = <span>"Jiayi-Pan/Countdown-Tasks-3to4"</span><br>    <span># 数据集拆分</span><br>    dataset_splits: <span>str</span> = <span>"train"</span><br>    <span># 分词器名称或路径</span><br>    tokenizer_name_or_path: <span>str</span> = <span>None</span><br><br><span>@dataclass</span><br><span>class</span> <span>SwanlabArguments</span>:<br>    <span>"""SwanLab参数的数据类"""</span><br><br>    <span># 是否使用 SwanLab</span><br>    swanlab: <span>bool</span><br>    <span># SwanLab 用户名</span><br>    workspace: <span>str</span><br>    <span># SwanLab 的项目名</span><br>    project: <span>str</span><br>    <span># SwanLab 的实验名</span><br>    experiment_name: <span>str</span><br><br><span># 配置日志记录器</span><br>logging.basicConfig(level=logging.INFO)<br>logger = logging.getLogger(__name__)<br>logger.setLevel(logging.INFO)<br>handler = logging.StreamHandler()<br>handler.setFormatter(<br>    logging.Formatter(<span>"%(asctime)s - %(name)s - %(levelname)s - %(message)s"</span>)<br>)  <span># 设置日志格式</span><br><br>logger.addHandler(handler)<br><br><span>def</span> <span>format_reward_func</span>(completions, **kwargs):<br>    <span>"""<br>    格式奖励函数，检查模型输出格式是否匹配: &lt;think&gt;...&lt;/think&gt;&lt;answer&gt;...&lt;/answer&gt;<br><br>    参数:<br>        completions (list[str]): 生成的输出<br>    返回:<br>        list[float]: 奖励分数<br>    """</span><br>    <span># 初始化奖励列表</span><br>    rewards = []<br>    <span># 遍历生成的输出</span><br>    <span>for</span> completion <span>in</span> completions:<br>        <span>try</span>:<br>            <span># 在生成的输出前添加&lt;think&gt;标签，便于后续正则表达式匹配</span><br>            completion = <span>"&lt;think&gt;"</span> + completion<br><br>            <span>if</span> random.random() &lt; <span>0.1</span>:  <span># 1% 的概率将生成输出写入文件</span><br>                <span># 创建生成输出目录（如果不存在）</span><br>                os.makedirs(<span>"completion_samples"</span>, exist_ok=<span>True</span>)<br>                log_file = os.path.join(<span>"completion_samples"</span>, <span>"completion_samples.txt"</span>)<br>                <span>with</span> <span>open</span>(log_file, <span>"a"</span>) <span>as</span> f:<br>                    f.write(<span>f"\n\n==============\n"</span>)<br>                    f.write(completion)  <span># 写入生成的输出</span><br><br>            <span># 定义正则表达式模式，用于匹配 &lt;think&gt; 和 &lt;answer&gt; 标签</span><br>            regex = <span>r"^&lt;think&gt;([^&lt;]*(?:&lt;(?!/?think&gt;)[^&lt;]*)*)&lt;\/think&gt;\n&lt;answer&gt;([\s\S]*?)&lt;\/answer&gt;$"</span><br>            <span>match</span> = re.search(regex, completion, re.DOTALL)  <span># 使用正则表达式进行匹配</span><br><br>            <span>if</span> <span>match</span> <span>is</span> <span>None</span> <span>or</span> <span>len</span>(<span>match</span>.groups()) != <span>2</span>:<br>                rewards.append(<span>0.0</span>)  <span># 如果格式不正确，奖励为 0</span><br>            <span>else</span>:<br>                rewards.append(<span>1.0</span>)  <span># 如果格式正确，奖励为 1</span><br>        <span>except</span> Exception:<br>            rewards.append(<span>0.0</span>)  <span># 如果发生异常，奖励为 0</span><br><br>    <span>return</span> rewards<br><br><span>def</span> <span>equation_reward_func</span>(completions, target, nums, **kwargs):<br>    <span>"""<br>    方程奖励函数，检查计算结果是否正确，数字是否符合使用要求（每个数字只用一次，只使用所提供的数字）<br><br>    参数:<br>        completions (list[str]): 生成的输出<br>        target (list[str]): 预期的答案<br>        nums (list[str]): 可用的数字<br><br>    返回:<br>        list[float]: 奖励分数<br>    """</span><br>    <span># 初始化奖励列表</span><br>    rewards = []<br>    <span># 遍历生成的输出、预期的答案和可用的数字</span><br>    <span>for</span> completion, gt, numbers <span>in</span> <span>zip</span>(completions, target, nums):<br>        <span>try</span>:<br>            <span># 在生成的输出前添加 &lt;think&gt; 标签，便于后续正则表达式匹配</span><br>            completion = <span>"&lt;think&gt;"</span> + completion<br>            <span># 定义正则表达式模式，用于匹配 &lt;answer&gt; 标签</span><br>            <span>match</span> = re.search(<span>r"&lt;answer&gt;(.*?)&lt;\/answer&gt;"</span>, completion)<br>            <span>if</span> <span>match</span> <span>is</span> <span>None</span>:<br>                rewards.append(<span>0.0</span>)  <span># 如果没有匹配到 &lt;answer&gt; 标签，奖励为 0</span><br>                <span>continue</span><br>            equation = <span>match</span>.group(<span>1</span>).strip()  <span># 提取 &lt;answer&gt; 标签中的内容</span><br>            <span># 提取方程中的所有数字</span><br>            used_numbers = [<span>int</span>(n) <span>for</span> n <span>in</span> re.findall(<span>r"\d+"</span>, equation)]<br><br>            <span># 检查所有数字是否被使用且只使用一次</span><br>            <span>if</span> <span>sorted</span>(used_numbers) != <span>sorted</span>(numbers):<br>                rewards.append(<span>0.0</span>)<br>                <span>continue</span><br><br>            <span># 定义允许的字符模式，只允许数字、运算符、括号和空白字符</span><br>            allowed_pattern = <span>r"^[\d+\-*/().\s]+$"</span><br>            <span>if</span> <span>not</span> re.<span>match</span>(allowed_pattern, equation):<br>                rewards.append(<span>0.0</span>)  <span># 如果方程包含不允许的字符，奖励为 0</span><br>                <span>continue</span><br><br>            <span># 计算方程的结果</span><br>            result = <span>eval</span>(equation, {<span>"__builtins__"</span>: <span>None</span>}, {})<br>            <span># 检查方程是否正确且与预期答案匹配（误差小于 1e-5）</span><br>            <span>if</span> <span>abs</span>(<span>float</span>(result) - <span>float</span>(gt)) &lt; <span>1e-5</span>:<br>                rewards.append(<span>1.0</span>)  <span># 如果正确，奖励为 1</span><br><br>                <span># 10% 的概率将成功的样本写入文件</span><br>                <span>if</span> random.random() &lt; <span>0.10</span>:<br>                    <span># 创建生成输出目录（如果不存在）</span><br>                    os.makedirs(<span>"completion_samples"</span>, exist_ok=<span>True</span>)<br>                    log_file = os.path.join(<br>                        <span>"completion_samples"</span>, <span>"success_completion_samples.txt"</span><br>                    )<br>                    <span>with</span> <span>open</span>(log_file, <span>"a"</span>) <span>as</span> f:<br>                        f.write(<span>f"\n\n==============\n"</span>)<br>                        f.write(completion)  <span># 写入生成的输出</span><br>            <span>else</span>:<br>                rewards.append(<span>0.0</span>)  <span># 如果不正确，奖励为 0</span><br>        <span>except</span> Exception:<br>            rewards.append(<span>0.0</span>)  <span># 如果评估失败，奖励为 0</span><br><br>    <span>return</span> rewards<br><br><span>def</span> <span>thought_len_reward_func</span>(completions, **kwargs):<br>    <span>"""<br>    思考长度奖励函数，检查 &lt;think&gt; 标签的长度是否大于 1000<br><br>    参数:<br>        completions (list[str]): 生成的输出<br>    返回:<br>        list[float]: 奖励分数<br>    """</span><br>    <span># 初始化奖励列表</span><br>    rewards = []<br>    <span># 遍历生成的输出</span><br>    <span>for</span> completion <span>in</span> completions:<br>        <span>try</span>:<br>            <span># 在生成的输出前添加 &lt;think&gt; 标签，便于后续正则表达式匹配</span><br>            completion = <span>"&lt;think&gt;"</span> + completion<br>            <span># 定义正则表达式模式，用于匹配 &lt;think&gt; 标签</span><br>            <span>match</span> = re.search(<span>r"&lt;think&gt;(.*?)&lt;/think&gt;"</span>, completion)<br>            <span># 如果匹配到 &lt;think&gt; 标签</span><br>            <span>if</span> <span>match</span>:<br>                thought_process = <span>match</span>.group(<span>1</span>).strip()  <span># 提取 &lt;think&gt; 标签中的内容</span><br>                thought_length = <span>len</span>(thought_process)  <span># 计算思考过程的长度</span><br>                <span>if</span> thought_length &gt; <span>1000</span>:<br>                    rewards.append(<span>1.0</span>)  <span># 如果思考过程长度大于 1000，奖励为 1</span><br>                <span>else</span>:<br>                    rewards.append(<span>0.0</span>)  <span># 否则奖励为 0</span><br>            <span>else</span>:<br>                rewards.append(<span>0.0</span>)  <span># 如果没有匹配到 &lt;think&gt; 标签，奖励为 0</span><br>                <span>continue</span><br>        <span>except</span> Exception:<br>            rewards.append(<span>0.0</span>)  <span># 如果发生异常，奖励为 0</span><br><br>    <span>return</span> rewards<br><br><span>def</span> <span>get_checkpoint</span>(training_args: GRPOConfig):<br>    <span>"""<br>    获取最后一个检查点<br><br>    参数:<br>        training_args (GRPOConfig): 训练参数<br>    返回:<br>        str: 最后一个检查点的路径，如果没有检查点，则返回 None<br>    """</span><br>    last_checkpoint = <span>None</span><br>    <span>if</span> os.path.isdir(training_args.output_dir):  <span># 如果输出目录存在</span><br>        <span># 获取最后一个检查点</span><br>        last_checkpoint = get_last_checkpoint(training_args.output_dir)<br>    <span>return</span> last_checkpoint<br><br><span># 定义 GRPO 训练函数</span><br><span>def</span> <span>grpo_function</span>(<br>    model_args: ModelConfig,<br>    dataset_args: DatasetArguments,<br>    training_args: GRPOConfig,<br>    callbacks: <span>List</span>,<br>):<br>    <span># 记录模型参数</span><br>    logger.info(<span>f"Model parameters <span>{model_args}</span>"</span>)<br>    <span># 记录训练/评估参数</span><br>    logger.info(<span>f"Training/evaluation parameters <span>{training_args}</span>"</span>)<br><br>    <span># 加载分词器</span><br>    tokenizer = AutoTokenizer.from_pretrained(<br>        (<br>            <span># 如果有指定分词器，则使用指定的分词器，否则使用模型名称</span><br>            dataset_args.tokenizer_name_or_path<br>            <span>if</span> dataset_args.tokenizer_name_or_path<br>            <span>else</span> model_args.model_name_or_path<br>        ),<br>        revision=model_args.model_revision,  <span># 使用指定的模型版本</span><br>        trust_remote_code=model_args.trust_remote_code,  <span># 允许使用远程代码</span><br>    )<br>    <span># 如果分词器没有填充标记，则使用结束标记作为填充标记</span><br>    <span>if</span> tokenizer.pad_token <span>is</span> <span>None</span>:<br>        tokenizer.pad_token = tokenizer.eos_token<br><br>    <span># 加载数据集</span><br>    dataset = load_dataset(<br>        dataset_args.dataset_id_or_path, split=dataset_args.dataset_splits<br>    )<br>    <span># 随机选择 50K 个样本，看你喜好定数字，但是数据集有 409K 个样本</span><br>    dataset = dataset.shuffle(seed=training_args.seed).select(<span>range</span>(<span>50000</span>))<br><br>    <span>def</span> <span>generate_r1_prompt</span>(numbers, target):<br>        <span>"""<br>        生成 R1 Countdown 游戏提示词<br><br>        参数:<br>            numbers (list[int]): 数字列表<br>            target (int): 目标值<br>        返回:<br>            dict: 生成的一个数据样本<br>        """</span><br>        <span># 定义提示词前缀</span><br>        r1_prefix = [<br>            {<br>                <span>"role"</span>: <span>"user"</span>,<br>                <span>"content"</span>: <span>f"使用给定的数字 <span>{numbers}</span>，创建一个等于 <span>{target}</span> 的方程。你可以使用基本算术运算（+、-、*、/）一次或多次，但每个数字只能使用一次。在 &lt;think&gt; &lt;/think&gt; 标签中展示你的思考过程，并在 &lt;answer&gt; &lt;/answer&gt; 标签中返回最终方程，例如 &lt;answer&gt; (1 + 2) / 3 &lt;/answer&gt;。在 &lt;think&gt; 标签中逐步思考。"</span>,<br>            },<br>            {<br>                <span>"role"</span>: <span>"assistant"</span>,<br>                <span>"content"</span>: <span>"让我们逐步解决这个问题。\n&lt;think&gt;"</span>,  <span># 结尾使用 `&lt;think&gt;` 促使模型开始思考</span><br>            },<br>        ]<br><br>        <span>return</span> {<br>            <span>"prompt"</span>: tokenizer.apply_chat_template(<br>                r1_prefix, tokenize=<span>False</span>, continue_final_message=<span>True</span><br>            ),  <span># 提示词，continue_final_message=True 表示将提示词中的最后一个消息继续到最终的输出中</span><br>            <span>"target"</span>: target,<br>            <span>"nums"</span>: numbers,<br>        }<br><br>    <span># 将数据集转换为 R1 Countdown 游戏提示词</span><br>    dataset = dataset.<span>map</span>(<span>lambda</span> x: generate_r1_prompt(x[<span>"nums"</span>], x[<span>"target"</span>]))<br>    <span># 将数据集拆分为训练集和测试集，拆分比例为 9:1</span><br>    train_test_split = dataset.train_test_split(test_size=<span>0.1</span>)<br>    train_dataset = train_test_split[<span>"train"</span>]  <span># 获取训练集</span><br>    test_dataset = train_test_split[<span>"test"</span>]  <span># 获取测试集</span><br><br>    <span># 设置 GRPOTrainer</span><br>    trainer = GRPOTrainer(<br>        model=model_args.model_name_or_path,  <span># 模型名称或路径</span><br>        <span># 奖励函数列表，用于计算奖励分数</span><br>        reward_funcs=[<br>            format_reward_func,  <span># 格式奖励函数</span><br>            equation_reward_func,  <span># 方程奖励函数</span><br>            thought_len_reward_func,  <span># 思考长度奖励函数</span><br>        ],<br>        args=training_args,<br>        train_dataset=train_dataset,<br>        eval_dataset=test_dataset,<br>        callbacks=callbacks,<br>    )<br><br>    last_checkpoint = get_checkpoint(training_args)  <span># 检查最后一个检查点</span><br>    <span># 如果检测到检查点且指定从检查点恢复训练，则记录信息</span><br>    <span>if</span> last_checkpoint <span>is</span> <span>not</span> <span>None</span> <span>and</span> training_args.resume_from_checkpoint <span>is</span> <span>None</span>:<br>        logger.info(<span>f"Checkpoint detected, resuming training at <span>{last_checkpoint}</span>."</span>)<br><br>    logger.info(<br>        <span>f'*** Starting training <span>{datetime.now().strftime(<span>"%Y-%m-%d %H:%M:%S"</span>)}</span> for <span>{training_args.num_train_epochs}</span> epochs***'</span><br>    )<br><br>    <span># 训练模型</span><br>    train_result = trainer.train(resume_from_checkpoint=last_checkpoint)<br><br>    <span># 记录和保存指标</span><br>    metrics = train_result.metrics<br>    metrics[<span>"train_samples"</span>] = <span>len</span>(train_dataset)<br>    trainer.log_metrics(<span>"train"</span>, metrics)<br>    trainer.save_metrics(<span>"train"</span>, metrics)<br>    trainer.save_state()<br><br>    logger.info(<span>"*** Training complete ***"</span>)<br><br>    <span># 保存模型和分词器</span><br>    logger.info(<span>"*** Save model ***"</span>)<br>    trainer.model.config.use_cache = <span>True</span><br>    trainer.save_model(training_args.output_dir)<br>    logger.info(<span>f"Model saved to <span>{training_args.output_dir}</span>"</span>)<br>    training_args.distributed_state.wait_for_everyone()  <span># 等待所有进程加载</span><br>    tokenizer.save_pretrained(training_args.output_dir)<br>    logger.info(<span>f"Tokenizer saved to <span>{training_args.output_dir}</span>"</span>)<br><br>    logger.info(<span>"*** Training complete! ***"</span>)<br><br><span>def</span> <span>main</span>():<br>    <span>"""主函数，用于执行主训练循环"""</span><br>    <span># 解析命令行参数和配置文件</span><br>    parser = TrlParser((ModelConfig, DatasetArguments, GRPOConfig, SwanlabArguments))<br>    model_args, dataset_args, training_args, swanlab_args = (<br>        parser.parse_args_and_config()<br>    )<br><br>    <span># 如果使用 SwanLab，则创建 SwanLab 回调对象，用于训练信息记录</span><br>    <span>if</span> swanlab_args.swanlab:<br>        swanlab_callback = SwanLabCallback(<br>            workspace=swanlab_args.workspace,<br>            project=swanlab_args.project,<br>            experiment_name=swanlab_args.experiment_name,<br>        )<br>        callbacks = [swanlab_callback]<br>    <span>else</span>:<br>        callbacks = <span>None</span><br><br>    <span># 运行主训练循环</span><br>    grpo_function(model_args, dataset_args, training_args, callbacks=callbacks)<br><br><span>if</span> __name__ == <span>"__main__"</span>:<br>    main()<br></section></pre></section><h1>启动训练</h1><section>肯定有一些同学已经等不及要开始跑模型训练了，那启动训练的命令很简单，在终端运行下面的内容（<span>根据自己需求修改</span>），也可以把它保存为 <span>train_Datawhale-R1.sh</span> 然后在终端运行 <span>bash train_Datawhale-R1.sh</span>。　</section><section><section><br></section><pre><section><span># 如果你要限制计算卡编号，请在这里设置，例如只使用 cuda:1-3，如果不用限制，就删除下面这行</span><br><span>export</span> CUDA_VISIBLE_DEVICES=1,2,3<br><br>accelerate launch \<br>    --num_processes 2 \<br>    --config_file deepspeed_zero3.yaml \<br>    train_Datawhale-R1.py \<br>    --config Datawhale-R1.yaml</section></pre></section><section>注意：<span>--num_processes</span> 是由你希望使用的计算卡数量决定，我们之前在配置文件那里说过，要留一张卡作为 vllm 的推理卡，那么 <span>--num_processes</span> 的数值应该是你要使用的计算卡数量 <span>n-1</span>，例如我有 3 张卡，我的 <span>--num_processes</span> 应该为 <span>2</span>。这里的 <span>--num_processes</span> 的数值也会把 <span>deepspeed_zero3.yaml</span> 的<span>num_processes</span> 设置的 <span>8</span> 给覆盖掉。　</section><section>另外，同样像上文所说，如果你有定制的硬件配置需求，请不要使用 <span>--config_file</span> 参数。　</section><section>出现这样的提示就说明模型已经训练起来啦！可以在 Swanlab 看炫酷的训练数据了（手机也能看，特别适合天选炼丹人）。　</section><section><img data-imgfileid=100216638 data-ratio=0.2564814814814815 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUdnIupEnf5xXEjsE8ZTKib7ib1bTghUR49LiaBIAibt7viaEsjRrpWCYy8fw/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUdnIupEnf5xXEjsE8ZTKib7ib1bTghUR49LiaBIAibt7viaEsjRrpWCYy8fw/640?wx_fmt=png&amp;from=appmsg"></section><section><img data-imgfileid=100216639 data-ratio=0.55 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUpH036RITNuo9Ou5qCia6MQK8owokzibic01ATvVEhLRbkVJzAeGZX2WCA/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUpH036RITNuo9Ou5qCia6MQK8owokzibic01ATvVEhLRbkVJzAeGZX2WCA/640?wx_fmt=png&amp;from=appmsg"></section><h1>训练流程详解</h1><h2>流程总览</h2><section>我们来梳理一遍 Datawhale-R1 训练流程：　</section><ol><li><section><span>将提示词输入到 Qwen 2.5 模型。</span></section></li><li><section><span>Qwen 2.5 输出多个带思考的回答（本实验设置为 8，由 </span><span>num_generations</span><span> 参数决定）。</span></section></li><li><section><span>模型的回答分别传入三个奖励函数计算，计算的结果相加。</span></section></li><li><section><span>将奖励值传入 GRPO 策略中，GRPO 根据奖励值来决定如何调整 Qwen 2.5 模型。</span></section></li><li><section><span>重复上述流程（本实验重复了 450 次，由 </span><span>max_steps</span><span> 参数决定）。</span></section></li></ol><p><img data-galleryid data-imgfileid=100216659 data-ratio=0.5916666666666667 data-s=300,640 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUYqHKZgiamzIWibWrPG8jnbM6f1Xu3yqrD1SRFYzcoWaOXZVgiaSgLsA4w/640?wx_fmt=png" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUYqHKZgiamzIWibWrPG8jnbM6f1Xu3yqrD1SRFYzcoWaOXZVgiaSgLsA4w/640?wx_fmt=png"></p><section>有些同学可能不太熟悉强化学习，我们会在后续其他的文章中介绍强化学习相关的概念。在这里我们用一个例子来比喻一下：我们现在假设有一所学校，里面有一个数学老师（GRPO 策略），还有一个班级（Qwen 2.5 模型，我们假设班级中的所有同学能力相同），学校每个月要月考（多步），每次月考是班级根据试卷（提示词，一份试卷只有一道题）写出多份答卷（班级有多个同学，所以会有多份答卷，对应多个带思考的模型回答，这些回答不一定是相同的），这时候数学老师就要去批改这些答卷（奖励函数计算），评卷规则是：　</section><ol><li><section><span>检查答题格式是否规范（格式奖励函数）</span></section></li><li><section><span>解题结果是否正确（方程奖励函数）</span></section></li><li><section><span>解题步骤是否详细（思考长度奖励函数）</span></section></li></ol><section>最后，把每部分的分数相加，得到多个试卷分数（多个奖励值，用 Python 列表表示，每个回答都对应一个奖励值），数学老师根据班级的月考分数来判断下一步如何调整教学计划（调整模型），来让这个班级在下一次月考中尽可能得到更高的分数。　</section><section>如果我们说得更细致一点，其实是数学老师会教整个班级“看到什么之后写什么”，比如看到题目就要写“解：”，看到“x+1=2”就要写“解得：x=1”，力求让组成回答的每一个字都是最合适的（位置要合适，用词也要合适）从而去获得最高的分数。　</section><section>这里的思考长度奖励函数是我们新加入的，用于鼓励模型进行更长的思考。所以我们应该有个朴素的感受，随着训练的不断进行，Datawhale-R1 的输出格式应该会越来越规范，正确率也会不断提高，思考的长度也会增加。　</section><h2>核心代码介绍</h2><section>我们稍微介绍一下代码中每个核心步骤的输入输出样例，让大家心里有个底。首先是各种 <span>xxx_args</span> 参数，它其实就是根据下面这行代码，去获取我们传入的 <span>Datawhale-R1.yaml</span> 里面的参数。　</section><section><section><br></section><pre><section>parser = TrlParser((ModelConfig, DatasetArguments, GRPOConfig, SwanlabArguments))<br>model_args, dataset_args, training_args, swanlab_args = (<br>    parser.parse_args_and_config()<br>)</section></pre></section><section>你可以看到我们定义了一个 <span>SwanlabArguments</span> 类，<span>TrlParser</span> 会去寻找 <span>Datawhale-R1.yaml</span> 中跟 <span>SwanlabArguments</span> 有关的参数，并把它赋值给 <span>swanlab_args</span>，由于每个参数名被要求是唯一的，不能重复，所以 <span>TrlParser</span> 能把不同的参数正确赋值给对应变量（根据 <span>ModelConfig, DatasetArguments, GRPOConfig, SwanlabArguments</span> 的顺序，赋值给 <span>model_args, dataset_args, training_args, swanlab_args</span>）　</section><section><section><br></section><pre><section><span># train_Datawhale-R1.py</span><br><br><span>@dataclass</span><br><span>class</span> <span>SwanlabArguments</span>:<br>    <span>"""SwanLab参数的数据类"""</span><br><br>    <span># 是否使用 SwanLab</span><br>    swanlab: <span>bool</span><br>    <span># SwanLab 用户名</span><br>    workspace: <span>str</span><br>    <span># SwanLab 的项目名</span><br>    project: <span>str</span><br>    <span># SwanLab 的实验名</span><br>    experiment_name: <span>str</span></section></pre></section><section><section><br></section><pre><section><span># Datawhale-R1.yaml</span><br><br><span># Swanlab 训练流程记录参数</span><br><span>swanlab:</span> <span>true</span> <span># 是否开启 Swanlab </span><br><span>workspace:</span> <span>&lt;用户名&gt;</span><br><span>project:</span> <span>&lt;项目名，整个复现项目的名称，例如：Datawhale-R1-by_xxx&gt;</span><br><span>experiment_name:</span> <span>&lt;实验名，某次超参数运行的自定义名称，例如：qwen2.5-3B-lr:5e-7_beta:0.001&gt;</span></section></pre></section><section>接下来就到了 <span>grpo_function</span> 里，我们首先来看看我们的数据集长什么样子，我们的任务其实很简单，它很像 24 点游戏，给定若干个数字 <span>nums</span>，例如 <span>[44, 19, 35]</span> ，模型要用四则运算，告诉我们一个方程，它的计算结果正好是 <span>target</span>，例如 <span>98</span>，详细要求我们在 prompt 中给大家展示。　</section><section><img data-imgfileid=100216635 data-ratio=1.1489637305699483 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUibTJYA3DGlnpK1Cx3cwa9icoOz0D8elQzX7ibe209jQL3tu7Wahicdsdjg/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=772 src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUibTJYA3DGlnpK1Cx3cwa9icoOz0D8elQzX7ibe209jQL3tu7Wahicdsdjg/640?wx_fmt=png&amp;from=appmsg"></section><section>然后我们的 prompt 如下，利用 Python 的 f-strings 功能来填入具体数值，并且在 <span>assistant</span> 的结尾加入了 <span>\n&lt;think></span>，来促使我们的模型开始按要求逐步思考。提示词是用 DeepSeek 翻译的 mini-r1 的提示词，咱们中国人阅读中文的速度更快些。　</section><section><section><br></section><pre><section>r1_prefix = [<br>    {<br>        <span>"role"</span>: <span>"user"</span>,<br>        <span>"content"</span>: <span>f"使用给定的数字 <span>{numbers}</span>，创建一个等于 <span>{target}</span> 的方程。你可以使用基本算术运算（+、-、*、/）一次或多次，但每个数字只能使用一次。在 &lt;think&gt; &lt;/think&gt; 标签中展示你的思考过程，并在 &lt;answer&gt; &lt;/answer&gt; 标签中返回最终方程，例如 &lt;answer&gt; (1 + 2) / 3 &lt;/answer&gt;。在 &lt;think&gt; 标签中逐步思考。"</span>,<br>    },<br>    {<br>        <span>"role"</span>: <span>"assistant"</span>,<br>        <span>"content"</span>: <span>"让我们逐步解决这个问题。\n&lt;think&gt;"</span>,  <span># 结尾使用 `&lt;think&gt;` 促使模型开始思考</span><br>    },<br>]</section></pre></section><section>在这里我们会把 prompt 转换为 Qwen 2.5 的提示词模版，让它以更熟悉的方式来接收提示词，并且我们把 <span>让我们逐步解决这个问题。\n<span>&lt;think></span><think></think></span> 作为模型输出的开头，让它接着续写。用 Python 字典的方式返回样本，这样 TRL 会在调用奖励函数的时候，帮我们把键名设为为对应的参数；另外，TRL 会把模型的多个输出设为 <span>completions</span>。　</section><section><section><br></section><pre><section><span>return</span> {<br>    <span>"prompt"</span>: tokenizer.apply_chat_template(<br>        r1_prefix, tokenize=<span>False</span>, continue_final_message=<span>True</span><br>    ),  <span># 提示词，continue_final_message=True 表示将提示词中的最后一个消息继续到最终的输出中</span><br>    <span>"target"</span>: target,<br>    <span>"nums"</span>: numbers,<br>}</section></pre></section><section><span>map</span> 方法会帮我们把实际的 <span>nums</span> 和 <span>target</span> 填入到 prompt 里，我们根据上面举的例子，来看一个具体的提示词：　</section><section><section><br></section><pre><section><span># 将数据集转换为 R1 Countdown 游戏提示词</span><br>dataset = dataset.<span>map</span>(<span>lambda</span> x: generate_r1_prompt(x[<span>"nums"</span>], x[<span>"target"</span>]))<br><br><span># 举例</span><br>nums = [<span>44</span>, <span>19</span>, <span>35</span>]<br>target = <span>98</span><br>r1_prefix = {<br>    <span>"role"</span>: <span>"user"</span>,<br>    <span>"content"</span>: <span>f"使用给定的数字 [44, 19, 35]，创建一个等于 98 的方程。你可以使用基本算术运算（+、-、*、/）一次或多次，但每个数字只能使用一次。在 &lt;think&gt; &lt;/think&gt; 标签中展示你的思考过程，并在 &lt;answer&gt; &lt;/answer&gt; 标签中返回最终方程，例如 &lt;answer&gt; (1 + 2) / 3 &lt;/answer&gt;。在 &lt;think&gt; 标签中逐步思考。"</span>,<br>},<br>{<br>    <span>"role"</span>: <span>"assistant"</span>,<br>    <span>"content"</span>: <span>"让我们逐步解决这个问题。\n&lt;think&gt;"</span>,  <span># 结尾使用 `&lt;think&gt;` 促使模型开始思考</span><br>},<br><br><span># 转换为 Qwen 提示词模版后</span><br>prompt = <span>"&lt;|im_start|&gt;system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n使用给定的数字 [44, 19, 35]，创建一个等于 98 的方程。你可以使用基本算术运算（+、-、*、/）一次或多次，但每个数字只能使用一次。在 &lt;think&gt; &lt;/think&gt; 标签中展示你的思考过程，并在 &lt;answer&gt; &lt;/answer&gt; 标签中返回最终方程，例如 &lt;answer&gt; (1 + 2) / 3 &lt;/answer&gt;。在 &lt;think&gt; 标签中逐步思考。&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n让我们逐步解决这个问题。\n&lt;think&gt;"</span> <span># 模型将在 \n&lt;think&gt; 后续写</span></section></pre></section><section>我们最后来看一个奖励函数的例子，TRL 将多个模型输出变成一个列表，叫做 <span>completions</span>，并将数据集中的其他内容根据键名传入到对应参数。所以我们需要使用 for 循环遍历所有的 <span>completions</span>，并对每个输出进行判断打分，最后返回每个输出的得分列表 <span>reward</span> 给 <span>GRPO 策略</span>（例如：[0.0, 1.0, 0.0]），让其判断下一步如何调整。</section><section><section><br></section><pre><section><span>def</span> <span>equation_reward_func</span>(completions, target, nums, **kwargs):<br>    <span>"""<br>    参数:<br>        completions (list[str]): 生成的输出<br>        target (list[str]): 预期的答案<br>        nums (list[str]): 可用的数字<br><br>    返回:<br>        list[float]: 奖励分数<br>    """</span><br>    <span># 初始化奖励列表</span><br>    rewards = []<br>    <span># 遍历生成的输出、预期的答案和可用的数字</span><br>    <span>for</span> completion, gt, numbers <span>in</span> <span>zip</span>(completions, target, nums):<br>        ... <span># 进行一些 rewards.append() 操作</span><br>    <span>return</span> rewards</section></pre></section><section>OK，我们对复现流程的介绍就大致结束了，我们会在文末提供完整的文档，开源我们的复现工作。　</section><h1>训练结果解读</h1><section>现在我们来看看模型表现出了什么有意思的现象，提前声明，这不是严谨的科学研究，会有很多分析漏洞。首先我们使用了学习率预热和学习率衰减，在训练前期学习率都很大，后期慢慢衰减下来。　</section><section><img data-imgfileid=100216637 data-ratio=0.48703703703703705 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUoA4RPxXzd73gPDkEHpfTZ0GJD17ePHhtTVaycg0iahwxjyysUlaljjw/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUoA4RPxXzd73gPDkEHpfTZ0GJD17ePHhtTVaycg0iahwxjyysUlaljjw/640?wx_fmt=png&amp;from=appmsg"></section><section>对比下面两张图，我们发现模型前期学习输出格式的速度很快，大概 20 到 30 步就能学得很好。但是后来由于我们的思考长度奖励函数，模型的输出长度被拉长，发生严重的重复现象，导致超出 4096 的输出被截断，格式不完整，格式奖励函数的奖励值就大幅下降，后面模型又开始缩短输出，稳定在 300 到 400，又恢复到正确格式。　</section><section><img data-imgfileid=100216636 data-ratio=0.47962962962962963 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUlxo8X4G1bIGeNcMf8kBbNMN5enXXKl7R0zbWZvB9ppTh89WEg1V35Q/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUlxo8X4G1bIGeNcMf8kBbNMN5enXXKl7R0zbWZvB9ppTh89WEg1V35Q/640?wx_fmt=png&amp;from=appmsg"></section><section><img data-imgfileid=100216640 data-ratio=0.4962962962962963 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUeq20bITdU0Hq0FAPiaK01J86zdFABHklf6XqDGz63cDibPqscWzWUkQA/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUeq20bITdU0Hq0FAPiaK01J86zdFABHklf6XqDGz63cDibPqscWzWUkQA/640?wx_fmt=png&amp;from=appmsg"></section><section>模型的不断重复输出看着其实挺可怕的，Visual Studio Code 会匹配相同字符并高亮，大家可以看看，右侧红框的缩略图几乎都是重复的回答。　</section><section><img data-imgfileid=100216644 data-ratio=0.6333333333333333 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUdunVjvKeFib0rDtKwyicefdmFBtrymRybrNZkbiaTDCMI6VxZmANcVfwA/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUdunVjvKeFib0rDtKwyicefdmFBtrymRybrNZkbiaTDCMI6VxZmANcVfwA/640?wx_fmt=png&amp;from=appmsg"></section><section>我们发现，模型被鼓励拉长输出的时候，计算正确率也在提升，所以我们有个不严谨的判断，似乎拉长模型输出，能带一定的计算正确率的提升。观察下图可以发现，在 120 步时，模型的输出在越变越长，平均输出长度已经被拉到 400 左右，越来越多的输出已经超过 1000，方程计算正确率也在逐步升高，但是这时已经发生一些重复问题导致格式错误。　</section><section><img data-imgfileid=100216641 data-ratio=0.46111111111111114 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyU5KKe8jzUpRc9rGI5kMd6G0WZl8ib09V7HIFdbicguYorUOrgGEJAHkFA/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyU5KKe8jzUpRc9rGI5kMd6G0WZl8ib09V7HIFdbicguYorUOrgGEJAHkFA/640?wx_fmt=png&amp;from=appmsg"></section><section>其实从上图我们也可以看到 GRPO 已经意识到重复问题带来的奖励值下降，它在 200 步左右开始逐步限制模型输出长度，而这时模型的计算正确率也保持在 0.3 到 0.4 左右。　</section><section>我们还发现，在训练初期，你会看到比较明显的方程奖励提升，而输出长度不断减小。模型似乎有一种趋向于缩短思考长度的趋势，所以我们引入思考长度奖励函数来对抗这种趋势，我们把它解释为模型计算能力提升之后，就像学霸一眼秒杀题目一样，模型不想输出更多“废话”来解释解题过程。　</section><section><img data-imgfileid=100216642 data-ratio=0.5898148148148148 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUuyuicOnwc2D1Uog7ibpoKLRGPddAMHAJrVhebftOHCGIt7UQqm1dqm1Q/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUuyuicOnwc2D1Uog7ibpoKLRGPddAMHAJrVhebftOHCGIt7UQqm1dqm1Q/640?wx_fmt=png&amp;from=appmsg"></section><section>在训练开始 1 分钟左右，我们就观察到下面的输出，还以为我们重现了 Aha Moment。后来证明其实不是，Qwen 2.5 很喜欢反复试错、验算，反复试错很容易导致上文提及的重复输出问题。　</section><section><img data-imgfileid=100216643 data-ratio=0.45092592592592595 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUsHgicdtgDzBTSJJqoE1Yktyich2OPfRcvzBFxZRWBtCjXOYbKiclDsxkA/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUsHgicdtgDzBTSJJqoE1Yktyich2OPfRcvzBFxZRWBtCjXOYbKiclDsxkA/640?wx_fmt=png&amp;from=appmsg"></section><section><img data-imgfileid=100216645 data-ratio=0.5120370370370371 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUqibeHjVXRDI7srfRXzCUIgV9Xiba5AMEf7J3mQSwbWmoribRypyYShJibw/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUqibeHjVXRDI7srfRXzCUIgV9Xiba5AMEf7J3mQSwbWmoribRypyYShJibw/640?wx_fmt=png&amp;from=appmsg"></section><section><img data-imgfileid=100216648 data-ratio=0.24814814814814815 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUjNBG3ibFHqLueBzeO675G8nQvYTicAw5JzXIKHuL02JlZYDPdbo791AA/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUjNBG3ibFHqLueBzeO675G8nQvYTicAw5JzXIKHuL02JlZYDPdbo791AA/640?wx_fmt=png&amp;from=appmsg"></section><section><img data-imgfileid=100216649 data-ratio=0.5064814814814815 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyURlIBk0UCqyXbficH3KeOacYCs0q7OosxfN9Wjo3cwZcz36r08xwJOGg/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyURlIBk0UCqyXbficH3KeOacYCs0q7OosxfN9Wjo3cwZcz36r08xwJOGg/640?wx_fmt=png&amp;from=appmsg"></section><section>我们发现了另一种语言混用现象，哈哈哈。current n. 电流; adj. 当前的。　</section><section><img data-imgfileid=100216647 data-ratio=0.3398148148148148 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyU71psKk71LrDQLq5pd3cexxTfylWu4YLHJE2jZuOqfUZyaxsDIKRIcg/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyU71psKk71LrDQLq5pd3cexxTfylWu4YLHJE2jZuOqfUZyaxsDIKRIcg/640?wx_fmt=png&amp;from=appmsg"></section><section>所以结论就是，我们没有复现 Aha Moment。其实在观察大量 Qwen 2.5 的输出之后，一种直觉告诉我，可能 Aha Moment 跟模型本身的输出风格相关，网友都说 DeepSeek 文风很锐利、很活泼，但是 Qwen 2.5 给人的感觉总是冷静、平和。简单做了一个不严谨的测试，可能能够佐证这个想法，我要求两个模型用 Aha Moment 的语气跟我说话，再随便回复了一个字，观察两个模型本身对 Aha Moment 的映射是会输出什么。我们另外测试了 Llama 和 MiniCPM，它们的输出风格都跟 Qwen 很接近，试图像说教一样给你做比喻，所以我大胆判断，可能写武侠小说的大模型更容易观察到 Aha Moment。　</section><section><img data-imgfileid=100216646 data-ratio=0.6064814814814815 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyU81px1wf85iaiaGCrwM2l5dE3p4YibIvZiccINDgauEj1oSCiap7DK3Zj3mg/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyU81px1wf85iaiaGCrwM2l5dE3p4YibIvZiccINDgauEj1oSCiap7DK3Zj3mg/640?wx_fmt=png&amp;from=appmsg"></section><section><img data-imgfileid=100216651 data-ratio=0.6333333333333333 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUgNghK5tZUfwSkdOTY6WiaG6tGUia8ib1yQoaqWLO4I7ibOSMYaBC59ROww/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUgNghK5tZUfwSkdOTY6WiaG6tGUia8ib1yQoaqWLO4I7ibOSMYaBC59ROww/640?wx_fmt=png&amp;from=appmsg"></section><section>我们会一同公布模型输出的采样文本文件，大家也可以在里面找到一些我们还没有发现的新奇玩意，欢迎向 Unlock-DeepSeek 团队报告你的发现。　</section><h1>展望</h1><section>在本文写作前一天，我们发现<span>另一组团队也公开了他们的 Qwen 2.5 7B 的 R1 Zero 复现结果</span><span>（https://zhuanlan.zhihu.com/p/21290410831）</span>，他们也观察到了很多有趣的结果，虽然他们的曲线非常震荡，但是也稍微能看出一点佐证我们观点的证据：似乎拉长模型输出，能带一定的计算正确率的提升。他们的工作非常棒！我们就不用去验证 7B 模型的性能了，非常环保，节能减排。大家也可以追踪观察社区其他小组的复现报告，相信开源社区的力量！　</section><section>最后嘱咐一些要点，　</section><ul><li><section><span>Math 模型不太好用，它有固有的数学输出会影响格式奖励，可能需要更长的步长才能纠正，不环保，训了一会我就停了。</span></section></li></ul><section><img data-imgfileid=100216650 data-ratio=0.5935185185185186 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUWMSRz5pPTIMbwV0QEkJXooqIBBkFplcYHg9YuGGYrGzMJbSddNRRTQ/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUWMSRz5pPTIMbwV0QEkJXooqIBBkFplcYHg9YuGGYrGzMJbSddNRRTQ/640?wx_fmt=png&amp;from=appmsg"></section><section><img data-imgfileid=100216653 data-ratio=0.6037037037037037 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUeqssLiaicnGicEuHuUAbWEuDuic6wMMwtGhCicI9piaPAibyWcxAv4iafqyX3Q/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUeqssLiaicnGicEuHuUAbWEuDuic6wMMwtGhCicI9piaPAibyWcxAv4iafqyX3Q/640?wx_fmt=png&amp;from=appmsg"></section><ul><li><section><span>小于 3B 的模型真不好用，没什么必要再试验了，DeepSeek 官方蒸馏的 1.5B 的推理也很烂，小模型承受了太多它不该承受的东西。我们甚至还在 0.5B 的模型看到了俄语，但是找不到图了。</span></section></li></ul><section><img data-imgfileid=100216652 data-ratio=0.600925925925926 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUWibzDTonHwGgoLD9tR4aF7ZJ2IwENSzUomRdyd7UkUySvIVgEAGSgFQ/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsFEcCZBoqjoz9iaNDk83XPyUWibzDTonHwGgoLD9tR4aF7ZJ2IwENSzUomRdyd7UkUySvIVgEAGSgFQ/640?wx_fmt=png&amp;from=appmsg"></section><ul><li><section><span>这种训练方式用来规范模型输出格式特别好用。</span></section></li><li><section><span>Jian Hu 报告 GRPO 有严重震荡问题</span><span>（https://zhuanlan.zhihu.com/p/14888098807）</span><span>，或许大家可以试试其他算法。</span></section></li><li><section><span>如果你的资源充足，可以试试更大的模型，希望在开源社区能够见到大家的新发现。</span></section></li><li><section><span>TRL 目前的 LoRA 模块有严重 Bug，请不要使用。</span></section></li><li><section><span>最后一点，要复现，请用 TinyZero，省钱！</span></section></li></ul><h1>完整文件获取</h1><section>Unlock-DeepSeek 团队后续会陆续发布更多关于 DeepSeek 相关工作解读的文章（马上就会发布 GRPO 解读文章），敬请关注，我们下次再见！　</section><section><section>Unlock-DeepSeek 项目主页：https://datawhalechina.github.io/unlock-deepseek/　</section><section>Github 仓库：https://github.com/datawhalechina/unlock-deepseek　</section><section>Gitee 国内仓库：https://gitee.com/anine09/unlock-deepseek　</section><section>Swanlab 实验数据：https://swanlab.cn/@anine09/datawhale-r1/overview　</section><section>模型会在晚些时候上传 HuggingFace 和 ModelScope，并在项目中公布，虽然模型本身没什么用。　</section><section>复现文件在 Datawhale-R1 文件夹。　</section><section>Unlock-DeepSeek 项目目前并不完善，并且正在快速迭代，请持续关注。　</section></section><section><img data-backh=234 data-backw=578 data-galleryid data-imgfileid=100216654 data-ratio=0.40555555555555556 data-s=300,640 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsEoyqBuoSeZBYGia4FrNqibThuOLnV2mc5w2np56PD2KQyAOMdyh50NHEqKXDBIVfm4rXeUCnXtU1mg/640?wx_fmt=other&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1&amp;tp=webp" data-type=png data-w=900 src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsEoyqBuoSeZBYGia4FrNqibThuOLnV2mc5w2np56PD2KQyAOMdyh50NHEqKXDBIVfm4rXeUCnXtU1mg/640?wx_fmt=other&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1&amp;tp=webp"></section><section><strong><span><span>一起“</span><span><strong><span>点</span></strong></span><span><strong><span>赞<span>”</span></span></strong></span><strong><span>三连</span></strong><span>↓</span></span></strong></section><p><mp-style-type data-value=10000></mp-style-type></p></div><hr><a href=https://mp.weixin.qq.com/s/Z7P61IV3n4XYeC0Et_fvwg ,target=_blank rel="noopener noreferrer">原文链接</a></div><footer class=post__footer><div class="post__tags tags clearfix"><svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M4 0h8s2 0 4 2l15 15s2 2 0 4L21 31s-2 2-4 0L2 16s-2-2-2-4V3s0-3 4-3m3 10a3 3 0 000-6 3 3 0 000 6"/></svg><ul class=tags__list><li class=tags__item><a class="tags__link btn" href=../../../tags/fetched/ rel=tag>fetched</a></li><li class=tags__item><a class="tags__link btn" href=../../../tags/datawhale/ rel=tag>Datawhale</a></li></ul></div></footer></article></main><div class="authorbox clearfix"><div class=authorbox__header><span class=authorbox__name>About Bloger</span></div></div><nav class="pager flex"><div class="pager__item pager__item--prev"><a class=pager__link href=../../../posts/2025-02/_%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E6%8E%A8%E5%87%BAomnihuman_%E4%BB%8E%E5%8D%95%E5%BC%A0%E7%85%A7%E7%89%87%E7%94%9F%E6%88%90%E9%80%BC%E7%9C%9F%E5%85%A8%E8%BA%AB%E5%8A%A8%E6%80%81%E8%A7%86%E9%A2%91/ rel=prev><span class=pager__subtitle>«&#8201;Previous</span><p class=pager__title>​字节跳动推出OmniHuman 从单张照片生成逼真全身动态视频</p></a></div><div class="pager__item pager__item--next"><a class=pager__link href=../../../posts/2025-02/%E6%88%90%E4%B8%BA%E7%A5%A8%E6%88%BF%E5%BD%B1%E5%8F%B2%E7%AC%AC%E4%B8%80%E7%9A%84%E5%BF%85%E8%A6%81%E6%9D%A1%E4%BB%B6/ rel=next><span class=pager__subtitle>Next&#8201;»</span><p class=pager__title>成为票房影史第一的必要条件</p></a></div></nav><section class=comments><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//kkitown.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section></div><aside class=sidebar><div class="widget-search widget"><form class=widget-search__form role=search method=get action=https://google.com/search><input class=widget-search__field type=search placeholder=Search… name=q aria-label=Search…>
<input class=widget-search__submit type=submit value=Search>
<input type=hidden name=sitesearch value=https://ixxmu.github.io/></form></div><div class="widget-recent widget"><h4 class=widget__title>Recent Posts</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=../../../posts/2025-10/%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BA%BA%E6%80%BB%E5%96%9C%E6%AC%A2%E5%90%83%E8%84%86%E8%84%86%E7%9A%84%E4%B8%9C%E8%A5%BF_/>为什么人总喜欢吃脆脆的东西？</a></li><li class=widget__item><a class=widget__link href=../../../posts/2025-10/tether%E5%AE%83%E7%9A%84%E5%B9%95%E5%90%8E%E6%8E%8C%E8%88%B5%E4%BA%BA/>Tether它的幕后掌舵人</a></li><li class=widget__item><a class=widget__link href=../../../posts/2025-10/_%E7%BB%8F%E6%B5%8E%E5%AD%A6%E4%BA%BA_%E4%B8%A8%E6%AF%94%E7%89%B9%E5%B8%81%E4%B8%8E%E4%BC%A6%E6%95%A6%E7%9A%84%E4%B8%AD%E5%9B%BD%E8%AF%88%E9%AA%97%E7%8A%AF/>《经济学人》丨比特币与伦敦的中国诈骗犯</a></li><li class=widget__item><a class=widget__link href=../../../posts/2025-10/%E4%B8%80%E4%B8%AA%E5%8D%B0%E5%BA%A6%E4%BA%BA%E6%98%AF%E5%A6%82%E4%BD%95%E6%8A%8A%E5%85%A8%E6%9D%91%E8%80%81%E4%B9%A1%E6%90%AC%E5%8E%BB%E7%BE%8E%E5%9B%BD%E7%9A%84_/>一个印度人是如何把全村老乡搬去美国的？</a></li><li class=widget__item><a class=widget__link href=../../../posts/2025-10/%E5%A4%A7%E6%B2%B3%E5%8D%97_%E4%B8%8B%E6%B2%89%E5%B8%82%E5%9C%BA___%E6%89%98%E8%B5%B7%E5%8D%83%E4%BA%BF_%E8%9C%9C%E9%9B%AA%E5%86%B0%E5%9F%8E_/>大河南“下沉市场” ，托起千亿“蜜雪冰城”</a></li></ul></div></div><div class="widget-categories widget"><h4 class=widget__title>Categories</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=../../../categories/duty/>Duty</a></li><li class=widget__item><a class=widget__link href=../../../categories/duty2/>Duty2</a></li></ul></div></div><div class="widget-taglist widget"><h4 class=widget__title>Tags</h4><div class=widget__content><a class="widget-taglist__link widget__link btn" href=../../../tags/cnbeta/ title=Cnbeta>Cnbeta (149)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/datawhale/ title=Datawhale>Datawhale (9)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/drpei/ title=Drpei>Drpei (6)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/fetched/ title=Fetched>Fetched (766)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/githubdaily/ title=GitHubDaily>GitHubDaily (7)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E4%B8%81%E9%A6%99%E5%9B%AD/ title=丁香园>丁香园 (30)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E4%BA%BA%E7%89%A9/ title=人物>人物 (16)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E5%8D%97%E6%96%B9%E5%91%A8%E6%9C%AB/ title=南方周末>南方周末 (11)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E5%B0%8F%E4%BC%97%E8%BD%AF%E4%BB%B6/ title=小众软件>小众软件 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E5%B0%8F%E5%A4%A7%E5%A4%AB%E6%BC%AB%E7%94%BB/ title=小大夫漫画>小大夫漫画 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E5%B0%91%E6%95%B0%E6%B4%BE/ title=少数派>少数派 (6)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E5%BE%AA%E5%9B%A0%E7%BC%89%E8%8D%AF/ title=循因缉药>循因缉药 (9)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%96%B0%E4%B9%A1%E5%9C%9F/ title=新乡土>新乡土 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%98%9F%E7%90%83%E5%95%86%E4%B8%9A%E8%AF%84%E8%AE%BA/ title=星球商业评论>星球商业评论 (7)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%9C%88%E5%85%89%E5%8D%9A%E5%AE%A2/ title=月光博客>月光博客 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%9C%AA%E9%97%BBcode/ title=未闻Code>未闻Code (6)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%A7%BD%E8%BE%B9%E5%BE%80%E4%BA%8B/ title=槽边往事>槽边往事 (9)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%AF%8F%E6%97%A5%E4%BA%BA%E7%89%A9/ title=每日人物>每日人物 (10)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%B5%AA%E6%BD%AE%E5%B7%A5%E4%BD%9C%E5%AE%A4/ title=浪潮工作室>浪潮工作室 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%B5%AE%E4%B9%8B%E9%9D%99/ title=浮之静>浮之静 (6)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%81%BC%E8%A7%81/ title=灼见>灼见 (8)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%86%8A%E8%A8%80%E7%86%8A%E8%AF%AD/ title=熊言熊语>熊言熊语 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%94%9F%E4%BF%A1%E6%8A%80%E8%83%BD%E6%A0%91/ title=生信技能树>生信技能树 (10)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%94%9F%E4%BF%A1%E8%8F%9C%E9%B8%9F%E5%9B%A2/ title=生信菜鸟团>生信菜鸟团 (6)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%9F%A5%E8%AF%86%E5%88%86%E5%AD%90/ title=知识分子>知识分子 (14)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%BD%91%E6%98%93%E6%95%B0%E8%AF%BB/ title=网易数读>网易数读 (7)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E8%B4%A2%E6%96%B0/ title=财新>财新 (6)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E8%B4%A2%E7%BB%8F%E4%B8%89%E5%88%86%E9%92%9F/ title=财经三分钟>财经三分钟 (7)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E8%B5%9B%E5%85%88%E7%94%9F/ title=赛先生>赛先生 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E9%83%91%E5%B7%9E%E6%A5%BC%E5%B8%82/ title=郑州楼市>郑州楼市 (32)</a></div><a href=../../../tags/></a></div><div class="widget-social widget"><h4 class="widget-social__title widget__title">Social</h4><div class="widget-social__content widget__content"><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=Facebook rel="noopener noreferrer" href=https://facebook.com/username target=_blank><svg class="widget-social__link-icon icon icon-facebook" width="24" height="24" viewBox="0 0 352 352"><path d="m0 32v288c0 17.5 14.5 32 32 32h288c17.5.0 32-14.5 32-32V32c0-17.5-14.5-32-32-32H32C14.5.0.0 14.5.0 32zm320 0v288h-83V212h41.5l6-48H237v-31c0-14 3.5-23.5 23.5-23.5h26V66c-4.4-.6-19.8-1.5-37.5-1.5-36.9.0-62 22.2-62 63.5v36h-42v48h42v108H32V32z"/></svg>
<span>Facebook</span></a></div><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=Twitter rel="noopener noreferrer" href=https://twitter.com/username target=_blank><svg class="widget-social__link-icon icon icon-twitter" width="24" height="24" viewBox="0 0 384 312"><path d="m384 36.9c-14.1 6.3-29.3 10.5-45.2 12.4 16.3-9.7 28.8-25.2 34.6-43.6-15.2 9-32.1 15.6-50 19.1-14.4-15.2-34.9-24.8-57.5-24.8-43.5.0-78.8 35.3-78.8 78.8.0 6.2.7 12.2 2 17.9-65.5-3.3-123.5-34.6-162.4-82.3C20 26 16.1 39.6 16.1 54c0 27.3 13.9 51.4 35 65.6-12.9-.4-25.1-4-35.7-9.9v1c0 38.2 27.2 70 63.2 77.2-6.6 1.8-13.6 2.8-20.8 2.8-5.1.0-10-.5-14.8-1.4 10 31.3 39.1 54.1 73.6 54.7-27 21.1-60.9 33.7-97.8 33.7-6.4.0-12.6-.4-18.8-1.1C34.9 299 76.3 312 120.8 312c144.9.0 224.1-120 224.1-224.1.0-3.4-.1-6.8-.2-10.2 15.4-11.1 28.7-25 39.3-40.8z"/></svg>
<span>Twitter</span></a></div><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=Instagram rel="noopener noreferrer" href=https://www.instagram.com/username target=_blank><svg class="widget-social__link-icon icon icon-instagram" width="24" height="24" viewBox="0 0 256 256"><circle cx="193" cy="59" r="15"/><path fill-rule="evenodd" d="M101 0h54c41 0 58.4 3.9 74.5 17C256.2 37.5 256 74.8 256 97.7v60c0 26.7.0 60.4-26.5 81.4-16 13.4-33.5 16.9-74.5 16.9h-54c-41 0-57.5-3.5-74.5-16.9C1 218.9.5 186.3.1 160.5L0 155V97.7c0-23-.2-60.2 26.5-80.7C45 2 60 0 101 0zm4.9 23h44.3c45.8.0 58.3 3.5 70.3 17.5 11.8 13.2 12 30.1 12.5 62.9V156c.2 20.8.3 45.8-12.5 59.5-12 14-24.5 17.5-70.3 17.5h-44.3c-45.9.0-57.3-3.5-70.4-17.5-12.2-13-12.3-36.5-12.4-56.7v-55.6c.4-32.6.7-49.6 12.4-62.7C48 26.5 60 23 105.9 23zm19.6 144.5a42 42 0 100-84 42 42 0 000 84zm0 22.5a64.5 64.5.0 100-129 64.5 64.5.0 000 129z"/></svg>
<span>Instagram</span></a></div></div></div></aside></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2025 文字轨迹.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=../../../js/menu.js></script><script src=../../../js/custom.js></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script></body></html>