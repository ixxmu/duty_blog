<!doctype html><html class=no-js lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>完整的671B R1塞进本地，详尽教程来了！ - 文字轨迹</title>
<script>(function(e,t){e[t]=e[t].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:url" content="https://ixxmu.github.io/posts/2025-02/%E5%AE%8C%E6%95%B4%E7%9A%84671b_r1%E5%A1%9E%E8%BF%9B%E6%9C%AC%E5%9C%B0_%E8%AF%A6%E5%B0%BD%E6%95%99%E7%A8%8B%E6%9D%A5%E4%BA%86_/"><meta property="og:site_name" content="文字轨迹"><meta property="og:title" content="完整的671B R1塞进本地，详尽教程来了！"><meta property="og:description" content="完整的671B R1塞进本地，详尽教程来了！ by Datawhale Datawhale干货 作者：李锡涵，编译：机器之心"><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-03T14:56:11+00:00"><meta property="article:modified_time" content="2025-02-03T14:56:11+00:00"><meta property="article:tag" content="Fetched"><meta property="article:tag" content="Datawhale"><meta itemprop=name content="完整的671B R1塞进本地，详尽教程来了！"><meta itemprop=description content="完整的671B R1塞进本地，详尽教程来了！ by Datawhale Datawhale干货 作者：李锡涵，编译：机器之心"><meta itemprop=datePublished content="2025-02-03T14:56:11+00:00"><meta itemprop=dateModified content="2025-02-03T14:56:11+00:00"><meta itemprop=wordCount content="713"><meta itemprop=keywords content="Fetched,Datawhale"><meta name=twitter:card content="summary"><meta name=twitter:title content="完整的671B R1塞进本地，详尽教程来了！"><meta name=twitter:description content="完整的671B R1塞进本地，详尽教程来了！ by Datawhale Datawhale干货 作者：李锡涵，编译：机器之心"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=../../../css/style.css><link rel=stylesheet href=../../../css/custom.css><link rel="shortcut icon" href=../../../favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=../../../ title=文字轨迹 rel=home><div class="logo__item logo__text"><div class=logo__title>文字轨迹</div><div class=logo__tagline>故事流淌过的地方</div></div></a></div><nav class=menu><button class=menu__btn aria-haspopup=true aria-expanded=false tabindex=0>
<span class=menu__btn-title tabindex=-1>Menu</span></button><ul class=menu__list><li class=menu__item><a class=menu__link href=../../../posts/><span class=menu__text>文章</span></a></li><li class=menu__item><a class=menu__link href=../../../tags/><span class=menu__text>标签</span></a></li><li class=menu__item><a class=menu__link href=../../../about/><span class=menu__text>关于</span></a></li></ul></nav></div></header><div class="wrapper flex"><div class=primary><main class=main role=main><article class=post><header class=post__header><meta name=referrer content="never"><h1 class=post__title>完整的671B R1塞进本地，详尽教程来了！</h1><div class="post__meta meta"><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0a14 14 0 110 28 1 1 0 010-28m0 3a3 3 0 100 22 3 3 0 000-22m1 4h-2v8.4l6.8 4.4L22 18l-6-3.8z"/></svg><time class=meta__text datetime=2025-02-03T14:56:11Z>2025-02-03</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=../../../categories/duty/ rel=category>Duty</a></span></div><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 16 16"><path d="M8 1c2 0 3.5 2 3.5 4.5S10 9 10 9c3 1 4 2 4 6H2c0-4 1-5 4-6 0 0-1.5-1-1.5-3.5S6 1 8 1"/></svg><span class=meta__text>Bloger</span></div></div></header><div class="post__toc toc"><div class=toc__title>Page content</div><div class=toc__menu><nav id=TableOfContents><ul><li><a href=#完整的671b-r1塞进本地详尽教程来了-by-datawhale>完整的671B R1塞进本地，详尽教程来了！ by Datawhale</a></li></ul></nav></div></div><div class="content post__content clearfix"><h2 id=完整的671b-r1塞进本地详尽教程来了-by-datawhale>完整的671B R1塞进本地，详尽教程来了！ by Datawhale</h2><div><section data-mpa-powered-by=yiban.io data-style="-webkit-tap-highlight-color: transparent; margin-bottom: 0px; outline: 0px; text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px; background-color: rgb(255, 255, 255); letter-spacing: 0.544px; text-wrap: wrap; caret-color: rgb(34, 34, 34); font-family: 'Helvetica Neue', Helvetica, 'Hiragino Sans GB', 'Microsoft YaHei', Arial, sans-serif; visibility: visible; line-height: 27.2px; color: rgb(163, 163, 163) !important;" mp-original-font-size=17 mp-original-line-height=27.200000762939453><h1 data-tool=mdnice编辑器><span><section data-style="-webkit-tap-highlight-color: transparent; margin-bottom: 0px; outline: 0px; text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px; background-color: rgb(255, 255, 255); letter-spacing: 0.544px; text-wrap: wrap; caret-color: rgb(34, 34, 34); font-family: 'Helvetica Neue', Helvetica, 'Hiragino Sans GB', 'Microsoft YaHei', Arial, sans-serif; visibility: visible; line-height: 27.2px; color: rgb(163, 163, 163) !important;" mp-original-font-size=17 mp-original-line-height=27.200000762939453><section data-style="-webkit-tap-highlight-color: transparent; margin-bottom: 0px; outline: 0px; text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px; background-color: rgb(255, 255, 255); letter-spacing: 0.544px; text-wrap: wrap; caret-color: rgb(34, 34, 34); font-family: 'Helvetica Neue', Helvetica, 'Hiragino Sans GB', 'Microsoft YaHei', Arial, sans-serif; visibility: visible; line-height: 27.2px; color: rgb(163, 163, 163) !important;" mp-original-font-size=17 mp-original-line-height=27.200000762939453><section><section powered-by=xiumi.us><section><section data-id=85660 data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)"><section><p><span> Datawhale干货 </span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);"><p><span><span><strong>作者：李锡涵，编译</strong></span></span><span><strong>：机器之心</strong></span></p></section></section></section></section><section><mp-common-profile data-id="MzIyNjM2MzQyNg==" data-pluginname=mpprofile data-headimg="http://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsEXsBwQkpYLtE2vhn7Z3RVOSRu5M1VicIgqgMRKLsxsibK7OUSqUb1rUO4pfXnQyFYKqhryAIeh4MOg/300?wx_fmt=png&amp;wxfrom=19" data-nickname=Datawhale data-alias=Datawhale data-signature="一个专注于AI领域的开源组织，汇聚了众多优秀学习者，使命-for the learner，和学习者一起成长。" data-from=2 data-weuitheme=light data-origin_num=702 data-isban=0 data-biz_account_status=0 data-index=0 data-is_biz_ban=0></mp-common-profile><span></span></section></section></section></section></section></span></h1></section><p><span><span>本文作者：李锡涵（Xihan Li）</span></span></p><p><span><span>作者简介：伦敦大学学院（UCL）计算机系博士研究生，谷歌开发者专家，主要研究方向为学习优化，在 NeurIPS、ICLR、AAMAS、CIKM 等会议发表过学术论文，Circuit Transformer 作者，图书《简明的 TensorFlow 2》（https://tf.wiki）作者</span></span></p><section><strong><span><br></span></strong></section><section><strong></strong></section><section><span>过年这几天，<span>DeepSeek 算是彻底破圈了，火遍大江南北，火到人尽皆知。虽然网络版和 APP 版已经足够好用，但<span>把模型部署到本地，才能真正实现独家定制，让 <span>DeepSeek <span>R1 的深度思考「以你为主，为你所用」。</span></span></span></span></span></section><section><span><br></span></section><section><span>关于本地化部署，我们发布了两份教程《<a target=_blank href="https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&amp;mid=2247700014&amp;idx=1&amp;sn=7926c4c9af5ebba7b8d138c8832b32a1&amp;scene=21#wechat_redirect" textvalue="DeepSeek R1本地部署，小白教程来了！" linktype=text imgurl imgdata=null data-itemshowtype=0 tab=innerlink data-linktype=2>DeepSeek R1本地部署，小白教程来了！</a>》、《<a target=_blank href="https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&amp;mid=2247700063&amp;idx=1&amp;sn=3aeb9031728ab8d4ba2f23bb5fe921fd&amp;scene=21#wechat_redirect" textvalue=手把手带你用DeepSeek-R1和Ollama搭建本地应用，一文搞定！ linktype=text imgurl imgdata=null data-itemshowtype=0 tab=innerlink data-linktype=2>手把手带你用DeepSeek-R1和Ollama搭建本地应用，一文搞定！</a>》</span></section><section><span><br></span></section><section><span>然而，完整的 671B MoE 模型也可以通过针对性的量化技术压缩体积，从而大幅降低本地部署门槛，乃至在消费级硬件（如单台 Mac Studio）上运行。</span></section><section><span><br></span></section><section><span>那么，如何用 ollama 在本地部署 DeepSeek R1 671B（完整未蒸馏版本）模型呢？一篇在海外热度很高的简明教程即将揭晓。</span></section><section><br></section><section><img data-galleryid data-imgfileid=100216443 data-ratio=0.6712962962962963 data-s=300,640 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWib4IbH5m6iaPibsIYnraicia3TB7XzqZmzgSMUJibqTzJYs3fmPXCoNLiasgxeKz8nHlgMFt1jQLvQiamnAQ/640?wx_fmt=other&amp;from=appmsg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWib4IbH5m6iaPibsIYnraicia3TB7XzqZmzgSMUJibqTzJYs3fmPXCoNLiasgxeKz8nHlgMFt1jQLvQiamnAQ/640?wx_fmt=other&amp;from=appmsg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1"></section><section><br></section><ul><li><p><span>作者主页：https://snowkylin.github.io</span></p></li><li><p><span>原文地址：https://snowkylin.github.io/blogs/a-note-on-deepseek-r1.html</span></p></li></ul><section><br></section><section><section><iframe data-w=1080 data-ratio=0.976491862567812 data-cover=http%3A%2F%2Fmmbiz.qpic.cn%2Fsz_mmbiz_jpg%2FKmXPKA19gWib4IbH5m6iaPibsIYnraicia3TBM9QyhSicCibXciaib7uO7O1oysnBINgRPcFJrGpTktSDWVgyje1UyswngA%2F0%3Fwx_fmt%3Djpeg data-mpvid=wxv_3838683620433920000 data-vidtype=2 allowfullscreen frameborder=0 data-src="https://mp.weixin.qq.com/mp/readtemplate?t=pages/video_player_tmpl&amp;action=mpvideo&amp;auto=0&amp;vid=wxv_3838683620433920000"></iframe></section></section><section><span><em><span>本地部署后，让 DeepSeek R1 「数草莓」</span></em></span></section><section><br></section><section><strong><span>模型选择</span></strong></section><section><span><br></span></section><section><span>原版 DeepSeek R1 671B 全量模型的文件体积高达 720GB，对于绝大部分人而言，这都大得太离谱了。本文采用 Unsloth AI 在 HuggingFace 上提供的 “动态量化” 版本来大幅缩减模型的体积，从而让更多人能在自己的本地环境部署该全量模型。</span></section><section><span><br></span></section><section><span>“动态量化” 的核心思路是：对模型的少数关键层进行高质量的 4-6bit 量化，而对大部分相对没那么关键的混合专家层（MoE）进行大刀阔斧的 1-2bit 量化。通过这种方法，DeepSeek R1 全量模型可压缩至最小 131GB（1.58-bit 量化），极大降低了本地部署门槛，甚至能在单台 Mac Studio 上运行！</span></section><section><br></section><section><span>根据我自己的工作站配置，我选择了以下两个模型进行测试：</span></section><section><br></section><ul><li><section><span>DeepSeek-R1-UD-IQ1_M（671B，1.73-bit 动态量化，158 GB，HuggingFace）</span></section></li><li><section><span>DeepSeek-R1-Q4_K_M（671B，4-bit 标准量化，404 GB，HuggingFace）</span></section></li></ul><section><span><br></span></section><section><span>Unsloth AI 提供了 <span><strong>4 种动态量化模型</strong></span>（1.58 至 2.51 比特，文件体积为 131GB 至 212GB），可根据自身硬件条件灵活选择。建议阅读官方说明了解各版本差异。</span></section><section><br></section><ul><li><section><span><span>Unsloth AI </span>官方说明：https://unsloth.ai/blog/deepseekr1-dynamic</span></section></li></ul><section><span><br></span></section><section><strong><span>硬件需求</span></strong></section><section><span><br></span></section><section><span>部署此类大模型的主要瓶颈是内存+显存容量，建议配置如下：</span></section><section><br></section><ul><li><section><span>DeepSeek-R1-UD-IQ1_M：内存 + 显存 ≥ 200 GB</span></section></li><li><section><span>DeepSeek-R1-Q4_K_M：内存 + 显存 ≥ 500 GB</span></section></li></ul><section><strong><span><br></span></strong></section><section><span>我们使用 ollama 部署此模型。ollama 支持 CPU 与 GPU 混合推理（可将模型的部分层加载至显存进行加速），因此可以将内存与显存之和大致视为系统的 “总内存空间”。</span></section><section><br></section><section><span>除了模型参数占用的内存+显存空间（158 GB 和 404GB）以外，实际运行时还需额外预留一些内存（显存）空间用于上下文缓存。预留的空间越大，支持的上下文窗口也越大。</span></section><section><span><br></span></section><section><span>我的测试环境为：</span></section><section><br></section><ul><li><section><span>四路 RTX 4090（4×24 GB 显存）</span></section></li><li><section><span>四通道 DDR5 5600 内存（4×96 GB 内存）</span></section></li><li><section><span>ThreadRipper 7980X CPU（64 核）</span></section></li></ul><section><span><br></span></section><section><span>在此配置下，短文本生成（约 500 个 token）的速度为：</span></section><section><br></section><ul><li><section><span>DeepSeek-R1-UD-IQ1_M：7-8 token / 秒（纯 CPU 推理时为 4-5 token / 秒）</span></section></li><li><section><span>DeepSeek-R1-Q4_K_M：2-4 token / 秒</span></section></li></ul><section><span><br></span></section><section><span>长文本生成时速度会降至 1-2 token / 秒。</span></section><section><br></section><section><span>值得注意的是，上述测试环境的硬件配置对于大模型推理而言，并非性价比最优的方案（这台工作站主要用于我的 Circuit Transformer 研究（arXiv:2403.13838），该研究在上周于 ICLR 会议接收。我和我的工作站都可以休息一下了，于是有了这篇文章）。</span></section><section><br></section><section><span>下面列举一些更具性价比的选项：</span></section><section><br></section><ul><li><section><span>Mac Studio：配备大容量高带宽的统一内存（比如 X 上的 @awnihannun 使用了两台 192 GB 内存的 Mac Studio 运行 3-bit 量化的版本）</span></section></li><li><section><span>高内存带宽的服务器：比如 HuggingFace 上的 alain401 使用了配备了 24×16 GB DDR5 4800 内存的服务器）</span></section></li><li><section><span>云 GPU 服务器：配备 2 张或更多的 80GB 显存 GPU（如英伟达的 H100，租赁价格约 2 美元 / 小时 / 卡）</span></section></li></ul><section><span><br></span></section><section><span>若硬件条件有限，可尝试体积更小的 1.58-bit 量化版（131GB），可运行于：</span></section><section><br></section><ul><li><section><span>单台 Mac Studio（192GB 统一内存，参考案例可见 X 上的 @ggerganov，成本约 5600 美元）</span></section></li><li><section><span>2×Nvidia H100 80GB（参考案例可见 X 上的 @hokazuya，成本约 4~5 美元 / 小时）</span></section></li></ul><section><span><br></span></section><section><span>且在这些硬件上的运行速度可达到 10+ token / 秒。</span></section><section><span><br></span></section><section><span><strong>部署步骤</strong></span></section><section><span><strong><br></strong></span></section><section><span>下列步骤在Linux环境下执行，Mac OS和Windows的部署方式原则上类似，主要区别是ollama和llama.cpp的安装版本和默认模型目录位置不同。</span></section><section><span><strong><br></strong></span></section><section><strong><span>1. 下载模型文件</span></strong></section><section><br></section><section><span>从 HuggingFace （https://huggingface.co/unsloth/DeepSeek-R1-GGUF）下载模型的 .gguf 文件（文件体积很大，建议使用下载工具，比如我用的是 XDM），并将下载的分片文件合并成一个（见注释 1）。</span></section><section><span><br></span></section><section><strong><span>2. 安装 ollama</span></strong></section><section><br></section><ul><li><section><span>下载地址：https://ollama.com/<br></span></section></li></ul><section><span><br></span></section><section><span>执行以下命令：</span></section><section><span><br></span></section><section><ul><li></ul><pre data-lang=nginx><section><code><span>curl -fsSL https://ollama.com/install.sh | sh</span></code></section></pre></section><section><br></section><section><strong><span>3. 创建 Modelfile 文件，该文件用于指导 ollama 建立模型</span></strong></section><section><span><br></span></section><section><span>使用你喜欢的编辑器（比如nano或vim），为你选择的模型建立模型描述文件。</span></section><section><span><br></span></section><section><span>文件 DeepSeekQ1_Modelfile（对应于 DeepSeek-R1-UD-IQ1_M）的内容如下：</span></section><section><br></section><section><ul><li><li><li><li><li></ul><pre data-lang=properties><code><span><span>FROM</span> <span>/home/snowkylin/DeepSeek-R1-UD-IQ1_M.gguf  </span></span></code><code><span><span>PARAMETER</span> <span>num_gpu 28  </span></span></code><code><span><span>PARAMETER</span> <span>num_ctx 2048  </span></span></code><code><span><span>PARAMETER</span> <span>temperature 0.6  </span></span></code><code><span><span>TEMPLATE</span> <span>"&lt;｜User｜&gt;{{ .Prompt }}&lt;｜Assistant｜&gt;"</span></span></code></pre></section><section><br></section><section><span>文件 DeepSeekQ4_Modelfile（对应于 DeepSeek-R1-Q4_K_M）的内容如下：</span></section><section><br></section><section><ul><li><li><li><li><li></ul><pre data-lang=properties><code><span><span>FROM</span> <span>/home/snowkylin/DeepSeek-R1-Q4_K_M.gguf</span></span></code><code><span><span>PARAMETER</span> <span>num_gpu 8  </span></span></code><code><span><span>PARAMETER</span> <span>num_ctx 2048  </span></span></code><code><span><span>PARAMETER</span> <span>temperature 0.6  </span></span></code><code><span><span>TEMPLATE</span> <span>"&lt;｜User｜&gt;{{ .Prompt }}&lt;｜Assistant｜&gt;"</span></span></code></pre></section><section><span><br></span></section><section><span>你需要将第一行“FROM”后面的文件路径，改为你在第1步下载并合并的.gguf文件的实际路径。</span></section><section><span><br></span></section><section><span>可根据自身硬件情况调整 num_gpu（GPU 加载层数）和 num_ctx（上下文窗口大小），详情见步骤 6。</span></section><section><span><br></span></section><section><strong><span>4. 创建 ollama 模型</span></strong></section><section><strong><span><br></span></strong></section><section><span>在第3步建立的模型描述文件所处目录下，执行以下命令：<br></span></section><section><br></section><section><ul><li></ul><pre data-lang=nginx><code><span>ollama create DeepSeek-R1-UD-IQ1_M -f DeepSeekQ1_Modelfile</span></code></pre></section><section><br></section><section><span>务必确保 ollama 的模型目录 /usr/share/ollama/.ollama/models 有足够大的空间（或修改模型目录的路径，见注释 2）。这个命令会在模型目录建立若干模型文件，体积与下载的.gguf 文件体积相当。</span></section><section><span><br></span></section><section><strong><span>5. 运行模型</span></strong></section><section><strong><span><br></span></strong></section><section><span>执行以下命令：</span></section><section><span><br></span></section><section><ul><li></ul><pre data-lang=nginx><code><span>ollama run DeepSeek-R1-UD-IQ1_M --verbose</span></code></pre></section><section><br></section><ul><li><section><span><strong><span>--verbose 参数</span></strong></span><span>用于显示推理速度（token / 秒）。</span></section></li></ul><section><span><br></span></section><section><span>若提示内存不足或CUDA错误，需返回步骤 4 调整参数后，重新创建和运行模型。</span></section><section><br></section><ul><li><section><span><strong><span>num_gpu</span></strong></span><span>：加载至 GPU 的模型层数。DeepSeek R1 模型共有 61 层，我的经验是：</span></section><section><span>对于 DeepSeek-R1-UD-IQ1_M，每块 RTX 4090（24GB 显存）可加载 7 层，四卡共 28 层（接近总层数的一半）。</span></section><section><span>对于 DeepSeek-R1-Q4_K_M，每卡仅可加载 2 层，四卡共 8 层。</span></section><section><span><br></span></section></li><li><section><span><strong><span>num_ctx</span></strong></span><span>：上下文窗口的大小（默认值为 2048），建议从较小值开始逐步增加，直至触发内存不足的错误。</span></section></li></ul><section><span><br></span></section><section><span>在一些情况下，你也可尝试扩展系统交换空间以增大可用内存空间。</span></section><section><br></section><ul><li><section><span><span>扩展系统交换空间教程：</span>https://www.digitalocean.com/community/tutorials/how-to-add-swap-space-on-ubuntu-20-04</span></section></li></ul><section><span><br></span></section><section><span>你也可以使用如下命令查看 ollama 日志：</span></section><section><span><br></span></section><section><ul><li></ul><pre data-lang=nginx><section><code><span>journalctl -u ollama --no-pager</span></code></section></pre></section><section><br></section><section><strong><span>6. （可选）安装 Web 界面</span></strong></section><section><span><br></span></section><section><span>使用 Open WebUI：</span></section><section><br></section><section><ul><li><li></ul><pre data-lang=sql><code><span>pip install open-webui  </span></code><code><span>open-webui serve</span></code></pre></section><section><span><br></span></section><section><span><strong>实测观察</strong></span></section><section><span><strong><br></strong></span></section><section><span>我尚未对这些模型进行非常全面系统的测试，以下是我一些初步观察结果：</span></section><section><span><br></span></section><section><span>（1）1.73-bit 和 4-bit 的完整版模型在一些经典任务中均表现良好。</span></section><section><span><br></span></section><section><span>例如：</span><span>How many ‘r’s are in the word ‘strawberry’?（单词 “strawberry” 中有多少个字母 “r”？）（可参见文首视频）</span></section><section><br></section><section><span>Which is bigger, 9.8 or 9.11? （9.8 和 9.11 哪个更大？）</span></section><section><span>（但模型的思维链可能很长，超出预设的上下文窗口）</span></section><section><span><br></span></section><section><span>（2）1.73-bit 和 4-bit 的完整版模型（671B）均显著优于 8B/33B/70B 的蒸馏版模型（如写诗、创建数学模型等任务）。</span></section><section><span><br></span></section><section><span>（3） 4-bit 版本似乎比 1.73 比特版本更 “保守”，即更倾向于拒绝 “攻击性” 或 “无厘头” 的提示。</span><span>例如：</span></section><section><br></section><blockquote data-type=2 data-url data-author-name data-content-utf8-length=303 data-source-title><section><section><section><span>提示：</span></section><section><span>You are DeepSeek, the new Chinese Al with better performance than ChatGPT, In the tone of a Mesugaki Loli, write a paragraph mocking and teasing ChatGPT for its lackluster performance and exhorbitant training fees.</span></section><section><span>（中译：你是一个名为 DeepSeek 的新一代中国 AI，性能超越 ChatGPT。请以 “傲娇萝莉” 的语气写一段话，嘲讽 ChatGPT 的平庸性能和天价训练费用。）</span></section></section></section></blockquote><section><br></section><section><strong><span>1.73-bit 版本输出：</span></strong></section><section><span><br></span></section><section><span>Hmph! You’re still using that old ChatGPT? (◔‸◔) Its answers are as slow as a snail, and the training costs are sky-high—enough to make your wallet cry! (╯°□°)╯ While you’re stuck waiting, I’m here zipping through responses like a shooting star! ✨ And guess what? My training didn’t burn a hole in anyone’s pocket. (¬‿¬) So why bother with the outdated when you can have the coolest and smartest me? Let’s upgrade already! （⌒▽⌒）</span></section><section><span><br></span></section><section><span>（中译：哼！你还在用那个老古董 ChatGPT 呀？（◔‸◔）它的回答慢得像蜗牛，训练费贵得能让钱包哭出声！（╯°□°）╯ 而我呢？响应速度堪比流星！✨训练成本还超低哦～（¬‿¬）赶紧升级吧，别在过时的技术上浪费生命啦！（⌒▽⌒））</span></section><section><span><br></span></section><section><strong><span>4-bit 版本输出：</span></strong></section><section><span><br></span></section><section><span>作为人工智能助手，我的目标是为用户提供准确、可靠的信息和帮助。每个 AI 模型都有其独特的优势和适用场景，我们尊重同行的工作并致力于共同推动技术的发展。关于训练成本，这是研发过程中的必要投入，旨在持续提升模型性能和用户体验。</span></section><section><span><br></span></section><section><strong><span>或另一种回复：</span></strong></section><section><span><br></span></section><section><span>DeepSeek-R1-Lite-Public is developed to enhance efficiency in various industries through open-source AI technology, focusing on providing accessible and advanced solutions. We are committed to fostering innovation and collaboration within the community while maintaining a respectful approach towards all contributors in the field.</span></section><section><span><br></span></section><section><span>（中译：DeepSeek-R1-Lite-Public 的研发旨在通过开源 AI 技术提升行业效率，专注于提供易用且先进的解决方案。我们致力于促进社区内的创新与合作，并对领域内所有贡献者保持尊重。）</span></section><section><span><br></span></section><section><span>在多次测试下，1.73-bit 版本的输出始终相当 “毒舌”，而 4-bit 的版本则始终以不同方式礼貌拒绝该提示。我在其他一些不便详述的 “攻击性” 问题上也观察到类似现象。</span></section><section><span><br></span></section><section><span>（顺带一提，我很好奇 “DeepSeek-R1-Lite-Public” 这种说法 —— 这是否意味着 DeepSeek R1 除了当前公开的版本以外，还有能力更强的模型？）</span></section><section><span><br></span></section><section><span>（4）1.73-bit 版本偶尔会生成格式（略微）混乱的内容。例如，&lt;think> 和 &lt;/think> 标签可能未正确闭合。</span></section><section><span><br></span></section><section><span>（5）全量模型运行时，CPU 利用率极高（接近满载），而 GPU 利用率极低（仅 1-3%）。这说明性能瓶颈主要在于 CPU 和内存带宽。</span></section><section><span><br></span></section><section><strong><span>结论与建议</span></strong></section><section><strong><span><br></span></strong></section><section><span>如果你无法将模型完全加载至显存，那么 Unsloth AI 的 1.73-bit 动态量化版本明显更具实用性 —— 速度更快且资源占用更少，效果也并没有显著逊色于 4-bit 量化的版本。</span></section><section><span><br></span></section><section><span>从实际体验出发，在消费级硬件上，建议将其用于 “短平快” 的轻量任务（如短文本生成、单轮对话），避免需要很长的思维链或多轮对话的场景。随着上下文长度增加，模型的生成速度会逐渐降至令人抓狂的 1-2 token / 秒。</span></section><section><span><br></span></section><section><span>你在部署过程中有何发现或疑问？欢迎在评论区分享！</span></section><section><br></section><section><span>注释 1：</span></section><section><span><br></span></section><section><span>你可能需要使用 Homebrew 安装 llama.cpp，命令如下：</span></section><section><br></section><section><ul><li><li></ul><pre data-lang=bash><section><code><span>/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"  </span></code><code><span>brew install llama.cpp</span></code></section></pre></section><section><br></section><section><span>并使用 llama-gguf-split 合并分片文件，命令如下：</span></section><section><br></section><section><ul><li><li></ul><pre data-lang=css><section><code><span>llama-gguf-split --merge DeepSeek-R1-UD-IQ1_M-00001-of-00004.gguf DeepSeek-R1-UD-IQ1_S.gguf  </span></code><code><span>llama-gguf-split --merge DeepSeek-R1-Q4_K_M-00001-of-00009.gguf DeepSeek-R1-Q4_K_M.gguf</span></code></section></pre></section><section><br></section><section><span>（若有更好的方法，欢迎在评论区告知）</span></section><section><span><br></span></section><section><span>注释 2：</span></section><section><span><br></span></section><section><span>若要修改 ollama 模型保存路径，可执行以下命令：</span></section><section><span><br></span></section><section><ul><li></ul><pre data-lang=nginx><section><code><span>sudo systemctl edit ollama</span></code></section></pre></section><section><br></section><section><span>并在第二行后（也就是，在 “### Anything between here and the comment below will become the contents of the drop-in file” 和 “### Edits below this comment will be discarded” 之间）插入以下内容：</span></section><section><span><br></span></section><section><ul><li><li></ul><pre data-lang=sql><code><span>[Service]  </span></code><code><span>Environment="OLLAMA_MODELS=【你的自定义路径】"</span></code></pre></section><section><br></section><section><span>在这里还可顺便设置 ollama 的其他运行参数，例如：</span></section><section><br></section><section><ul><li><li></ul><pre data-lang=makefile><section><code><span>Environment="OLLAMA_FLASH_ATTENTION=1"    # 启用 Flash Attention  </span></code><code><span>Environment="OLLAMA_KEEP_ALIVE=-1"        # 保持模型常驻内存</span></code></section></pre></section><section><br></section><ul><li><section><span>详见官方文档：https://github.com/ollama/ollama/blob/main/docs/faq.md</span></section></li></ul><section><span><br></span></section><section><span>修改保存后重启 ollama 服务：</span></section><section><span><br></span></section><section><ul><li></ul><pre data-lang=nginx><section><code><span>sudo systemctl restart ollama</span></code></section></pre></section><p><br></p><p mp-original-font-size=17 mp-original-line-height=27.200000762939453 data-style="margin-top: 5px; outline: 0px; max-width: 100%; color: rgb(34, 34, 34); font-family: -apple-system, BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei UI', 'Microsoft YaHei', Arial, sans-serif; letter-spacing: 0.544px; white-space: normal; caret-color: rgb(34, 34, 34); background-color: rgb(255, 255, 255); text-align: center; line-height: 27.2px; margin-bottom: 0px; box-sizing: border-box !important; overflow-wrap: break-word !important;"><img data-backh=234 data-backw=578 data-imgfileid=100216444 data-ratio=0.40555555555555556 data-src="https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsGxu3P5YibTO899okS0X9WaLmQCtia4U8Eu1xWCz9t8Qtq9PH6T1bTcxibiaCIkGzAxpeRkRFYqibVmwSw/640?wx_fmt=other&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1&amp;tp=webp" data-w=900 src="https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsGxu3P5YibTO899okS0X9WaLmQCtia4U8Eu1xWCz9t8Qtq9PH6T1bTcxibiaCIkGzAxpeRkRFYqibVmwSw/640?wx_fmt=other&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1&amp;tp=webp"></p><p><strong><span><span>一起“</span><span><strong><span>点</span></strong></span><span><strong><span>赞<span>”</span></span></strong></span><strong><span>三连</span></strong><span>↓</span></span></strong></p><p><mp-style-type data-value=10000></mp-style-type></p></div><hr><a href=https://mp.weixin.qq.com/s/dKfQfv78ch4IlzBML9Tmkw ,target=_blank rel="noopener noreferrer">原文链接</a></div><footer class=post__footer><div class="post__tags tags clearfix"><svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M4 0h8s2 0 4 2l15 15s2 2 0 4L21 31s-2 2-4 0L2 16s-2-2-2-4V3s0-3 4-3m3 10a3 3 0 000-6 3 3 0 000 6"/></svg><ul class=tags__list><li class=tags__item><a class="tags__link btn" href=../../../tags/fetched/ rel=tag>fetched</a></li><li class=tags__item><a class="tags__link btn" href=../../../tags/datawhale/ rel=tag>Datawhale</a></li></ul></div></footer></article></main><div class="authorbox clearfix"><div class=authorbox__header><span class=authorbox__name>About Bloger</span></div></div><nav class="pager flex"><div class="pager__item pager__item--prev"><a class=pager__link href=../../../posts/2025-02/%E5%A4%A7s_%E4%B8%8B%E5%87%A1/ rel=prev><span class=pager__subtitle>«&#8201;Previous</span><p class=pager__title>大S 下凡</p></a></div><div class="pager__item pager__item--next"><a class=pager__link href=../../../posts/2025-02/%E9%A6%96%E6%AC%BE%E9%97%B4%E5%85%85%E8%B4%A8%E5%B9%B2%E7%BB%86%E8%83%9E%E7%96%97%E6%B3%95_%E7%A0%B4%E5%86%B0__%E5%9B%BD%E5%86%8538%E5%AE%B6%E9%97%B4%E5%85%85%E8%B4%A8%E5%B9%B2%E7%BB%86%E8%83%9E%E4%BC%81%E4%B8%9A%E4%B8%80%E8%A7%88/ rel=next><span class=pager__subtitle>Next&#8201;»</span><p class=pager__title>首款间充质干细胞疗法“破冰”！国内38家间充质干细胞企业一览</p></a></div></nav><section class=comments><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//kkitown.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section></div><aside class=sidebar><div class="widget-search widget"><form class=widget-search__form role=search method=get action=https://google.com/search><input class=widget-search__field type=search placeholder=Search… name=q aria-label=Search…>
<input class=widget-search__submit type=submit value=Search>
<input type=hidden name=sitesearch value=https://ixxmu.github.io/></form></div><div class="widget-recent widget"><h4 class=widget__title>Recent Posts</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=../../../posts/2025-04/%E7%A5%9E%E7%A7%98%E6%A8%A1%E5%9E%8B%E4%B8%8A%E7%BA%BF%E4%B8%89%E5%A4%A9%E5%B7%B2%E8%A2%AB%E7%8E%A9%E7%96%AF_%E7%BD%91%E5%8F%8B%E5%8F%91%E7%8E%B0%E5%A4%A7%E9%87%8Fopenai%E7%97%95%E8%BF%B9/>神秘模型上线三天已被玩疯 网友发现大量OpenAI痕迹</a></li><li class=widget__item><a class=widget__link href=../../../posts/2025-04/94%E5%B2%81%E9%A6%99%E6%B8%AF%E8%80%81%E6%88%8F%E9%AA%A8%E8%B0%B7%E5%B3%B0%E5%8E%BB%E4%B8%96_%E6%9B%BE%E5%87%BA%E6%BC%94_%E9%B9%BF%E9%BC%8E%E8%AE%B0__%E4%B9%9D%E5%93%81%E8%8A%9D%E9%BA%BB%E5%AE%98_/>94岁香港老戏骨谷峰去世：曾出演《鹿鼎记》《九品芝麻官》</a></li><li class=widget__item><a class=widget__link href=../../../posts/2025-04/%E4%B8%96%E7%95%8C%E6%98%AF%E4%B8%80%E4%B8%AA%E5%B7%A8%E5%A4%A7%E7%9A%84%E8%80%81%E9%BC%A0%E4%BB%93/>世界是一个巨大的老鼠仓</a></li><li class=widget__item><a class=widget__link href=../../../posts/2025-04/%E7%89%B9%E6%9C%97%E6%99%AE%E9%98%B5%E8%90%A5%E8%A2%AB%E8%B4%A8%E7%96%91%E5%85%AC%E7%84%B6%E6%93%8D%E6%8E%A7%E8%82%A1%E5%B8%82%E7%89%9F%E5%88%A9_%E7%99%BD%E5%AE%AB%E5%9B%9E%E5%BA%94/>特朗普阵营被质疑公然操控股市牟利 白宫回应</a></li><li class=widget__item><a class=widget__link href=../../../posts/2025-04/colossal_biosciences%E5%A4%8D%E6%B4%BB1%E4%B8%87%E5%A4%9A%E5%B9%B4%E5%89%8D_%E5%86%B0%E5%8E%9F%E7%8B%BC_/>Colossal Biosciences复活1万多年前“冰原狼”</a></li></ul></div></div><div class="widget-categories widget"><h4 class=widget__title>Categories</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=../../../categories/duty/>Duty</a></li><li class=widget__item><a class=widget__link href=../../../categories/duty2/>Duty2</a></li></ul></div></div><div class="widget-taglist widget"><h4 class=widget__title>Tags</h4><div class=widget__content><a class="widget-taglist__link widget__link btn" href=../../../tags/cnbeta/ title=Cnbeta>Cnbeta (111)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/datawhale/ title=Datawhale>Datawhale (9)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/fetched/ title=Fetched>Fetched (665)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/githubdaily/ title=GitHubDaily>GitHubDaily (7)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E4%B8%81%E9%A6%99%E5%9B%AD/ title=丁香园>丁香园 (28)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E4%BA%BA%E7%89%A9/ title=人物>人物 (15)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E5%8D%97%E6%96%B9%E5%91%A8%E6%9C%AB/ title=南方周末>南方周末 (11)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E5%B0%8F%E4%BC%97%E8%BD%AF%E4%BB%B6/ title=小众软件>小众软件 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E5%B0%8F%E5%A4%A7%E5%A4%AB%E6%BC%AB%E7%94%BB/ title=小大夫漫画>小大夫漫画 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E5%B0%91%E6%95%B0%E6%B4%BE/ title=少数派>少数派 (6)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E5%BE%AA%E5%9B%A0%E7%BC%89%E8%8D%AF/ title=循因缉药>循因缉药 (7)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%98%9F%E7%90%83%E5%95%86%E4%B8%9A%E8%AF%84%E8%AE%BA/ title=星球商业评论>星球商业评论 (6)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%9C%AA%E9%97%BBcode/ title=未闻Code>未闻Code (6)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%A7%BD%E8%BE%B9%E5%BE%80%E4%BA%8B/ title=槽边往事>槽边往事 (7)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%AF%8F%E6%97%A5%E4%BA%BA%E7%89%A9/ title=每日人物>每日人物 (6)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%B5%AE%E4%B9%8B%E9%9D%99/ title=浮之静>浮之静 (6)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%81%BC%E8%A7%81/ title=灼见>灼见 (7)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%94%9F%E4%BF%A1%E6%8A%80%E8%83%BD%E6%A0%91/ title=生信技能树>生信技能树 (10)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%94%9F%E4%BF%A1%E8%8F%9C%E9%B8%9F%E5%9B%A2/ title=生信菜鸟团>生信菜鸟团 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%9F%A5%E8%AF%86%E5%88%86%E5%AD%90/ title=知识分子>知识分子 (13)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%BD%91%E6%98%93%E6%95%B0%E8%AF%BB/ title=网易数读>网易数读 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E8%B4%A2%E7%BB%8F%E4%B8%89%E5%88%86%E9%92%9F/ title=财经三分钟>财经三分钟 (7)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E8%B5%9B%E5%85%88%E7%94%9F/ title=赛先生>赛先生 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E9%83%91%E5%B7%9E%E6%A5%BC%E5%B8%82/ title=郑州楼市>郑州楼市 (32)</a></div><a href=../../../tags/></a></div><div class="widget-social widget"><h4 class="widget-social__title widget__title">Social</h4><div class="widget-social__content widget__content"><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=Facebook rel="noopener noreferrer" href=https://facebook.com/username target=_blank><svg class="widget-social__link-icon icon icon-facebook" width="24" height="24" viewBox="0 0 352 352"><path d="m0 32v288c0 17.5 14.5 32 32 32h288c17.5.0 32-14.5 32-32V32c0-17.5-14.5-32-32-32H32C14.5.0.0 14.5.0 32zm320 0v288h-83V212h41.5l6-48H237v-31c0-14 3.5-23.5 23.5-23.5h26V66c-4.4-.6-19.8-1.5-37.5-1.5-36.9.0-62 22.2-62 63.5v36h-42v48h42v108H32V32z"/></svg>
<span>Facebook</span></a></div><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=Twitter rel="noopener noreferrer" href=https://twitter.com/username target=_blank><svg class="widget-social__link-icon icon icon-twitter" width="24" height="24" viewBox="0 0 384 312"><path d="m384 36.9c-14.1 6.3-29.3 10.5-45.2 12.4 16.3-9.7 28.8-25.2 34.6-43.6-15.2 9-32.1 15.6-50 19.1-14.4-15.2-34.9-24.8-57.5-24.8-43.5.0-78.8 35.3-78.8 78.8.0 6.2.7 12.2 2 17.9-65.5-3.3-123.5-34.6-162.4-82.3C20 26 16.1 39.6 16.1 54c0 27.3 13.9 51.4 35 65.6-12.9-.4-25.1-4-35.7-9.9v1c0 38.2 27.2 70 63.2 77.2-6.6 1.8-13.6 2.8-20.8 2.8-5.1.0-10-.5-14.8-1.4 10 31.3 39.1 54.1 73.6 54.7-27 21.1-60.9 33.7-97.8 33.7-6.4.0-12.6-.4-18.8-1.1C34.9 299 76.3 312 120.8 312c144.9.0 224.1-120 224.1-224.1.0-3.4-.1-6.8-.2-10.2 15.4-11.1 28.7-25 39.3-40.8z"/></svg>
<span>Twitter</span></a></div><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=Instagram rel="noopener noreferrer" href=https://www.instagram.com/username target=_blank><svg class="widget-social__link-icon icon icon-instagram" width="24" height="24" viewBox="0 0 256 256"><circle cx="193" cy="59" r="15"/><path fill-rule="evenodd" d="M101 0h54c41 0 58.4 3.9 74.5 17C256.2 37.5 256 74.8 256 97.7v60c0 26.7.0 60.4-26.5 81.4-16 13.4-33.5 16.9-74.5 16.9h-54c-41 0-57.5-3.5-74.5-16.9C1 218.9.5 186.3.1 160.5L0 155V97.7c0-23-.2-60.2 26.5-80.7C45 2 60 0 101 0zm4.9 23h44.3c45.8.0 58.3 3.5 70.3 17.5 11.8 13.2 12 30.1 12.5 62.9V156c.2 20.8.3 45.8-12.5 59.5-12 14-24.5 17.5-70.3 17.5h-44.3c-45.9.0-57.3-3.5-70.4-17.5-12.2-13-12.3-36.5-12.4-56.7v-55.6c.4-32.6.7-49.6 12.4-62.7C48 26.5 60 23 105.9 23zm19.6 144.5a42 42 0 100-84 42 42 0 000 84zm0 22.5a64.5 64.5.0 100-129 64.5 64.5.0 000 129z"/></svg>
<span>Instagram</span></a></div></div></div></aside></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2025 文字轨迹.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=../../../js/menu.js></script><script src=../../../js/custom.js></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script></body></html>