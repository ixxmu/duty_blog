<!doctype html><html class=no-js lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>DeepSeek关键RL算法GRPO，手把手教你从头跑通！ - 文字轨迹</title>
<script>(function(e,t){e[t]=e[t].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:url" content="https://ixxmu.github.io/posts/2025-03/deepseek%E5%85%B3%E9%94%AErl%E7%AE%97%E6%B3%95grpo_%E6%89%8B%E6%8A%8A%E6%89%8B%E6%95%99%E4%BD%A0%E4%BB%8E%E5%A4%B4%E8%B7%91%E9%80%9A_/"><meta property="og:site_name" content="文字轨迹"><meta property="og:title" content="DeepSeek关键RL算法GRPO，手把手教你从头跑通！"><meta property="og:description" content="DeepSeek关键RL算法GRPO，手把手教你从头跑通！ by Datawhale Datawhale分享 作者：Andriy Burkov，编译：机器之心"><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-02T08:26:22+00:00"><meta property="article:modified_time" content="2025-03-02T08:26:22+00:00"><meta property="article:tag" content="Fetched"><meta property="article:tag" content="Datawhale"><meta itemprop=name content="DeepSeek关键RL算法GRPO，手把手教你从头跑通！"><meta itemprop=description content="DeepSeek关键RL算法GRPO，手把手教你从头跑通！ by Datawhale Datawhale分享 作者：Andriy Burkov，编译：机器之心"><meta itemprop=datePublished content="2025-03-02T08:26:22+00:00"><meta itemprop=dateModified content="2025-03-02T08:26:22+00:00"><meta itemprop=wordCount content="382"><meta itemprop=keywords content="Fetched,Datawhale"><meta name=twitter:card content="summary"><meta name=twitter:title content="DeepSeek关键RL算法GRPO，手把手教你从头跑通！"><meta name=twitter:description content="DeepSeek关键RL算法GRPO，手把手教你从头跑通！ by Datawhale Datawhale分享 作者：Andriy Burkov，编译：机器之心"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=../../../css/style.css><link rel=stylesheet href=../../../css/custom.css><link rel="shortcut icon" href=../../../favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=../../../ title=文字轨迹 rel=home><div class="logo__item logo__text"><div class=logo__title>文字轨迹</div><div class=logo__tagline>故事流淌过的地方</div></div></a></div><nav class=menu><button class=menu__btn aria-haspopup=true aria-expanded=false tabindex=0>
<span class=menu__btn-title tabindex=-1>Menu</span></button><ul class=menu__list><li class=menu__item><a class=menu__link href=../../../posts/><span class=menu__text>文章</span></a></li><li class=menu__item><a class=menu__link href=../../../tags/><span class=menu__text>标签</span></a></li><li class=menu__item><a class=menu__link href=../../../about/><span class=menu__text>关于</span></a></li></ul></nav></div></header><div class="wrapper flex"><div class=primary><main class=main role=main><article class=post><header class=post__header><meta name=referrer content="never"><h1 class=post__title>DeepSeek关键RL算法GRPO，手把手教你从头跑通！</h1><div class="post__meta meta"><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0a14 14 0 110 28 1 1 0 010-28m0 3a3 3 0 100 22 3 3 0 000-22m1 4h-2v8.4l6.8 4.4L22 18l-6-3.8z"/></svg><time class=meta__text datetime=2025-03-02T08:26:22Z>2025-03-02</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=../../../categories/duty/ rel=category>Duty</a></span></div><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 16 16"><path d="M8 1c2 0 3.5 2 3.5 4.5S10 9 10 9c3 1 4 2 4 6H2c0-4 1-5 4-6 0 0-1.5-1-1.5-3.5S6 1 8 1"/></svg><span class=meta__text>Bloger</span></div></div></header><div class="post__toc toc"><div class=toc__title>Page content</div><div class=toc__menu><nav id=TableOfContents><ul><li><a href=#deepseek关键rl算法grpo手把手教你从头跑通-by-datawhale>DeepSeek关键RL算法GRPO，手把手教你从头跑通！ by Datawhale</a></li></ul></nav></div></div><div class="content post__content clearfix"><h2 id=deepseek关键rl算法grpo手把手教你从头跑通-by-datawhale>DeepSeek关键RL算法GRPO，手把手教你从头跑通！ by Datawhale</h2><div><section data-mpa-powered-by=yiban.io data-style="-webkit-tap-highlight-color: transparent; margin-bottom: 0px; outline: 0px; text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px; background-color: rgb(255, 255, 255); letter-spacing: 0.544px; text-wrap: wrap; caret-color: rgb(34, 34, 34); font-family: 'Helvetica Neue', Helvetica, 'Hiragino Sans GB', 'Microsoft YaHei', Arial, sans-serif; visibility: visible; line-height: 27.2px; color: rgb(163, 163, 163) !important;" mp-original-font-size=17 mp-original-line-height=27.200000762939453><section data-mpa-powered-by=yiban.io data-style="-webkit-tap-highlight-color: transparent; margin-bottom: 0px; outline: 0px; text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px; background-color: rgb(255, 255, 255); letter-spacing: 0.544px; text-wrap: wrap; caret-color: rgb(34, 34, 34); font-family: 'Helvetica Neue', Helvetica, 'Hiragino Sans GB', 'Microsoft YaHei', Arial, sans-serif; visibility: visible; line-height: 27.2px; color: rgb(163, 163, 163) !important;" mp-original-font-size=17 mp-original-line-height=27.200000762939453><section data-mpa-powered-by=yiban.io data-style="-webkit-tap-highlight-color: transparent; margin-bottom: 0px; outline: 0px; text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px; background-color: rgb(255, 255, 255); letter-spacing: 0.544px; text-wrap: wrap; caret-color: rgb(34, 34, 34); font-family: 'Helvetica Neue', Helvetica, 'Hiragino Sans GB', 'Microsoft YaHei', Arial, sans-serif; visibility: visible; line-height: 27.2px; color: rgb(163, 163, 163) !important;" mp-original-font-size=17 mp-original-line-height=27.200000762939453><section data-mpa-powered-by=yiban.io data-style="-webkit-tap-highlight-color: transparent; margin-bottom: 0px; outline: 0px; text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px; background-color: rgb(255, 255, 255); letter-spacing: 0.544px; text-wrap: wrap; caret-color: rgb(34, 34, 34); font-family: 'Helvetica Neue', Helvetica, 'Hiragino Sans GB', 'Microsoft YaHei', Arial, sans-serif; visibility: visible; line-height: 27.2px; color: rgb(163, 163, 163) !important;" mp-original-font-size=17 mp-original-line-height=27.200000762939453><section data-mpa-powered-by=yiban.io><section powered-by=xiumi.us><section><section data-id=85660 data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)"><section><p><span> Datawhale分享 </span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);"><p><span><strong>作者：Andriy Burkov，编译：机器之心</strong></span></p></section></section></section></section><section><mp-common-profile data-id="MzIyNjM2MzQyNg==" data-pluginname=mpprofile data-headimg="http://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsEXsBwQkpYLtE2vhn7Z3RVOSRu5M1VicIgqgMRKLsxsibK7OUSqUb1rUO4pfXnQyFYKqhryAIeh4MOg/300?wx_fmt=png&amp;wxfrom=19" data-nickname=Datawhale data-alias=Datawhale data-signature="一个专注于AI领域的开源组织，汇聚了众多优秀学习者，使命-for the learner，和学习者一起成长。" data-from=2 data-is_biz_ban=0 data-weui-theme=light data-origin_num=721 data-isban=0 data-biz_account_status=0 data-index=0></mp-common-profile></section></section></section></section></section></section></section><section><span>GRPO（Group Relative Policy Optimization）是 DeepSeek-R1 成功的基础技术之一</span><span>。</span><br></section><section><br></section><section><span>简单来说，GRPO 算法丢弃了 critic model，放弃了价值函数近似，转而通过组内样本的相对比较来计算策略梯度，从而有效降低了训练的不稳定性，同时提高了学习效率。</span></section><section><br></section><section><span>既然 GRPO 如此有效，那么，你知道如何从头开始实现 GRPO 吗？</span></section><section><br></section><section><span>近日，AI 工程师和技术作家 Andriy Burkov 发布了一份「从头开始写 GRPO 代码」的教程，其中介绍了如何基于 Qwen2.5-1.5B-Instruct 模型构建一个使用 GRPO 的分布式强化学习流程。</span></section><section><br></section><section><span>不过，在我们深入这份教程之前，先简单介绍一下它的作者。Andriy Burkov 算得上是 AI 领域的一位著名科普作家，在加拿大拉瓦尔大学取得了计算机科学博士学位，还曾发表过两本颇受欢迎的 AI 主题著作：《100 页语言模型书》和《100 页机器学习书》；书中一步步详实地介绍了相关概念，并附带了简明的实现代码。</span></section><section><br></section><section><img data-backh=853 data-backw=562 data-galleryid data-imgfileid=100218734 data-ratio=1.5177570093457944 data-s=300,640 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsErnZlkQefQOXzicxWAftUY19lQjK11HXyJbRCA3QjyNR3N0vfufNibndbEzOYuAT1RI9bcpibF6y4gg/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1070 src="https://mmbiz.qpic.cn/sz_mmbiz_png/vI9nYe94fsErnZlkQefQOXzicxWAftUY19lQjK11HXyJbRCA3QjyNR3N0vfufNibndbEzOYuAT1RI9bcpibF6y4gg/640?wx_fmt=png&amp;from=appmsg"></section><section><span>接下来我们就来看看这份 GRPO 从头实现教程吧。</span></section><section><br></section><section><img alt=image.png data-imgfileid=503473693 data-ratio=0.39166666666666666 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9hpDaVcRVCxicraWBrWSD7Fb46fOwTJMRQ7lAFkBhPibYFwc6UEJXfCXD5tdsFwnH8yK1Fm1HkDfbw/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9hpDaVcRVCxicraWBrWSD7Fb46fOwTJMRQ7lAFkBhPibYFwc6UEJXfCXD5tdsFwnH8yK1Fm1HkDfbw/640?wx_fmt=png&amp;from=appmsg"></section><section><br></section><section><span>教程地址：https://github.com/aburkov/theLMbook/blob/main/GRPO_From_Scratch_Multi_GPU_DataParallel_Qwen_2_5_1_5B_Instruct.ipynb</span></section><section><br></section><section><span><strong>从头编写 GRPO 代码</strong></span></section><section><span><strong>使用 Qwen2.5-1.5B-Instruct 的分布式实现</strong></span></section><section><br></section><section><span>本教程将展示如何使用 GRPO 方法构建分布式强化学习（RL）流程，从而可以针对数学、逻辑和编程任务对语言模型进行微调。</span></section><section><br></section><section><span>首先需要明确，这些任务都存在一个唯一且正确的 ground truth 答案，可通过简单的字符串比较轻松加以验证。</span></section><section><br></section><section><span>GRPO 的发明者是 DeepSeek，最早是被用于微调 DeepSeek 的 R1 和 R1-Zero 模型 —— 它们可通过学习生成思维链（CoT）来更好地解决数学和逻辑问题。</span></section><section><br></section><section><span>本教程的目标是将通用语言模型 Qwen2.5-1.5B-Instruct 转换为数学问题求解器。我们将从头开始编写 GRPO 代码，然后将其与几个流行的库和工具集成起来，以实现分布式训练管道流程，包括：</span></section><section><br></section><ul><li><section><span>PyTorch：用于张量运算和分布式训练。</span></section></li><li><section><span>Hugging Face Transformers：用于加载预训练的语言模型和 tokenizer。</span></section></li><li><section><span>FlashAttention2：优化的注意力机制，有助于减少内存使用量并提高训练速度。</span></section></li><li><section><span>Weights & Biases (wandb)：用于实验跟踪、可视化和模型版本控制。</span></section></li></ul><section><br></section><section><span>本教程分为几个部分。首先是基本设置和导入，然后是数据格式化和答案提取、数据集准备、评估函数、奖励函数、训练设置和执行，最后加载和测试模型。此过程中，我们将从头实现 GRPO 算法。</span></section><section><br></section><section><span><strong>Part 1：基础设置和导入</strong></span></section><section><br></section><section><span>首先是安装并导入所有必要的模块。下面是导入库的一段代码截图。</span></section><section><br></section><section><img alt=image.png data-imgfileid=503473694 data-ratio=0.5638888888888889 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9hpDaVcRVCxicraWBrWSD7FIYvtPBFVqxmZDyS7ZwTibzYicRkq0r1IojeyQPcvdhrBXGic7icFhXQjBg/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9hpDaVcRVCxicraWBrWSD7FIYvtPBFVqxmZDyS7ZwTibzYicRkq0r1IojeyQPcvdhrBXGic7icFhXQjBg/640?wx_fmt=png&amp;from=appmsg"></section><section><span><em><span>部分代码截图。完整代码块参见 GitHub。</span></em></span></section><section><br></section><section><span>运行上述代码（参考项目完整代码），可以执行以下任务：</span></section><section><br></section><ul><li><section><span>设置随机种子：set_random_seed 函数通过为 Python 的随机模块、NumPy 和 PyTorch 设置种子，确保可复现性；</span></section></li><li><section><span>环境变量配置：设置 WANDB_API_KEY 和 WANDB_PROJECT 环境变量，以启用与 Weights & Biases 的实验跟踪；</span></section></li><li><section><span>导入必要的库，包括 random、copy、re、torch 等等。</span></section></li></ul><section><br></section><section><span><strong>Part 2：数据格式以及答案提取</strong></span></section><section><br></section><section><span>接下来，项目定义了数据格式，以及模型如何从输出和数据集中提取答案段落。</span></section><section><br></section><section><span>为了确保模型输出格式一致，项目还定义了一个系统提示。该提示指示模型生成包含 &lt; reasoning > 和 &lt; answer > 标签的输出。这一步通过两个函数完成：</span></section><section><br></section><ul><li><section><span>extract_answer_from_model_output：此函数获取模型的输出文本，并提取 &lt; answer > 标签内的内容；</span></section></li><li><section><span>extract_answer_from_dataset：此函数从 GSM8K 数据集中提取预期答案，该数据集使用 “####” 分隔符来分隔答案：</span></section></li></ul><section><span><br></span></section><section><span><img alt=image.png data-imgfileid=503473695 data-ratio=0.5076335877862596 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9hpDaVcRVCxicraWBrWSD7F0nSmk4wO0HVnyOveVk8Xp7yOY4icKCiaTROwQ9JM3qWZQlrbgicwsx5zQ/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1048 src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9hpDaVcRVCxicraWBrWSD7F0nSmk4wO0HVnyOveVk8Xp7yOY4icKCiaTROwQ9JM3qWZQlrbgicwsx5zQ/640?wx_fmt=png&amp;from=appmsg"></span></section><section><span><em><span>部分代码截图。完整代码块参见 GitHub。</span></em></span><br></section><section><br></section><section><span><strong>Part 3：数据准备</strong></span></section><section><br></section><section><span>该项目使用 GSM8K 数据集进行训练。项目使用了该数据集中的示例来训练模型，基于强化学习（RL）训练范式，让模型生成多个问题解答样本，之后作者将这些解答与 GSM8K 示例中的标准答案进行对比，如果匹配，就为 RL 算法（GRPO）提供高奖励，然后更新模型权重，以增加模型下次获得高奖励的可能性。</span></section><section><br></section><section><span>实验过程是这样的。首先从 Hugging Face 加载数据集，然后格式化每个示例，包括系统提示和用户提示。这段实现代码中还定义了两个辅助函数：prepare_dataset 以及 build_prompt。</span></section><section><br></section><section><img alt=image.png data-imgfileid=503473696 data-ratio=0.7731481481481481 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9hpDaVcRVCxicraWBrWSD7FZylKE8pEXwnNuvIHB5Qia5H95jXwO1QswaldiajXXQQYTdTuibC5ialMEA/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9hpDaVcRVCxicraWBrWSD7FZylKE8pEXwnNuvIHB5Qia5H95jXwO1QswaldiajXXQQYTdTuibC5ialMEA/640?wx_fmt=png&amp;from=appmsg"></section><section><span><em><span>部分代码截图。完整代码块参见 GitHub。</span></em></span></section><section><br></section><section><span><strong>Part 4：评估函数</strong></span></section><section><br></section><section><span>评估对于跟踪模型的进展至关重要。因此作者定义了一些函数，从而可以在一组示例上对模型进行评估。该项目的评估函数执行以下任务：</span></section><section><br></section><ul><li><section><span>token 化提示并生成响应：模型的输出是在 token 化提示的基础上生成的。</span></section></li><li><section><span>提取预测答案：从生成的响应中提取答案。</span></section></li><li><section><span>将预测答案与预期答案进行比较：这种比较是通过精确匹配以及数值等价检查来完成的。</span></section></li></ul><section><br></section><section><span>在这段代码中，两个辅助函数 _extract_last_number 和 _extract_single_number 被用来从文本中提取数字。评估函数 evaluate_model 使用这些辅助函数来确定预测答案是否正确：</span></section><section><br></section><section><img alt=image.png data-imgfileid=503473697 data-ratio=0.7953703703703704 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9hpDaVcRVCxicraWBrWSD7FdMUWGoIaEpC9gCEHXxl9TqI4dsAckG97xBk4Gib5FI9WwkWnsotCd4w/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9hpDaVcRVCxicraWBrWSD7FdMUWGoIaEpC9gCEHXxl9TqI4dsAckG97xBk4Gib5FI9WwkWnsotCd4w/640?wx_fmt=png&amp;from=appmsg"></section><section><span><em><span>部分代码截图。完整代码块参见 GitHub。</span></em></span></section><section><br></section><section><span><strong>Part 5：奖励函数</strong></span></section><section><br></section><section><span>在强化学习中，奖励函数是必不可缺的，作者定义了两个奖励函数：</span></section><section><br></section><section><span>correctness_reward：这个函数根据生成的答案是否正确来分配奖励。采用两种方式：精确的字符串匹配和数值等价检查，将模型输出的答案与预期答案进行比较。完全匹配会获得更高的奖励（2.0），而基于数值等价的匹配会获得较小的奖励（1.5）。</span></section><section><br></section><section><span>format_reward：这个函数鼓励模型遵循所需的类似 XML 的输出格式。它为生成文本中存在 &lt; reasoning>、&lt;/reasoning>、&lt;answer > 和 &lt;/answer > 标签提供小额奖励。</span></section><section><br></section><section><img alt=image.png data-imgfileid=503473698 data-ratio=0.9717457114026236 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9hpDaVcRVCxicraWBrWSD7F1fibnXicxwibPldicrfDZpWusZibPrb7orXyG6v6ibnMAS5qEgOERbuxICUg/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=991 src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9hpDaVcRVCxicraWBrWSD7F1fibnXicxwibPldicrfDZpWusZibPrb7orXyG6v6ibnMAS5qEgOERbuxICUg/640?wx_fmt=png&amp;from=appmsg"></section><section><span><em><span>部分代码截图。完整代码块参见 GitHub。</span></em></span></section><section><br></section><section><span><strong>Part 6：从头开始实现 DataParallel GRPO</strong></span></section><section><br></section><section><span>这一节，我们将从头实现 GRPO 算法的所有构建模块。首先，这里假设运行代码的机器至少有 2 台 GPU。为此，这里要使用 PyTorch 的 DataParallel API 来将策略模型放在多个 GPU 核心上，每个 GPU 核心都有该模型的一个副本。然后将批量数据分散在这些 GPU 核心上完成处理。</span></section><section><br></section><section><img alt=image.png data-imgfileid=503473699 data-ratio=0.4823943661971831 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9hpDaVcRVCxicraWBrWSD7Fc51LAJ30GEWvZDFg1p4ntoyhC6qNFWwjNFL2WNFF1VWsygiceS1icovw/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=852 src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9hpDaVcRVCxicraWBrWSD7Fc51LAJ30GEWvZDFg1p4ntoyhC6qNFWwjNFL2WNFF1VWsygiceS1icovw/640?wx_fmt=png&amp;from=appmsg"></section><section><span><em><span>部分代码截图。完整代码块参见 GitHub。</span></em></span></section><section><br></section><section><span><strong>Part 7：训练设置和执行</strong></span></section><section><br></section><section><span>这一节，我们将所有组件组合在一起，完成设置并开始训练。</span></section><section><br></section><section><span>首先，加载预训练的模型和 tokenizer，准备评估数据，然后使用上面从头实现的 train_with_grpo 进行强化学习微调。</span></section><section><br></section><section><span>关键步骤包括：</span></section><section><br></section><ul><li><section><span>模型和 tokenizer 初始化：使用优化设置（使用 torch.bfloat16 和 FlashAttention2）加载模型 Qwen/Qwen2.5-1.5B-Instruct。tokenizer 也要加载，其填充 token 设置为序列末尾 token。使用 torch.bfloat16 加载模型会将其参数转换为每个数值使用 16 位而不是 32 位的形式，这可将模型的内存使用量减少一半，并且可加快在现代 GPU 上的训练速度。</span></section></li><li><section><span>初步评估：在微调之前，根据几个示例对模型进行评估，以确定基准性能。</span></section></li><li><section><span>强化学习微调：为从头开始实现 GRPO 的训练函数 train_with_grpo 配置适当的训练参数和奖励函数。然后，在剩余的训练数据上执行强化学习训练。</span></section></li><li><section><span>最终评估和模型保存：强化学习微调后，再次评估模型，并保存最终模型。</span></section></li></ul><section><br></section><section><span>下面的代码会执行以下功能：</span></section><section><br></section><ul><li><section><span>确定设备（如果有 GPU 就用 GPU，否则就用 CPU）。</span></section></li><li><section><span>加载预训练版 Qwen2.5-1.5B-Instruct 模型和 tokenizer。tokenizer 的 pad token 设置为 eos_token。</span></section></li><li><section><span>保留一小部分数据集用于评估，以提供基线。</span></section></li><li><section><span>通过启用梯度检查点和禁用 KV 缓存，优化模型的内存效率。</span></section></li><li><section><span>步骤 1：在微调之前评估模型，以建立基线准确性。</span></section></li><li><section><span>步骤 2：使用 train_with_grpo 函数和我们定义的奖励函数（format_reward 和 correctness_reward，合并为 combined_reward）执行强化学习微调。这里使用了多台 GPU 训练模型。</span></section></li><li><section><span>步骤 3：将最终的微调模型和 tokenizer 保存到磁盘。</span></section></li></ul><section><br></section><section><span>GRPO 训练流程使用的超参数如下。</span></section><section><br></section><section><span><strong>训练配置</strong></span></section><section><br></section><section><span>以下参数设定了使用上面的 GRPO 算法实现强化学习微调运行的配置：</span></section><section><br></section><ul><li><section><span>num_iterations=1：从当前策略模型创建新参考模型的外部迭代次数。一次迭代是指在整个数据集上执行一次通过。</span></section></li><li><section><span>num_steps=500：训练循环将执行最多 500 个步骤，每个步骤处理一批样本。</span></section></li><li><section><span>batch_size=7：在 8 台 GPU 的情况下，每个步骤每批处理 7 个样本，每台 GPU 上放置 1 个样本。使用一个 GPU (0) 被 DataParallel 用作主节点来聚合梯度并收集输出。</span></section></li><li><section><span>num_generations=14：对于训练数据中的每个提示词，训练器将生成 14 个不同的完成结果。这些生成结果将被用于计算指导强化学习更新的相对优势（或奖励信号）。如果你的 GPU 的 VRAM 较少，请减少此数字。</span></section></li><li><section><span>max_completion_length=400：在生成完成结果（序列的 response 部分）时，生成上限为 400 个 token。这限制了模型在 RL 阶段生成的输出的长度。如果你的 GPU 的 VRAM 较少，请减少此数字。</span></section></li><li><section><span>beta=0.04：GRPO 损失函数中 KL 散度惩罚的系数。这控制的是模型与参考模型的偏差程度。</span></section></li><li><section><span>learning_rate=5e-6：RL 微调的学习率。为了实现稳定的策略更新，这里使用了相对较低的学习率。</span></section></li><li><section><span>mu=1：对每批 rollout 数据执行的策略更新次数。在这里，我们每批只执行一次更新。</span></section></li><li><section><span>epsilon=0.1：GRPO 的 PPO 组件的 clipping 参数。这可以防止策略在单次更新中发生太大的变化。</span></section></li></ul><section><br></section><section><span>在微调之前和之后都会对模型进行评估，以衡量准确率的提高情况。最后，将微调后的模型保存到 grpo_finetuned_model 目录中。</span></section><section><br></section><section><img alt=image.png data-imgfileid=503473700 data-ratio=0.6657381615598886 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9hpDaVcRVCxicraWBrWSD7FrpnnAcqEu2OGdtwvOqBp3OY3TqOYe0ffIPVIvgLVZC33Jvq0ZA3ECg/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=718 src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9hpDaVcRVCxicraWBrWSD7FrpnnAcqEu2OGdtwvOqBp3OY3TqOYe0ffIPVIvgLVZC33Jvq0ZA3ECg/640?wx_fmt=png&amp;from=appmsg"></section><section><span><em><span>部分代码截图。完整代码块参见 GitHub。</span></em></span></section><section><br></section><section><span>教程中还给出了详细的执行情况，可作参考。</span></section><section><br></section><section><img alt=image.png data-imgfileid=503473701 data-ratio=0.5777777777777777 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9hpDaVcRVCxicraWBrWSD7FvPIlUXBRu2OuOSYEEm870DVQuzqetHyrSMzR6Q9yeoSYPbiaOMbLJtw/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9hpDaVcRVCxicraWBrWSD7FvPIlUXBRu2OuOSYEEm870DVQuzqetHyrSMzR6Q9yeoSYPbiaOMbLJtw/640?wx_fmt=png&amp;from=appmsg"></section><section><br></section><section><span>下面我们也简单看看其训练过程。</span></section><section><br></section><section><span>首先，初始配置后，我们可以看到运行 GRPO 之前的准确度为 23.33%。</span></section><section><span><br></span></section><section><span><img alt=image.png data-imgfileid=503473702 data-ratio=0.43320235756385067 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9hpDaVcRVCxicraWBrWSD7FJGVIuUoh7S06Bht1eaic3jw3RSpKntBFpe2ufic41dR6ytXhr4t0PNicg/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1018 src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9hpDaVcRVCxicraWBrWSD7FJGVIuUoh7S06Bht1eaic3jw3RSpKntBFpe2ufic41dR6ytXhr4t0PNicg/640?wx_fmt=png&amp;from=appmsg"></span></section><section><br></section><section><span>然后经过 500 步的 1 轮 GRPO 迭代，下图展示了相关的训练动态：</span></section><section><br></section><section><img alt=image.png data-imgfileid=503473703 data-ratio=1.2626582278481013 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9hpDaVcRVCxicraWBrWSD7FEV5GHbETWUzk5oqn9Jiciaxgh0sehiaqquGRttOx7cy13Gg3qjWAgwryQ/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=632 src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9hpDaVcRVCxicraWBrWSD7FEV5GHbETWUzk5oqn9Jiciaxgh0sehiaqquGRttOx7cy13Gg3qjWAgwryQ/640?wx_fmt=png&amp;from=appmsg"></section><section><br></section><section><span>训练完成后，自然还需要对模型进行新一轮的评估。这里采用了 30 个评估样本来进行评估，以下展示了其中一个模型回答正确的示例：</span></section><section><span><br></span></section><section><span><img alt=sp_250301_112209.png data-imgfileid=503473704 data-ratio=1.3666666666666667 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9hpDaVcRVCxicraWBrWSD7FC8JkYE0Tf4ibJxsvdfuQe7ibfvpcjuw1KHdDbDuRicCAgoaQibnSHgMBTA/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9hpDaVcRVCxicraWBrWSD7FC8JkYE0Tf4ibJxsvdfuQe7ibfvpcjuw1KHdDbDuRicCAgoaQibnSHgMBTA/640?wx_fmt=png&amp;from=appmsg"></span></section><section><br></section><section><span>整体表现如何呢？可以看到，经过一轮 GRPO 之后，Qwen-2.5-1.5B-Instruct 模型答对了 30 问题中的 27 题，实现了 90% 的准确度。相较于 GRPO 之前的 23.33%，可说是实现了性能飞跃。</span></section><section><span><br></span></section><section><span><img alt=image.png data-imgfileid=503473705 data-ratio=0.6287037037037037 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9hpDaVcRVCxicraWBrWSD7FAn9ciaEiceZqyaSN4pxafbb1TwYnWWKKMjrsuJCelkVSyPTKnjBWnFmw/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9hpDaVcRVCxicraWBrWSD7FAn9ciaEiceZqyaSN4pxafbb1TwYnWWKKMjrsuJCelkVSyPTKnjBWnFmw/640?wx_fmt=png&amp;from=appmsg"></span></section><section><br></section><section><img alt=image.png data-imgfileid=503473706 data-ratio=0.5027777777777778 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9hpDaVcRVCxicraWBrWSD7FNvdzXBkDgHqQtWjUS0TfCDogcdkNVEQ5ftCJvpwQZibe0QAx6FzMcHw/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=1080 src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9hpDaVcRVCxicraWBrWSD7FNvdzXBkDgHqQtWjUS0TfCDogcdkNVEQ5ftCJvpwQZibe0QAx6FzMcHw/640?wx_fmt=png&amp;from=appmsg"></section><section><br></section><section><span>上面两张图展示了模型的学习过程动态，可以看到：平均奖励在 2.25 左右就趋于稳定了（理论最大值为 0.8 + 2.0 = 2.8）。相比于另一处微调的 Qwen-2.5-0.5B-Instruct（获得的平均奖励为 1.4），这个数字相当高了，参阅：https://github.com/aburkov/theLMbook/blob/main/GRPO_Qwen_0_5_Instruct.ipynb</span></section><section><br></section><section><span>如果使用更大的模型并允许更长的生成时间，模型正确解答问题的能力还将进一步提升。但是，如果要训练更大的模型，不仅需要将数据分布在多台 GPU 上，还需要将模型分开放在多台 GPU 上，这需要用到 DeepSpeed 或 FSDP（完全分片数据并行）等模型并行工具。</span></section><section><br></section><section><span>下面加载和测试已经微调的模型：</span></section><section><br></section><section><img alt=image.png data-imgfileid=503473707 data-ratio=0.43924191750278707 data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9hpDaVcRVCxicraWBrWSD7F3O34HqnX136zN2YxutNj2JOLQG8tZG3OebnFk1Vnjln7JA9uUtdrJA/640?wx_fmt=png&amp;from=appmsg" data-type=png data-w=897 src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9hpDaVcRVCxicraWBrWSD7F3O34HqnX136zN2YxutNj2JOLQG8tZG3OebnFk1Vnjln7JA9uUtdrJA/640?wx_fmt=png&amp;from=appmsg"></section><section><span><em><span>完整代码见原笔记本</span></em></span></section><section><br></section><section><span>加载完成后测试一下，首先问问 1+1 等于几：</span></section><section><span><br></span></section><section><img data-imgfileid=503473708 data-ratio=0.71875 data-s=300,640 data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9hpDaVcRVCxicraWBrWSD7F0IPuZ11yIuvRVSlzmib2pVNCbn61wsDibvTicnicMictaqXucY5hLqic5ccg/640?wx_fmt=gif&amp;from=appmsg" data-type=gif data-w=640 src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9hpDaVcRVCxicraWBrWSD7F0IPuZ11yIuvRVSlzmib2pVNCbn61wsDibvTicnicMictaqXucY5hLqic5ccg/640?wx_fmt=gif&amp;from=appmsg"></section><section><br></section><section><span>可以看到，模型反复思考了很多次，终于认定确实等于 2。</span></section><section><br></section><section><span>多次测试后还可以发现，该模型没有学会生成序列结束（EOS）token，因此即使在 &lt;/answer> token 之后，输出序列仍会继续。这是预期的行为，因为我们使用的奖励函数中没有包含一个用于停止生成的奖励。我们也没有执行监督微调步骤 —— 该步骤可以让模型学会在 &lt;/answer> 之后立即生成 EOS。</span></section><section><span><br></span></section><section><span></span></section><p><img alt=图片 data-backh=234 data-backw=578 data-imgfileid=100218732 data-ratio=0.40555555555555556 data-src="https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsGxu3P5YibTO899okS0X9WaLmQCtia4U8Eu1xWCz9t8Qtq9PH6T1bTcxibiaCIkGzAxpeRkRFYqibVmwSw/640?wx_fmt=other&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1&amp;tp=webp" data-w=900 src="https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsGxu3P5YibTO899okS0X9WaLmQCtia4U8Eu1xWCz9t8Qtq9PH6T1bTcxibiaCIkGzAxpeRkRFYqibVmwSw/640?wx_fmt=other&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1&amp;tp=webp"><strong><span><span>一起“</span><span><strong><span>点</span></strong></span><span><strong><span>赞<span>”三连</span></span></strong></span><span>↓</span></span></strong></p><section><span mp-original-font-size=12 mp-original-line-height=19.200000762939453></span></section><section><section><br></section></section><p><mp-style-type data-value=3></mp-style-type></p></div><hr><a href=https://mp.weixin.qq.com/s/gi8ee4m6borLPlPRBcp7Mg ,target=_blank rel="noopener noreferrer">原文链接</a></div><footer class=post__footer><div class="post__tags tags clearfix"><svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M4 0h8s2 0 4 2l15 15s2 2 0 4L21 31s-2 2-4 0L2 16s-2-2-2-4V3s0-3 4-3m3 10a3 3 0 000-6 3 3 0 000 6"/></svg><ul class=tags__list><li class=tags__item><a class="tags__link btn" href=../../../tags/fetched/ rel=tag>fetched</a></li><li class=tags__item><a class="tags__link btn" href=../../../tags/datawhale/ rel=tag>Datawhale</a></li></ul></div></footer></article></main><div class="authorbox clearfix"><div class=authorbox__header><span class=authorbox__name>About Bloger</span></div></div><nav class="pager flex"><div class="pager__item pager__item--prev"><a class=pager__link href=../../../posts/2025-03/%E9%94%BB%E7%82%BC%E5%87%8F%E8%82%A5%E5%92%8C%E8%82%BF%E7%98%A4%E6%81%B6%E7%97%85%E8%B4%A8%E7%9B%B8%E5%85%B3%E6%B6%88%E7%98%A6_%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB_%E5%8C%97%E5%A4%A7%E5%9B%A2%E9%98%9Fcell%E5%AD%90%E5%88%8A%E7%A0%94%E7%A9%B6%E5%91%8A%E8%AF%89%E4%BD%A0%E7%AD%94%E6%A1%88__/ rel=prev><span class=pager__subtitle>«&#8201;Previous</span><p class=pager__title>锻炼减肥和肿瘤恶病质相关消瘦，有什么区别？北大团队Cell子刊研究告诉你答案……</p></a></div><div class="pager__item pager__item--next"><a class=pager__link href=../../../posts/2025-03/%E7%89%B9%E6%9C%97%E6%99%AE%E5%9C%A8%E7%99%BD%E5%AE%AB%E5%8A%A0%E5%AF%86%E8%B4%A7%E5%B8%81%E5%B3%B0%E4%BC%9A%E5%89%8D%E8%AE%A8%E8%AE%BA%E6%88%98%E7%95%A5%E5%82%A8%E5%A4%87_%E6%AF%94%E7%89%B9%E5%B8%81%E4%BB%B7%E6%A0%BC%E9%A3%99%E5%8D%87/ rel=next><span class=pager__subtitle>Next&#8201;»</span><p class=pager__title>特朗普在白宫加密货币峰会前讨论战略储备 比特币价格飙升</p></a></div></nav><section class=comments><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//kkitown.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section></div><aside class=sidebar><div class="widget-search widget"><form class=widget-search__form role=search method=get action=https://google.com/search><input class=widget-search__field type=search placeholder=Search… name=q aria-label=Search…>
<input class=widget-search__submit type=submit value=Search>
<input type=hidden name=sitesearch value=https://ixxmu.github.io/></form></div><div class="widget-recent widget"><h4 class=widget__title>Recent Posts</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=../../../posts/2025-09/%E9%97%B2%E9%B1%BC%E4%B8%8A%E9%82%A3%E4%BA%9B%E9%80%86%E5%A4%A9%E7%9A%84%E6%B5%8B%E5%BA%8F%E4%BB%AA/>闲鱼上那些逆天的测序仪</a></li><li class=widget__item><a class=widget__link href=../../../posts/2025-09/%E8%AF%B4%E8%B5%B7%E8%BE%9B%E8%8A%B7%E8%95%BE_%E6%88%91%E6%83%B3%E8%B5%B7%E6%97%A9%E5%B9%B4%E4%B8%80%E4%BA%9B%E5%93%81%E5%91%B3_%E7%8B%AC%E7%89%B9_%E7%9A%84%E6%81%90%E6%80%96%E7%89%87/>说起辛芷蕾，我想起早年一些品味“独特”的恐怖片</a></li><li class=widget__item><a class=widget__link href=../../../posts/2025-09/%E5%A4%9A%E6%AC%A1%E7%94%9F%E5%AD%90%E5%90%8E_28%E5%B2%81%E5%86%9C%E6%9D%91%E6%99%BA%E9%9A%9C%E5%A5%B3%E5%AD%A9%E6%82%84%E7%84%B6%E7%A6%BB%E4%B8%96/>多次生子后，28岁农村智障女孩悄然离世</a></li><li class=widget__item><a class=widget__link href=../../../posts/2025-09/openai%E7%BD%95%E8%A7%81%E5%8F%91%E8%AE%BA%E6%96%87_%E6%88%91%E4%BB%AC%E6%89%BE%E5%88%B0%E4%BA%86ai%E5%B9%BB%E8%A7%89%E7%9A%84%E7%BD%AA%E9%AD%81%E7%A5%B8%E9%A6%96/>OpenAI罕见发论文：我们找到了AI幻觉的罪魁祸首</a></li><li class=widget__item><a class=widget__link href=../../../posts/2025-09/%E5%8D%97%E4%BA%AC%E5%8D%96%E6%B7%AB%E5%A4%B4%E7%9B%AE1_34%E4%BA%BF%E6%B8%AF%E5%85%83%E8%A2%AB%E7%91%9E%E9%93%B6%E5%8E%9F%E5%89%AF%E6%80%BB%E7%9B%91%E7%A7%81%E5%90%9E/>南京卖淫头目1.34亿港元被瑞银原副总监私吞</a></li></ul></div></div><div class="widget-categories widget"><h4 class=widget__title>Categories</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=../../../categories/duty/>Duty</a></li><li class=widget__item><a class=widget__link href=../../../categories/duty2/>Duty2</a></li></ul></div></div><div class="widget-taglist widget"><h4 class=widget__title>Tags</h4><div class=widget__content><a class="widget-taglist__link widget__link btn" href=../../../tags/cnbeta/ title=Cnbeta>Cnbeta (148)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/datawhale/ title=Datawhale>Datawhale (9)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/drpei/ title=Drpei>Drpei (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/fetched/ title=Fetched>Fetched (755)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/githubdaily/ title=GitHubDaily>GitHubDaily (7)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E4%B8%81%E9%A6%99%E5%9B%AD/ title=丁香园>丁香园 (30)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E4%BA%BA%E7%89%A9/ title=人物>人物 (15)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E5%8D%97%E6%96%B9%E5%91%A8%E6%9C%AB/ title=南方周末>南方周末 (11)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E5%B0%8F%E4%BC%97%E8%BD%AF%E4%BB%B6/ title=小众软件>小众软件 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E5%B0%8F%E5%A4%A7%E5%A4%AB%E6%BC%AB%E7%94%BB/ title=小大夫漫画>小大夫漫画 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E5%B0%91%E6%95%B0%E6%B4%BE/ title=少数派>少数派 (6)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E5%BE%AA%E5%9B%A0%E7%BC%89%E8%8D%AF/ title=循因缉药>循因缉药 (9)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%96%B0%E4%B9%A1%E5%9C%9F/ title=新乡土>新乡土 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%98%9F%E7%90%83%E5%95%86%E4%B8%9A%E8%AF%84%E8%AE%BA/ title=星球商业评论>星球商业评论 (7)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%9C%88%E5%85%89%E5%8D%9A%E5%AE%A2/ title=月光博客>月光博客 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%9C%AA%E9%97%BBcode/ title=未闻Code>未闻Code (6)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%A7%BD%E8%BE%B9%E5%BE%80%E4%BA%8B/ title=槽边往事>槽边往事 (9)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%AF%8F%E6%97%A5%E4%BA%BA%E7%89%A9/ title=每日人物>每日人物 (10)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%B5%AA%E6%BD%AE%E5%B7%A5%E4%BD%9C%E5%AE%A4/ title=浪潮工作室>浪潮工作室 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E6%B5%AE%E4%B9%8B%E9%9D%99/ title=浮之静>浮之静 (6)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%81%BC%E8%A7%81/ title=灼见>灼见 (8)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%86%8A%E8%A8%80%E7%86%8A%E8%AF%AD/ title=熊言熊语>熊言熊语 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%94%9F%E4%BF%A1%E6%8A%80%E8%83%BD%E6%A0%91/ title=生信技能树>生信技能树 (10)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%94%9F%E4%BF%A1%E8%8F%9C%E9%B8%9F%E5%9B%A2/ title=生信菜鸟团>生信菜鸟团 (6)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%9F%A5%E8%AF%86%E5%88%86%E5%AD%90/ title=知识分子>知识分子 (14)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E7%BD%91%E6%98%93%E6%95%B0%E8%AF%BB/ title=网易数读>网易数读 (7)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E8%B4%A2%E6%96%B0/ title=财新>财新 (6)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E8%B4%A2%E7%BB%8F%E4%B8%89%E5%88%86%E9%92%9F/ title=财经三分钟>财经三分钟 (7)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E8%B5%9B%E5%85%88%E7%94%9F/ title=赛先生>赛先生 (5)</a>
<a class="widget-taglist__link widget__link btn" href=../../../tags/%E9%83%91%E5%B7%9E%E6%A5%BC%E5%B8%82/ title=郑州楼市>郑州楼市 (32)</a></div><a href=../../../tags/></a></div><div class="widget-social widget"><h4 class="widget-social__title widget__title">Social</h4><div class="widget-social__content widget__content"><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=Facebook rel="noopener noreferrer" href=https://facebook.com/username target=_blank><svg class="widget-social__link-icon icon icon-facebook" width="24" height="24" viewBox="0 0 352 352"><path d="m0 32v288c0 17.5 14.5 32 32 32h288c17.5.0 32-14.5 32-32V32c0-17.5-14.5-32-32-32H32C14.5.0.0 14.5.0 32zm320 0v288h-83V212h41.5l6-48H237v-31c0-14 3.5-23.5 23.5-23.5h26V66c-4.4-.6-19.8-1.5-37.5-1.5-36.9.0-62 22.2-62 63.5v36h-42v48h42v108H32V32z"/></svg>
<span>Facebook</span></a></div><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=Twitter rel="noopener noreferrer" href=https://twitter.com/username target=_blank><svg class="widget-social__link-icon icon icon-twitter" width="24" height="24" viewBox="0 0 384 312"><path d="m384 36.9c-14.1 6.3-29.3 10.5-45.2 12.4 16.3-9.7 28.8-25.2 34.6-43.6-15.2 9-32.1 15.6-50 19.1-14.4-15.2-34.9-24.8-57.5-24.8-43.5.0-78.8 35.3-78.8 78.8.0 6.2.7 12.2 2 17.9-65.5-3.3-123.5-34.6-162.4-82.3C20 26 16.1 39.6 16.1 54c0 27.3 13.9 51.4 35 65.6-12.9-.4-25.1-4-35.7-9.9v1c0 38.2 27.2 70 63.2 77.2-6.6 1.8-13.6 2.8-20.8 2.8-5.1.0-10-.5-14.8-1.4 10 31.3 39.1 54.1 73.6 54.7-27 21.1-60.9 33.7-97.8 33.7-6.4.0-12.6-.4-18.8-1.1C34.9 299 76.3 312 120.8 312c144.9.0 224.1-120 224.1-224.1.0-3.4-.1-6.8-.2-10.2 15.4-11.1 28.7-25 39.3-40.8z"/></svg>
<span>Twitter</span></a></div><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=Instagram rel="noopener noreferrer" href=https://www.instagram.com/username target=_blank><svg class="widget-social__link-icon icon icon-instagram" width="24" height="24" viewBox="0 0 256 256"><circle cx="193" cy="59" r="15"/><path fill-rule="evenodd" d="M101 0h54c41 0 58.4 3.9 74.5 17C256.2 37.5 256 74.8 256 97.7v60c0 26.7.0 60.4-26.5 81.4-16 13.4-33.5 16.9-74.5 16.9h-54c-41 0-57.5-3.5-74.5-16.9C1 218.9.5 186.3.1 160.5L0 155V97.7c0-23-.2-60.2 26.5-80.7C45 2 60 0 101 0zm4.9 23h44.3c45.8.0 58.3 3.5 70.3 17.5 11.8 13.2 12 30.1 12.5 62.9V156c.2 20.8.3 45.8-12.5 59.5-12 14-24.5 17.5-70.3 17.5h-44.3c-45.9.0-57.3-3.5-70.4-17.5-12.2-13-12.3-36.5-12.4-56.7v-55.6c.4-32.6.7-49.6 12.4-62.7C48 26.5 60 23 105.9 23zm19.6 144.5a42 42 0 100-84 42 42 0 000 84zm0 22.5a64.5 64.5.0 100-129 64.5 64.5.0 000 129z"/></svg>
<span>Instagram</span></a></div></div></div></aside></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2025 文字轨迹.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=../../../js/menu.js></script><script src=../../../js/custom.js></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script></body></html>